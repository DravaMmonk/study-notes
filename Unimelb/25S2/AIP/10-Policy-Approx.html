
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="09-Value-Approx.html">
      
      
        <link rel="next" href="11-Integrating.html">
      
      
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>10 Policy Function Approximation - Drava's Study Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
        <script src="https://unpkg.com/iframe-worker/shim"></script>
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#10-policy-function-approximation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../index.html" title="Drava&#39;s Study Notes" class="md-header__button md-logo" aria-label="Drava's Study Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Drava's Study Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              10 Policy Function Approximation
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="pink"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../index.html" class="md-tabs__link">
        
  
  
    
  
  Drava's Study Notes

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Other/01-LC.html" class="md-tabs__link">
          
  
  
  Other

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="01-Search.html" class="md-tabs__link">
          
  
  
  Unimelb

        </a>
      </li>
    
  

    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../index.html" title="Drava&#39;s Study Notes" class="md-nav__button md-logo" aria-label="Drava's Study Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Drava's Study Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Drava's Study Notes
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Other
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Other
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Other/01-LC.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 Linear Classifier
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Unimelb
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Unimelb
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    25S2
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    25S2
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    AIP
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    AIP
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="01-Search.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 Search
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="02-Planning.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2 Planning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="03-Relaxation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3 Relaxation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="04-Exploration.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    4 Exploration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="05-RL.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    5 Reinforced Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="06-MDP.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    6 Markov Decision Process
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="07-MC.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    7 Monte Carlo
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="08-TD.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    8 Temporal Difference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="09-Value-Approx.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    9 Value Function Approximation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    10 Policy Function Approximation
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="10-Policy-Approx.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    10 Policy Function Approximation
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#value-based-policy-based" class="md-nav__link">
    <span class="md-ellipsis">
      
        Value-based → Policy-based
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Gradient
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#objective-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        Objective Function
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimise-the-object-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimise the Object Function
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deterministic-policy-stochastic-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Deterministic Policy → Stochastic Policy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deterministic Policy → Stochastic Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deterministicnear-deterministic-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Deterministic/Near-deterministic Policy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deterministic/Near-deterministic Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#-greedy" class="md-nav__link">
    <span class="md-ellipsis">
      
        ε-greedy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upper-confidence-bound-ucb" class="md-nav__link">
    <span class="md-ellipsis">
      
        Upper Confidence Bound (UCB)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stochastic Policy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stochastic Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        softmax
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gaussian" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gaussian
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#solve-the-score-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        Solve the Score Function
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#approximate-the-expected-return" class="md-nav__link">
    <span class="md-ellipsis">
      
        Approximate the Expected Return
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Approximate the Expected Return">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#average-state-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Average State Value
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-state-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Start State Value
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#average-one-step-reward" class="md-nav__link">
    <span class="md-ellipsis">
      
        Average One-step Reward
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policy-gradient-theorem" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Gradient Theorem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforce" class="md-nav__link">
    <span class="md-ellipsis">
      
        REINFORCE
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actor-Critic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Actor-Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qac" class="md-nav__link">
    <span class="md-ellipsis">
      
        QAC
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2c-advantage-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        A2C - Advantage actor-critic
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="11-Integrating.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    11 Integrating Learning and Planning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    IML
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    IML
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../IML/01-probability.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1 Probability
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    MoC
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    MoC
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MoC/01-introduction.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01 introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="10-policy-function-approximation">10 Policy Function Approximation<a class="headerlink" href="#10-policy-function-approximation" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="value-based-policy-based">Value-based → Policy-based<a class="headerlink" href="#value-based-policy-based" title="Permanent link">&para;</a></h2>
<p>In <strong>Value-based</strong> Algorithms:</p>
<ul>
<li>A policy is optimal if it can <span class="arithmatex">\(maximise\)</span> every state value</li>
</ul>
<div class="arithmatex">\[
\pi^\ast = \arg\max_\pi v_\pi(S)
\]</div>
<ul>
<li>We store all <span class="arithmatex">\(v_\pi(S)\)</span> in a tabular based on different <span class="arithmatex">\(\pi\)</span> so that we can choose best policy from them.</li>
</ul>
<p>In <strong>Policy-based</strong> Algorithms:</p>
<ul>
<li>We construct a model of <span class="arithmatex">\(\pi\)</span> and optimise by tuning its parameters <span class="arithmatex">\(\theta\)</span>.</li>
</ul>
<div class="arithmatex">\[
\pi^\ast = \pi(a\mid S,\theta)
\]</div>
<div class="admonition note">
<p class="admonition-title">Outcomes of Policy-Based Methods</p>
<p><strong>Advantages</strong></p>
<ul>
<li>High-level function approximation  </li>
<li>Much more space-efficient (no huge tabular, just a function with features/variables)  ⭐️</li>
<li>Better generalisation across large/continuous state spaces ✅ </li>
<li>Naturally supports <strong>stochastic policies</strong> ✅</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>May converge only to a <strong>local optimum</strong>  ⚠️</li>
<li>Policy evaluation suffers from <strong>high variance</strong> ⚠️</li>
<li>Monte-Carlo returns require full trajectory sampling → <strong>inefficient</strong>  </li>
<li>Difficult to use <strong>off-policy</strong> ❌</li>
<li>Updating <span class="arithmatex">\(\theta\)</span> changes the policy → changes the data distribution  </li>
<li>Requires resampling under the new policy (no reuse of old data)</li>
</ul>
</div>
<hr />
<h2 id="policy-gradient">Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permanent link">&para;</a></h2>
<ol>
<li>Use a object function to define optimal policies: <span class="arithmatex">\(J(\theta\)</span>)</li>
<li>Use gradient-based optimisation to search for optimal policies</li>
</ol>
<div class="arithmatex">\[
\theta_{t+1} = \theta + \alpha \nabla J(\theta_t)
\]</div>
<hr />
<h2 id="objective-function">Objective Function<a class="headerlink" href="#objective-function" title="Permanent link">&para;</a></h2>
<p>Goal: <span class="arithmatex">\(maximise\)</span> the <strong>expected long-term return</strong> under the policy:</p>
<div class="arithmatex">\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\, G(\tau) \,\right],
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(\tau\)</span> represents the trajectory generated by following such policy</li>
</ul>
<hr />
<h2 id="optimise-the-object-function">Optimise the Object Function<a class="headerlink" href="#optimise-the-object-function" title="Permanent link">&para;</a></h2>
<p>By using Gradient Descent:</p>
<p>$$
\theta_{k+1} = \theta_k - \alpha \nabla J(\theta_k)
$$
<span class="arithmatex">\(\nabla J(\theta_k)\)</span> can be computed by:</p>
<ul>
<li><strong>Finite Difference</strong><ul>
<li>Run one episode on <span class="arithmatex">\(\theta_k\)</span> and get <span class="arithmatex">\(J(\theta_k)\)</span></li>
<li>Slightly change <span class="arithmatex">\(\theta_k\)</span> to <span class="arithmatex">\(\theta_k + \epsilon \vec{e}_k\)</span><ul>
<li>where <span class="arithmatex">\(\epsilon \vec{e}_k\)</span> is a small noise (in vector form)</li>
</ul>
</li>
<li>Run one episode again on <span class="arithmatex">\(\theta_k + \epsilon \vec{e}_k\)</span> and get <span class="arithmatex">\(J(\theta_k + \epsilon \vec{e}_k)\)</span></li>
<li>Estimate gradient by using finite difference:</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
\nabla J(\theta) \approx 
\frac{J(\theta + \epsilon e_i) - J(\theta)}{\epsilon}
\]</div>
<ul>
<li><strong>True Gradient Descent</strong> (The Gradient-ascent Algorithm)</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
\nabla J(\theta)
&amp;= \sum_{\tau} \nabla P(\tau \mid \theta) \, G(\tau)
&amp;&amp; \text{(definition of expected return)} \\[4pt]
&amp;= \sum_{\tau} P(\tau \mid \theta) \, \nabla \log P(\tau \mid \theta) \, G(\tau)
&amp;&amp; \text{(apply log-derivative trick)} \\[4pt]
&amp;= \mathbb{E}_{\tau \sim P(\tau \mid \theta)} \big[ \nabla \log P(\tau \mid \theta) \, G(\tau) \big]
&amp;&amp; \text{(convert sum to expectation)} \\[4pt]
&amp;= \mathbb{E}_{\pi_\theta} \big[ \nabla \log \pi_\theta(A \mid S) \, G(\tau) \big]
&amp;&amp; \text{(expand trajectory likelihood)}
\end{aligned}
\]</div>
<ul>
<li><strong>Stochastic Gradient Descent</strong></li>
</ul>
<div class="arithmatex">\[
\nabla J(\theta) \approx \nabla \log \pi_\theta (a_t \mid s_t) \, G(\tau)
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(\nabla \log \pi_\theta(a \mid s)\)</span> is also called <strong>Score Function</strong></li>
</ul>
<blockquote>
<p>To compute Score Function, we can no longer use deterministic/non-linear/non-differentiable <span class="arithmatex">\(\{0,1\}\)</span> to describe policy. </p>
<p>So we introduce some <strong>soft policy</strong> strategies for making policy outputs are probabilistic.</p>
</blockquote>
<hr />
<h2 id="deterministic-policy-stochastic-policy">Deterministic Policy → Stochastic Policy<a class="headerlink" href="#deterministic-policy-stochastic-policy" title="Permanent link">&para;</a></h2>
<h3 id="deterministicnear-deterministic-policy">Deterministic/Near-deterministic Policy<a class="headerlink" href="#deterministicnear-deterministic-policy" title="Permanent link">&para;</a></h3>
<p>Give the optimal action directly:</p>
<div class="arithmatex">\[
\pi(a \mid s) = \mathbf{1}\{\, a = a^*(s) \,\}
\]</div>
<p>Or use soft policy for small explorations:</p>
<h4 id="-greedy">ε-greedy<a class="headerlink" href="#-greedy" title="Permanent link">&para;</a></h4>
<p>most time exploitation (<span class="arithmatex">\(P = 1 - \varepsilon\)</span>), little time exploration (<span class="arithmatex">\(\varepsilon\)</span>):</p>
<div class="arithmatex">\[
\pi(a \mid s) =
\begin{cases}
1 - \varepsilon + \dfrac{\varepsilon}{|\mathcal{A}|}, &amp; \text{if } a = \arg\max_{a'} Q(s,a') \\
\dfrac{\varepsilon}{|\mathcal{A}|}, &amp; \text{otherwise}
\end{cases}
\]</div>
<h4 id="upper-confidence-bound-ucb">Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Permanent link">&para;</a></h4>
<p>exploitation + exploration bonus:</p>
<div class="arithmatex">\[
a = \arg\max_a \left( Q(a) + c \sqrt{\frac{\ln t}{N(a)}} \right)
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(c\)</span> - importance of exploration (<strong>parameter</strong>)</li>
<li><span class="arithmatex">\(\ln{t}\)</span> - explore more when time goes</li>
<li><span class="arithmatex">\(N(a)\)</span> - not try too many times on one same action</li>
</ul>
<blockquote>
<p>All previous RL methods' outputs can be deterministic (The Optimum) which do not need <strong>differentiable</strong> probabilities.</p>
</blockquote>
<hr />
<h3 id="stochastic-policy">Stochastic Policy<a class="headerlink" href="#stochastic-policy" title="Permanent link">&para;</a></h3>
<p>Give the probability of each action:
$$
\pi(a \mid s) = P(a \mid s)
$$
- Ability of exploration; Robustness✅</p>
<h4 id="softmax">softmax<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\pi(a \mid s)
= \frac{\exp\!\left(h_\theta(s,a)\right)}
       {\sum_{a' \in \mathcal{A}} \exp\!\left(h_\theta(s,a')\right)}
\]</div>
<p>where <span class="arithmatex">\(h(\cdot)\)</span> is another feature function</p>
<h4 id="gaussian">Gaussian<a class="headerlink" href="#gaussian" title="Permanent link">&para;</a></h4>
<p>Action space needs to be continuous:</p>
<div class="arithmatex">\[
\pi(a \mid s)
= \frac{1}{\sqrt{2\pi\sigma_\theta^2(s)}}
  \exp\!\left(
    -\frac{\left(a-\mu_\theta(s)\right)^2}{2\sigma_\theta^2(s)}
  \right).
\]</div>
<hr />
<h2 id="solve-the-score-function">Solve the Score Function<a class="headerlink" href="#solve-the-score-function" title="Permanent link">&para;</a></h2>
<p>By applying Softmax:</p>
<div class="arithmatex">\[
\begin{align}
\nabla \log \pi_\theta(a \mid s)
&amp;= \nabla h(a) - \mathbb{E}_{a' \sim \pi_\theta}[\nabla h(a')]
\end{align}
\]</div>
<hr />
<p>By applying Gaussian (assume <span class="arithmatex">\(\sigma\)</span> is fixed):</p>
<div class="arithmatex">\[
\nabla \log \pi_\theta(a \mid s) = \frac{a - \mu_\theta(s)}{\sigma^2}\nabla \mu_\theta(s)
\]</div>
<div class="admonition warning">
<p class="admonition-title">ε-greedy CANNOT Be Used to solve the Score Function</p>
<p>The score-function estimator requires a <strong>differentiable</strong> stochastic policy so that the log-probability gradient is well-defined.</p>
<p>However, the <span class="arithmatex">\(\arg\max Q(s,a)\)</span> step used in ε-greedy is discrete and therefore <strong>non-differentiable</strong>, violating this requirement.</p>
</div>
<hr />
<h2 id="approximate-the-expected-return">Approximate the Expected Return<a class="headerlink" href="#approximate-the-expected-return" title="Permanent link">&para;</a></h2>
<h3 id="average-state-value">Average State Value<a class="headerlink" href="#average-state-value" title="Permanent link">&para;</a></h3>
<p>By <span class="arithmatex">\(maximise\)</span> weighted average of all state values:</p>
<div class="arithmatex">\[
J(\theta) 
= \sum_{s \in S}d^{\pi_\theta}(s)v^{\pi_\theta}(s)
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(d^{\pi_\theta}(s)\)</span> is the weight (importance) of each state</li>
</ul>
<hr />
<h3 id="start-state-value">Start State Value<a class="headerlink" href="#start-state-value" title="Permanent link">&para;</a></h3>
<p>In episodic environments, we may only care about value of start states:</p>
<div class="arithmatex">\[
J(\theta) 
= v^{\pi_\theta}(s_0)
\]</div>
<hr />
<h3 id="average-one-step-reward">Average One-step Reward<a class="headerlink" href="#average-one-step-reward" title="Permanent link">&para;</a></h3>
<p>By <span class="arithmatex">\(maximise\)</span> weighted average of all immediate rewards:</p>
<div class="arithmatex">\[
J(\theta) 
= \sum_{s \in S}d^{\pi_\theta}(s) 
\sum_{a \in A} \pi_\theta(a \mid s) \, R^a_s
\]</div>
<hr />
<h3 id="policy-gradient-theorem">Policy Gradient Theorem<a class="headerlink" href="#policy-gradient-theorem" title="Permanent link">&para;</a></h3>
<p>Average State Value and Average One-step Reward are <strong>Equivalent</strong>:</p>
<div class="arithmatex">\[
\bar{R}_\pi = (1 - \gamma) \, \bar{v}_\pi
\]</div>
<p>In Gradient Form:</p>
<div class="arithmatex">\[
\nabla \bar{R}_\pi \simeq { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; q_{\pi_{\theta}}(s,a) \bigr]}
\]</div>
<div class="arithmatex">\[
\nabla \bar{v}_\pi = \frac{1}{1 - \gamma} \nabla \bar{R}_\pi
\]</div>
<div class="arithmatex">\[
\nabla v_\pi(s_0) = { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s_0) \; q_{\pi_{\theta}}(s_0,a) \bigr]}
\]</div>
<p>In summary, gradient version of above methods can be simplified to just one formula:</p>
<div class="arithmatex">\[
\nabla_{\theta} J(\theta) 
= { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; q_{\pi_{\theta}}(s, a) \bigr]}
\]</div>
<blockquote>
<p>Then, we can use our familiar value approximators (MC/TD) to solve <span class="arithmatex">\(q_{\pi_{\theta}}(s, a)\)</span>.</p>
</blockquote>
<hr />
<h3 id="reinforce">REINFORCE<a class="headerlink" href="#reinforce" title="Permanent link">&para;</a></h3>
<p>Derive from MC, we can use return <span class="arithmatex">\(v_t\)</span> as an unbiased sample of <span class="arithmatex">\(q_{\pi_{\theta}}(s,a)\)</span>:</p>
<div class="arithmatex">\[
q_{\pi_{\theta}}(s,a) = v_t
\]</div>
<hr />
<h3 id="actor-critic">Actor-Critic<a class="headerlink" href="#actor-critic" title="Permanent link">&para;</a></h3>
<h4 id="qac">QAC<a class="headerlink" href="#qac" title="Permanent link">&para;</a></h4>
<p>Derive from TD, we can use a value function approximator to estimate action-value function:</p>
<div class="arithmatex">\[
q_{\pi_{\theta}}(s,a) = q(s,a, w)
\]</div>
<table>
<thead>
<tr>
<th></th>
<th>Actor</th>
<th>Critic</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Role</strong></td>
<td>Policy</td>
<td>Policy Evaluation (Value Function)</td>
</tr>
<tr>
<td><strong>Input</strong></td>
<td>Score</td>
<td>State, Reward, Action</td>
</tr>
<tr>
<td><strong>Task</strong></td>
<td>Update <span class="arithmatex">\(\theta\)</span> by Policy Gradient</td>
<td>Update <span class="arithmatex">\(w\)</span> by Value Learning</td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td>Action chosen by the policy</td>
<td>Score (Value / Advantage)</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="a2c-advantage-actor-critic">A2C - Advantage actor-critic<a class="headerlink" href="#a2c-advantage-actor-critic" title="Permanent link">&para;</a></h4>
<p>Introduce a baseline for reducing variance without changing the expectation<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>:</p>
<div class="arithmatex">\[
\begin{align}
\nabla_{\theta} J(\theta) 
&amp;= { \mathbb{E}_{\pi_{\theta}} 
\bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \;
q_{\pi_{\theta}}(s, a) \bigr]} \\[4pt]
&amp;= { \mathbb{E}_{\pi_{\theta}} 
\bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; 
\left[ q_{\pi_{\theta}}(s, a) - B(s) \right] \bigr]}
\end{align}
\]</div>
<ul>
<li>A good (<strong>sub-optimal</strong>) baseline is the state value function <span class="arithmatex">\(v_{\pi_\theta}(s)\)</span></li>
</ul>
<p>By applying it, we can get the <strong>Advantage Function</strong>:</p>
<div class="arithmatex">\[
A_{\pi_{\theta}}(s,a) =q_{\pi_{\theta}}(s, a) - v_{\pi_\theta}(s)
\]</div>
<p>Then,</p>
<div class="arithmatex">\[
\nabla_{\theta} J(\theta) 
= { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; A_{\pi_{\theta}}(s,a) \bigr]}
\]</div>
<p>Derive from TD, we can use <strong>TD error</strong> to <strong>unbiased</strong> estimate such function</p>
<div class="arithmatex">\[
\delta_{\pi_{\theta}} = r + \gamma v_{\pi_{\theta}}(s') - v_{\pi_{\theta}}(s)
\]</div>
<div class="arithmatex">\[
\begin{align}
\mathbb{E}_{\pi_{\theta}}[\delta_{\pi_{\theta}} \mid s,a]
&amp;= \mathbb{E}_{\pi_{\theta}}[r + \gamma v_{\pi_{\theta}}(s') \mid s,a] - v_{\pi_{\theta}}(s) \\[2pt]
&amp;= q^{\pi_{\theta}}(s,a) - v^{\pi_{\theta}}(s) \\[2pt]
&amp;= A^{\pi_{\theta}}(s,a)
\end{align}
\]</div>
<p>Finally, we actually just use the <strong>TD error</strong> to compute the policy gradient:</p>
<div class="arithmatex">\[
\nabla_{\theta} J(\theta)
= {\mathbb{E}_{\pi_{\theta}} \big[ \nabla_{\theta} \log \pi_{\theta}(s,a)\; \delta_{\pi_{\theta}} \big]}
\]</div>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>【第10课-Actor-Critic方法（Part2-Advantage Actor-Critic (A2C)）【强化学习的数学原理】-哔哩哔哩】 https://b23.tv/SlGApOj&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.expand", "navigation.sections", "navigation.top", "navigation.tabs", "search.highlight", "search.suggest", "toc.integrate", "content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips", "diagrams"], "search": "../../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>