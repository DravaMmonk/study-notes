# 9 Temporal Difference Learning
---
## Temporal Difference (TD) Learning
- Key Mechanisms: **bootstrap + sample**
	- Sampling: an "1-step experience" (TD(0))
		- $e = \{S_t \to A_t \to R_{t+1}, S_{t+1}\}$
	- Bootstrapping: use estimation of future values to update the current value

$$
\text{TD-target} = \text{sample} + \gamma \cdot \text{bootstrap}
$$

---
## TD (Prediction of State Value)
- For each step, if $s_t$ is visited, update its value by **looking forward** to the reward of next step(s).
- ‚ùåCan neither estimate action values nor optimal policies
### TD(0)
- Look into the next $1$ step:
	
	$$
	\begin{align}
	V(s_t) 
	&\gets V(s_t) + \alpha \big({{G_t^{(0)} - V(s_t))}} \\[2pt]
	&= V(s_t) + \alpha(R_{t+1} + \gamma V(s_{t+1}) - V(s_t))
	\end{align}
	$$

	- $\vec{v} = R_{t+1} + \gamma V(s_{t+1})$ - TD Target
	- $\delta = \vec{v} - V(s_t)$ - TD Error
		- *Reflects the difference between $v_t$ and $v_\pi$*


---
### TD(n) 
- Look into the next $n-1$ steps:

$$
G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + {{\gamma^n V(S_{t+n})}}
$$

$$
\begin{align*}
\color{red}{n} & \color{red}{= 1}\ \text{(TD(0))}      & G_t^{(1)}      & = R_{t+1} + \gamma V(S_{t+1})\\[2pt]
\color{red}{n} & \color{red}{= 2}                   & G_t^{(2)}      & = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})\\[2pt]
\color{red}{\vdots} &                   & \vdots         & \\[2pt]
\color{red}{n} & \color{red}{= \infty}\ \text{(MC)} & G_t^{(\infty)} & = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T
\end{align*}
$$

- Update in direction of error
$$
V(S_t) \;\gets\; V(S_t) + \alpha \big({{G_t^{(n)} - V(S_t)}} \big)
$$

- How to choose $n$:
	- Root Mean Square (RMS) Errors ÂùáÊñπÂ∑ÆÊ†π
		- change $\alpha$ ‚Üí vary RMS Errors ‚Üí different optimal $n$
	- Small n ‚Üí More rely on prediction, faster but higher bias
	- Large n ‚Üí More rely on exploitation, more precise but higher variance
- If $n = \infty$ , it turns into MC basic

---
### TD($\lambda$) 
- Average n-Steps Returns - use weight ${{(1 - \lambda)\lambda^{\,n-1}}}$ to balance short-term and long-term rewards
	$$
	G_t^{\lambda} \;=\; {{(1 - \lambda) \sum_{n=1}^{\infty} \lambda^{\,n-1}}} G_t^{(n)}
	$$

	$$
	V(S_t) \;\leftarrow\; V(S_t) + \alpha \big( G_t^{\lambda} - V(S_t) \big)
	$$

	- ‚úÖMemoryless - i.e space complexity is same as TD(0)
	- ‚ö†Ô∏èForward View - We cannot look into the far future during learningüëá

#### Backward View 
- backward propagation
	- state visited more times¬† = more responsible on current TD error
	- Update **online**, every step, from incomplete sequences

1. Update **Eligibility Trace**
	- For each state s, at each time step:
		- reduction by $\gamma \lambda$
		- $+1$ if currently visiting

$$
\begin{align}
E_0(s) & = 0 \\[0pt]
E_t(s) & = \gamma \lambda E_{t-1}(s) + \mathbf{1}(S_t = s)
\end{align}
$$
	
2. Compute TD Error
	$$
	\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
	$$

3. Update Value Function
	$$
	V(s) \gets V(s) + \alpha \, \delta_t \, E_t(s)
	$$

- In Batch/Offline update, Forward View = Backward View
$$
\sum_{t=1}^T \alpha \, \delta_t \, E_t(s) = \sum_{t=1}^T \alpha (G_t^\lambda  - V(S_t)) \; \mathbf{1}(S_t = S)
$$

---
## Sarsa (Prediction of Action Value)
- "State-action-reward-state-action" Algorithm
-  A state-action function version of TD

$$
Q(s_t,a_t) \gets Q(s_t,a_t) + \alpha (R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))
$$

---
### Sarsa
- For each episode:
	- For each state:
		- If $s_t \ne g$:
			- Collect the experience $\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\}$
				- $a_t$ ‚Üê $\pi(a \mid s_t)$
				- $r_{t+1}, s_{t+1}$  ‚Üê Environment
				- $a_{t+1}$  ‚Üê $\pi(a \mid s_{t+1})$
			- Update $q(s_t, a_t)$
			- Update $\pi$ based on policy iteration

---
### $n$-step Sarsa
- Sarsa version of TD(n):
$$
q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n})
$$

$$
Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t)
  + \alpha \Big( q_t^{(n)} - Q(S_t, A_t) \Big)
$$

---
### Sarsa($\lambda$)
- Sarsa version of TD($\lambda$)
$$
q_t^\lambda \;=\; (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)}
$$

- Forward View:
$$
Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t) + \alpha \Big( q_t^\lambda - Q(S_t, A_t) \Big)
$$

- Backward View:
$$
\begin{align*}
E_0(s,a) & = 0 \\[0pt]
E_t(s,a) & = \gamma \lambda E_{t-1}(s,a) + \mathbf{1}(S_t = s, A_t = a)
\end{align*}
$$

$$
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)
$$

$$
Q(s,a) \;\leftarrow\; Q(s,a) + \alpha \,\delta_t \, E_t(s,a)
$$

---
## Q-Learning (Off-Policy Control)
- **Off-policy**: Distinguish Target Policy from Behaviour Policy
	- ‚úÖTake use of experience from other policies
### On-policy vs. Off-policy
- Sarsa:
	- Target: $\vec{v} = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$
	- Behaviour: how generate $a_{t+1}$ from $s_{t+1}$
	- Target Policy = Behaviour Policy = $\pi_t$
- Q-Learning
	- Target: $\vec{v} = r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)$
	- Behaviour: how generate $a_t$ from $s_t$
	- Target Policy = Optimal (State-action-value) Policy
	- Behaviour Policy = Any policy

### Q-Learning
- For each episode generated by $\pi_b$:
	- For each time step:
		- Update q-value
			- $q_{t+1}(s_t, a_t) \gets q_t(s_t, a_t) + \alpha \bigl[  \left[r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)\right] - q_t(s_t, a_t) \bigr]$
		- Update target policy
			- $a = \arg\max_a q_{t+1} (s_t, a)$

---
## MC vs. TD

| Aspect                                     | Monte-Carlo (MC)                        | Temporal-Difference (TD)                                                                |
| ------------------------------------------ | --------------------------------------- | --------------------------------------------------------------------------------------- |
| Learning timing                            | Offline - Update after the episode ends | Online - Update immediately after receiving a reward                                    |
| Environment Assumption                     | episodic  only                          | both episodic and continuing                                                            |
| Target used                                | $\vec{v} = G_t$                         | $\vec{v} = R_{t+1} + \gamma V(S_{t+1}$)                                                 |
| Bootstrapping?                             | ‚ùå                                       | ‚úÖValue Updating relies on the previous estimate of this value (require initial guesses) |
| Variance                                   | ‚ö†Ô∏èHigh - Samples of lots of variables   | ‚úÖLow                                                                                    |
| Bias                                       | Unbiased (return is true target)        | Biased (bootstraps using V(s‚Ä≤))                                                         |
| Exploits Markov property?                  | ‚ùå                                       | ‚úÖ                                                                                       |
| Efficiency in Markov environment           | ‚ö†Ô∏èLess efficient                        | ‚úÖMore efficient                                                                         |
| Efficiency in non-Markov or PMDP env       | ‚úÖMore efficient with backup learning    | ‚ö†Ô∏èLess efficient                                                                        |
| Optimisation in limited experience (Batch) | Average Return = minimise MSE           | Fit the most likelihood Markov model that best explains the data                        |


##### Example - Driving Home

| State              | Elapsed Time (min) | Predicted Time to Go | Predicted Total Time | MC Update (After Back Home) | MC New Predicated Time | TD(0) Update - $\gamma = 1, \alpha = 0.5$ | TD New Predicated Time |     |
| ------------------ | ------------------ | -------------------- | -------------------- | --------------------------- | ---------------------- | ----------------------------------------- | ---------------------- | --- |
| leaving office     | 0                  | 30                   | 30                   | 43                          | 43-0=43                | 30+0.5(5+35-30)=35                        | 35-0=35                |     |
| reach car, raining | 5                  | 35                   | 40                   | 43                          | 43-5=38                | 40+0.5(20+15-40)=37.5                     | 37.5-5=32.5            |     |
| exit highway       | 20                 | 15                   | 35                   | 43                          | 43-20=23               | 35+0.5(30+10-35)=37.5                     | 37.5-20=17.5           |     |
| behind truck       | 30                 | 10                   | 40                   | 43                          | 43-30=13               | 40+0.5(40+3-40)=41.5                      | 41.5-30=11.5           |     |
| home street        | 40                 | 3                    | 43                   | 43                          | 43-40=3                | 43+0.5(43+0-43)=43                        | 43-40=3                |     |
| arrive home        | 43                 | 0                    | 43                   | 43                          | 43-43=0                | 43                                        | 43-43=0                |     |
|                    |                    |                      |                      |                             |                        |                                           |                        |     |
##### Example - BATCH

- 2-states: $A$, $B$
- Reward Signal = $\{0, 1\}$
- Sampled Episodes:
	- $A \to 0$, $B \to 0$, $B \to 1$, $B \to 1$, $B \to 1$, $B \to 1$, $B \to 1$, $B \to 1$, $B \to 0$
- MC Update:
	- $V(B) = 6/8 = 0.75$
	- $V(A) = 0 / 1 = 0$
- TD(0) Update:
	- $V(B) \to0.75$
	- $V(A) \to V(B) \to 0.75$

---