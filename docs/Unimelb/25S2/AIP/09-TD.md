# 9 Temporal Difference
---
## Non-incremental ‚Üí Incremental

**Non-incremental Methods(Batch)**

- Parameters updating per a **batch** of samples (a episode)
- Solving the optimal problem at once
- Offline Learning 
	- Can handle high stochastic/batch environment‚úÖ
	- Large space complexity‚ùå 
- Closed-form
	- High computation cost‚ùå

**Incremental Methods**

- Parameters updating per one sample (a step)
- Step by step approaching the optimum
- Online Learning ‚Üí One sample use once‚úÖ
- Higher generalisation‚úÖ
- Lower convergence; may can only approach local optima‚ö†Ô∏è

---
## Temporal Difference (TD) Learning

> TD is the most basic incremental method based on *Bootstrapping*.

**Bootstrapping**: Use estimation of future values to update the current value.

$$
\begin{align}
\text{TD-target} & = \text{sample} + \gamma \cdot \text{bootstrap} \\
& = \text{immediate reward} + \gamma \cdot \text{future value}
\end{align}
$$

---
## Prediction of State Value

For each step, if $s_t$ is visited, update its value by **looking forward** to the reward of next step(s).

- **CANNOT** estimate action values or optimal policies ‚ùå

### TD(0)

Look into the next $1$ step:
	
$$
\begin{align}
V(s_t) 
&\gets V(s_t) + \alpha \big({{G_t^{(1)} - V(s_t))}} \\[2pt]
&= V(s_t) + \alpha(R_{t+1} + \gamma V(s_{t+1}) - V(s_t))
\end{align}
$$

where

- $\vec{v} = R_{t+1} + \gamma V(s_{t+1})$ - TD Target
- $\delta = \vec{v} - V(s_t)$ - TD Error
	- *Reflects the difference between $v_t$ and $v_\pi$*


---
### TD(n) 

Look into $n-1$ steps:

$$
G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + {{\gamma^n V(S_{t+n})}}
$$

$$
\begin{align*}
\color{red}{n} & \color{red}{= 1}\ \text{(TD(0))}      & G_t^{(1)}      & = R_{t+1} + \gamma V(S_{t+1})\\[2pt]
\color{red}{n} & \color{red}{= 2}                   & G_t^{(2)}      & = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})\\[2pt]
\color{red}{\vdots} &                   & \vdots         & \\[2pt]
\color{red}{n} & \color{red}{= \infty}\ \text{(MC)} & G_t^{(\infty)} & = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T
\end{align*}
$$

Update in direction of error:

$$
V(S_t) \;\gets\; V(S_t) + \alpha \big({{G_t^{(n)} - V(S_t)}} \big)
$$


> [!tip]- How to choose a good $n$
> 
> - Root Mean Square (RMS) Errors
> 	- change $\alpha$ ‚Üí vary RMS Errors ‚Üí different optimal $n$
> - Large $n$ ‚Üí More rely on exploitation, more precise but higher variance
> - Small $n$ ‚Üí More rely on prediction, faster but higher bias
> - If $n = \infty$ , it turns into MC basic

---
### TD(Œª)

Average n-Steps Returns - use weight ${{(1 - \lambda)\lambda^{\,n-1}}}$ to balance short-term and long-term rewards

$$
G_t^{\lambda} \;=\; {{(1 - \lambda) \sum_{n=1}^{\infty} \lambda^{\,n-1}}} G_t^{(n)}
$$

$$
V(S_t) \;\leftarrow\; V(S_t) + \alpha \big( G_t^{\lambda} - V(S_t) \big)
$$

> [!NOTE] Outcomes: TD(Œª)
> 
> - ‚úÖMemoryless - i.e space complexity is same as TD(0)
> - ‚ö†Ô∏èForward View - We cannot look into the far future during learningüëá

#### Backward View 

Update weight step by step by using **Eligibility Trace**:

- Initially set traces of all states $E_0(s) \gets 0$
- In each step:
	 - For each state:
		- reduce traces value by $\gamma \lambda$
		- $E_t(s)+1$ if $s$ is currently visited

		$$
		E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbf{1}(S_t = s)
		$$
	
	- Compute TD Error
	
		$$
		\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
		$$

	- Update Value Function

		$$
		V(s) \gets V(s) + \alpha \, \delta_t \, E_t(s)
		$$


At the end of episodes: **Backward View = Forward View**

$$
\sum_{t=1}^T \alpha \, \delta_t \, E_t(s) = \sum_{t=1}^T \alpha (G_t^\lambda  - V(S_t)) \; \mathbf{1}(S_t = S)
$$


---
## Prediction of Action Value

**Sarsa** : "State-action-reward-state-action" Algorithm

>  A state-action function version of TD

$$
Q(s_t,a_t) \gets Q(s_t,a_t) + \alpha (R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))
$$

---
### Sarsa

- For each episode:
	- For each state:
		- If $s_t \ne g$:
			- Collect the experience $\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\}$
				- $a_t$ ‚Üê $\pi(a \mid s_t)$
				- $r_{t+1}, s_{t+1}$  ‚Üê Environment
				- $a_{t+1}$  ‚Üê $\pi(a \mid s_{t+1})$
			- Update $q(s_t, a_t)$
			- Update $\pi$ based on policy iteration

---
### $n$-step Sarsa

> Sarsa version of TD(n)

$$
q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n})
$$

$$
Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t)
  + \alpha \Big( q_t^{(n)} - Q(S_t, A_t) \Big)
$$

---
### Sarsa($\lambda$)

> Sarsa version of TD($\lambda$)

$$
q_t^\lambda \;=\; (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)}
$$

- Forward View:
$$
Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t) + \alpha \Big( q_t^\lambda - Q(S_t, A_t) \Big)
$$

- Backward View:

$$
\begin{align*}
E_0(s,a) & = 0 \\[0pt]
E_t(s,a) & = \gamma \lambda E_{t-1}(s,a) + \mathbf{1}(S_t = s, A_t = a)
\end{align*}
$$

$$
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)
$$

$$
Q(s,a) \;\leftarrow\; Q(s,a) + \alpha \,\delta_t \, E_t(s,a)
$$

---
### Off-Policy Control

**Off-policy**: Distinguish Target Policy and Behaviour (Sampling) Policy

- ‚úÖTake use of experience from other policies

### On-policy ‚Üí Off-policy

Sarsa (On-policy)

- Target: $\vec{v} = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})$
- Sample: Generate $a_t$ from $\pi_t(s_t)$ and $a_{t+1}$ from $\pi_t(s_{t+1})$
- Target Policy = Behaviour Policy = $\pi_t$ 

Q-Learning (Off-policy)

- Target: $\vec{v} = r_{t+1} + \gamma \max_{a \in \mathcal{A}} q_t(s_{t+1}, a)$
- Sample: Generate $a_t$ from $\pi_b(s_t)$
- Target Policy = Greedy Optimal $q_t$
- Behaviour Policy = Any policies ($\pi_b$)

### Q-Learning

- For each episode generated by $\pi_b$:
	- For each time step:
		- Update q-value
			- $q_{t+1}(s_t, a_t) \gets q_t(s_t, a_t) + \alpha \bigl(  \vec v - q_t(s_t, a_t) \bigr)$
		- Update target policy
			- $a = \arg\max_a q_{t+1} (s_t, a)$

---
## MC vs. TD

| Aspect                                     | Monte-Carlo (MC)                        | Temporal-Difference (TD)                                                                |
| ------------------------------------------ | --------------------------------------- | --------------------------------------------------------------------------------------- |
| Learning timing                            | Offline - Update after the episode ends | Online - Update immediately after receiving a reward                                    |
| Environment Assumption                     | episodic  only                          | both episodic and continuing                                                            |
| Target used                                | $\vec{v} = G_t$                         | $\vec{v} = R_{t+1} + \gamma V(S_{t+1}$)                                                 |
| Bootstrapping?                             | ‚ùå                                       | ‚úÖValue Updating relies on the previous estimate of this value (require initial guesses) |
| Variance                                   | ‚ö†Ô∏èHigh - Samples of lots of variables   | ‚úÖLow                                                                                    |
| Bias                                       | Unbiased (return is true target)        | Biased (bootstraps using V(s‚Ä≤))                                                         |
| Exploits Markov property?                  | ‚ùå                                       | ‚úÖ                                                                                       |
| Efficiency in Markov environment           | ‚ö†Ô∏èLess efficient                        | ‚úÖMore efficient                                                                         |
| Efficiency in non-Markov or PMDP env       | ‚úÖMore efficient with backup learning    | ‚ö†Ô∏èLess efficient                                                                        |
| Optimisation in limited experience (Batch) | Average Return = minimise MSE           | Fit the most likelihood Markov model that best explains the data                        |


##### Example - Driving Home

| State              | Elapsed Time (min) | Predicted Time to Go | Predicted Total Time | MC Update (After Back Home) | MC New Predicated Time | TD(0) Update - $\gamma = 1, \alpha = 0.5$ | TD New Predicated Time |
| ------------------ | ------------------ | -------------------- | -------------------- | --------------------------- | ---------------------- | ----------------------------------------- | ---------------------- |
| leaving office     | 0                  | 30                   | 30                   | 43                          | 43-0=43                | 30+0.5(5+35-30)=35                        | 35-0=35                |
| reach car, raining | 5                  | 35                   | 40                   | 43                          | 43-5=38                | 40+0.5(20+15-40)=37.5                     | 37.5-5=32.5            |
| exit highway       | 20                 | 15                   | 35                   | 43                          | 43-20=23               | 35+0.5(30+10-35)=37.5                     | 37.5-20=17.5           |
| behind truck       | 30                 | 10                   | 40                   | 43                          | 43-30=13               | 40+0.5(40+3-40)=41.5                      | 41.5-30=11.5           |
| home street        | 40                 | 3                    | 43                   | 43                          | 43-40=3                | 43+0.5(43+0-43)=43                        | 43-40=3                |
| arrive home        | 43                 | 0                    | 43                   | 43                          | 43-43=0                | 43                                        | 43-43=0                |

---