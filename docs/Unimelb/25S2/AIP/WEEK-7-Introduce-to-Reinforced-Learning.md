<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [WEEK 7 Introduce to Reinforced Learning](#week-7-introduce-to-reinforced-learning)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# WEEK 7 Introduce to Reinforced Learning
- Map Situations to Actions - so as to $maximise$ the Rewards

**Different from Automated Planning**
- Outcomes of actions are non-deterministic
- Use probabilistic presentation on environment & policy

**Different from Other ML**
- Sequential Data - Time Series (non-i.i.d)
- Trial-and-error Search - No supervisor, only signal from environment
- Delayed Reward - credit assignment matters

**Scenario**
- Environment is initially **unknown**
- Agent **interacts** with environment
- Agent improves its policy through prediction, planning, control and approximation

**Components of the environment**

**State**

- â’¶ State is Markov
- **State = f(History)**Â 

- Markov property ğ‘ƒ(ğ˜´â€™ | ğ˜´+ğ»(ğ˜´), ğ‘) = ğ‘ƒ(ğ˜´â€™ | ğ˜´, ğ‘)
- Once S is known, H can be thrown away


**Rewards**Â 

- A scalar feedback signal
- Indicates how well agent is doing at one step
- **Reward** **Hypothesis** _All_Â goals can be described by the maximisation of expected cumulative reward.

  

  

**Components of an RL Agent**

**Two Key ProblemsÂ  of RL**

- **Prediction**

- Evaluate the future rewards of state-actions â†’ Value Function

- **Control**

- Find the optimal policy â†’ Policy Function

  

**Model**

- Simulator of the environment based on observations of the agent
- Not necessary â†’ Model-based/Model-free
- **Probability Function**Â 

- ğ“Ÿ(ğ˜´, ğ‘, ğ˜´â€™) = ğ‘ƒ(ğ˜´â€™ | ğ˜´, ğ‘)

- **Reward Function**

- ğ“¡(ğ‘, ğ˜´) â€‰=â€‰ ğ”¼[ğ˜™â€™ | ğ˜´, ğ‘]

  

**Value Function**Â 

- Not necessary â†’ Value-based/Policy-based
- Define and predict value of the state and future reward

- ğ‘‰(ğ˜´) = ğ”¼[ğº | ğ˜´] Â 
- Value of State = The Expect of future rewards ğº
- **Bellman Equation** O(nÂ³)

- ğº = ğ˜™ + ğ›¾ğºâ€™
- ğ›¾ = discount factor

- â†’ ğ‘‰ = ğ“¡ + ğ›¾ğ“Ÿğ‘‰ â†’ ğ‘‰ = (ğ¼ - ğ›¾ğ“Ÿ)â»Â¹ğ“¡

  

**Policy Function**

- **Output:** agentâ€™s behaviour = next step action ğ‘
- **No Policy - Brute Force**

- ğ‘ = ğšğ«ğ  ğ¦ğšğ± ğ‘„(ğ˜´, ğ‘) = ğšğ«ğ  ğ¦ğšğ± ğšº ğ“Ÿ(ğ˜´, ğ‘, ğ˜´â€™) Â· ğ‘‰(ğ˜´â€™)
- Compare expectations of all valid actions

- **Deterministic policy**Â 

- ğ…(ğ˜´) = ğ‘Â 

- **Stochastic Policy**Â 

- ğ…(ğ‘ | ğ˜´) = ğ‘ƒ(ğ‘ | ğ˜´)
- Choose the optimal action or random one with a small probability
- + Ability of exploration; Robustness

- **Policy Gradient**

- ğ‰(ğœƒ) = ğ”¼[ğº] = ğšº ğ‘ƒ(ğœ | ğœƒ)ğº(ğœ)

- ğœ = A full path from initial state to the goal

- âˆ‡ğ‰(ğœƒ) = ğšº âˆ‡ğ‘ƒ(ğœ | ğœƒ)ğº(ğœ)

- â† âˆ‡ğ‘ƒ(ğœ | ğœƒ) = ğ‘ƒ(ğœ | ğœƒ)âˆ‡ã’ğ‘ƒ(ğœ | ğœƒ) (score function trick)
- = ğ”¼[âˆ‡ã’ğ‘ƒ(ğœ | ğœƒ)ğº(ğœ)]
- â† ğ‘ƒ(ğœ | ğœƒ)ğº(ğœ) = ğšº ğœ‹(ğ‘ | ğ˜´)ğº
- = ğ”¼[âˆ‡ã’ğœ‹(ğ‘ | ğ˜´)ğº]

- Maintain a set of parameters for maximising future rewards

  

**Learning Process**

Environment â†’ State, Reward â†’ Agent â†’ Update the previous policy â†’ Make theÂ  Decision â†’ Environment

  

  

**Categorise of RL agents**

- **Model Based** - Simulate environment to gain first experienceÂ 
- **Model Free** - Gain experience from real interaction directly

  

- **Value Based** - No Policy
- **Policy Based** - No Value Function = Policy Gradient
- **Actor-Critic** - Policy Based æ¼”å‘˜ + Value Based è¯„è®ºå®¶

  

  

**Exploration vs Exploitation Trade-off**

- **Exploitation** - select currently-known best action
- **Exploration** - try a new action

**Trade-off Strategies**

- **Îµ-greedy** all most all time (1-Îµ) exploitation (Îµ is very small)
- **softmax** set a probability to each action and a temperature Ï„ to controlÂ 

- P(a) = exp(Q_a/Ï„)/Î£exp(Q_others/Ï„)

- **Upper Confidence Bound (UCB)** exploitation + exploration bonus

- a= argmax(QÌ‚(a) + câˆš(ln t/N(a)))
- **c** importance of exploration (**parameter**)
- **ln t** explore more when time goes
- **N(a)** not try too many times on same action