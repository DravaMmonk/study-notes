# 8 Model Free Reinforced Learning
---
## Monte Carlo (MC) Learning
- Set the state as initial state and run-to-end in the environment for multiple times
- Calculate the **average reward** of them: _value = mean return_
- *Note: can only apply to **episodic** MDPs ï¼ˆæœ‰ç»ˆæ­¢çŠ¶æ€çš„Markov Decision Process)*

### MC Policy Evaluation
$$
V_\pi(s) = \mathbb{E}\left[G \mid s \right]
$$

- Compute empirical mean from current time, using as many samples as we can
- **First-Visit MC** Counting reward of only first visit of state s in each episode (from start to goal)
- **Every-Visit MC** Counting reward of every visit of s in each episode
- Approximate Vğœ‹(s) finally but often slow and high square error

  

**Temporal Difference (TD) Learning - bootstrap + sample**

- **Lively update Value of state** by â€œreward of current step + evaluation of next stepâ€

- Donâ€™t need to wait for the termination like MC, just approximate step by step

  

**TD(0) - Simplest TD Learning Algorithm**

- V(s) â† V(s) + ğ›¼[R + ğ›¾V(sâ€²) âˆ’ V(s)]

- **V** = value function
- **R** = reward
- ğœ¸ = discount factor (importance of next step reward)
- **R +** ğœ¸**V(sâ€²) âˆ’ V(s)** = ğœ¹ =TD error (if we under/over-evaluate V(s))
- ğœ¶ = learning rate (length of each step) for normalisation
- **V(s) â† V(s) +** ğœ¶ Â· ğœ¹ = Compromise of old V(s) and Error correction

  

**TD(n) - Look more steps into the future**

- G(t; n) = R(t+1) **+** ğœ¸R(t+2) + ğœ¸Â²R(t+3) + ... + ğœ¸â¿â»Â¹R(t+n) + ğœ¸â¿V(S(t+n))
- V(S(t)) â† V(S(t)) + ğ›¼(G(t;n) - V(S(t))

- When n â†’ âˆ, TD(n) = MC

- How to choose n:Â 

- Root Mean Square (RMS) Errors å‡æ–¹å·®æ ¹ â† ğ›¼**;** online/offline updates (ğ›¼ â†’ 0 or ğ›¼ â†’ 1)
- RMS Errors â†’ different optimal choices of n
- Small n â†’ More rely on prediction, faster but higher bias
- Large n â†’ More rely on exploitation, more precise but higher variance

**TD(**ğ€**) - Average 1,2,â€¦,n-Step Returns**

- G(t; ğœ†) = (1 - ğœ†) Â· ğ›´ ğœ†â¿â»Â¹ G(t; n)

- When ğœ† = 1, TD(ğœ†) = MC; ğœ† = 0, TD(ğœ†) = TD(0)

- Using weight (1 - ğœ†) Â· ğœ†â¿â»Â¹ to balance short-term and long-term rewards

  

  

**Eligibility Traces - Simplify calculation of G(t;** ğ€**)**

- Define a **trace** E(t; s) of every state = How many time visited and how far from current state

- E(t; s) â€‰=â€‰ ğ›¾ğœ†E(t-1; s) + _1 if S(t)â€‰=â€‰s else 0_

- If we visit the state, itâ€™s trace goes up suddenly (by 1), if we donâ€™t visit it, it falls down continuously

- V(s) â† V(s) + ğ›¼ Â· ğ›¿ Â· E(t; s)

  

**Forward View & Backward View**

- Original way calculating G(t; ğœ†) is a **Forward View** of ğœ†-Reward, we have to wait episodes terminate so that we can calculate all G(t; n) (offline)
- Eligibility Traces is a **Backward View** of ğœ†-Reward, we update prediction of state step by step(online)

- Backward Propagation - Use Eligibility Traces to propagate TD error ğ›¿

- state visited more timesÂ  = more responsible on current TD error

- **Forward View = Backward View (in Batch/offline update)**

  

  

**MC vs. TD**

- TD can learn without knowing the final outcome

- TD can learn **online** after every step
- MC must wait until episode ends.Â  (offline)
- TD can learn from incomplete sequences
- MC can only learn from complete sequences
- TD works in continuing (non-terminating) environment
- MC only works for episodic (terminating) environmentÂ 

- Bias vs. Variance Trade-Off

- MC based on rewards after terminate **unbiased**
- True TD target is an **unbiased** estimated of V(s)
- Predicted TD target use V(sâ€™) (next step value) to estimate Vğœ‹(s) (total value) â†’ **biased**
- MC bias0 varianceâ†‘
- TD biasâ†‘ varianceâ†“

- TD exploits Markov property while MC doesnâ€™t

- TD more efficient in Markov environment
- MC more effective in non-Markov environment

- **Partial observability & non-stationarity** â†’ Environment can be non-Markov

- Bootstrapping and Sampling

- TD samples and bootstraps; MC samples but does not bootstrap
- Tree Search (with heuristic search)/DP bootstraps but does not sample

  

  

**Monte Carlo Tree Search (MCTS) - planning with a model**

- Selection â†’ **Expansion** â†’ **Simulation** â†’ Back-propagationÂ 

  

Â **Multi-arm Bandits - UCB**