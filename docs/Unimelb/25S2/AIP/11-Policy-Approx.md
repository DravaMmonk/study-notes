# 11 Policy Function Approximation
---
## Value-based → Policy-based

In **Value-based** Algorithms:

- A policy is optimal if it can $maximise$ every state value

$$
\pi^\ast = \arg\max_\pi v_\pi(S)
$$

- We store all $v_\pi(S)$ in a tabular based on different $\pi$ so that we can choose best policy from them.

In **Policy-based** Algorithms:

- We construct a model of $\pi$ and optimise by tuning its parameters $\theta$.

$$
\pi^\ast = \pi(a\mid S,\theta)
$$



- Higher Level approximation✅
	- Higher space efficiency✅
	- Higher generalisability✅
	- Can learn stochastic policies✅
- May converges to a local optimum⚠️
- Policy evaluation is inefficient and high-variance❌
	- Sampling looks for the whole trace - inefficient❌
	- Off-policy unavailable ($\theta$ changed → data distribution changed → need new sampling)❌


---
## Policy Gradient

1. Use a object function to define optimal policies: $J(\theta$)
2. Use gradient-based optimisation to search for optimal policies

$$
\theta_{t+1} = \theta + \alpha \nabla J(\theta_t)
$$

---
## Objective Function

Goal: $maximise$ the **expected long-term return** under the policy:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\, G(\tau) \,\right],
$$

where

- $\tau$ represents the trajectory generated by following such policy

---
##  Optimise the Object Function

By using Gradient Descent:

$$
\theta_{k+1} = \theta_k - \alpha \nabla J(\theta_k)
$$
$\nabla J(\theta_k)$ can be computed by:

- **Finite Difference**
	- Run one episode on $\theta_k$ and get $J(\theta_k)$
	- Slightly change $\theta_k$ to $\theta_k + \epsilon \vec{e}_k$
		- where $\epsilon \vec{e}_k$ is a small noise (in vector form)
	- Run one episode again on $\theta_k + \epsilon \vec{e}_k$ and get $J(\theta_k + \epsilon \vec{e}_k)$
	- Estimate gradient by using finite difference:

$$
\nabla J(\theta) \approx 
\frac{J(\theta + \epsilon e_i) - J(\theta)}{\epsilon}
$$

- **True Gradient Descent** (The Gradient-ascent Algorithm)

$$
\begin{aligned}
\nabla J(\theta)
&= \sum_{\tau} \nabla P(\tau \mid \theta) \, G(\tau)
&& \text{(definition of expected return)} \\[4pt]
&= \sum_{\tau} P(\tau \mid \theta) \, \nabla \log P(\tau \mid \theta) \, G(\tau)
&& \text{(apply log-derivative trick)} \\[4pt]
&= \mathbb{E}_{\tau \sim P(\tau \mid \theta)} \big[ \nabla \log P(\tau \mid \theta) \, G(\tau) \big]
&& \text{(convert sum to expectation)} \\[4pt]
&= \mathbb{E}_{\pi_\theta} \big[ \nabla \log \pi_\theta(A \mid S) \, G(\tau) \big]
&& \text{(expand trajectory likelihood)}
\end{aligned}
$$

- **Stochastic Gradient Descent**

$$
\nabla J(\theta) \approx \nabla \log \pi_\theta (a_t \mid s_t) \, G(\tau)
$$

where

- $\nabla \log \pi_\theta(a \mid s)$ is also called **Score Function**

> [!NOTE]
> To compute Score Function, we can no longer use deterministic/non-linear/non-differentiable $\{0,1\}$ to describe policy. 
> 
> So we introduce some soft strategies to make policy probabilistic.

---
## Deterministic Policy → Stochastic/Soft Policy
### Deterministic/Near-deterministic Policy 

Give the optimal action directly:

$$
\pi(a \mid s) = \mathbf{1}\{\, a = a^*(s) \,\}
$$

Or use soft policy for small explorations:

#### ε-greedy 

most time exploitation ($P = 1 - \varepsilon$), little time exploration ($\varepsilon$):

$$
\pi(a \mid s) =
\begin{cases}
1 - \varepsilon + \dfrac{\varepsilon}{|\mathcal{A}|}, & \text{if } a = \arg\max_{a'} Q(s,a') \\
\dfrac{\varepsilon}{|\mathcal{A}|}, & \text{otherwise}
\end{cases}
$$


- All previous RL methods' outputs are deterministic (The Optimum).

#### Upper Confidence Bound (UCB) 

exploitation + exploration bonus:

$$
a = \arg\max_a \left( Q(a) + c \sqrt{\frac{\ln t}{N(a)}} \right)
$$

where

- $c$ - importance of exploration (**parameter**)
- $\ln{t}$ - explore more when time goes
- $N(a)$ - not try too many times on one same action

---
### Stochastic Policy 

Give the probability of each action:
$$
\pi(a \mid s) = P(a \mid s)
$$
- Ability of exploration; Robustness✅


 #### softmax 

$$
\pi(a \mid s)
= \frac{\exp\!\left(h_\theta(s,a)\right)}
       {\sum_{a' \in \mathcal{A}} \exp\!\left(h_\theta(s,a')\right)}
$$

where $h(\cdot)$ is another feature function

#### Gaussian

Action space needs to be continuous:

$$
\pi(a \mid s)
= \frac{1}{\sqrt{2\pi\sigma_\theta^2(s)}}
  \exp\!\left(
    -\frac{\left(a-\mu_\theta(s)\right)^2}{2\sigma_\theta^2(s)}
  \right).
$$

---
## Solve the Score Function

By applying Softmax:

$$
\begin{align}
\nabla \log \pi_\theta(a \mid s)
&= \nabla h(a) - \mathbb{E}_{a' \sim \pi_\theta}[\nabla h(a')]
\end{align}
$$

By applying Gaussian (assume $\sigma$ is fixed):

$$
\nabla \log \pi_\theta(a \mid s) = \frac{a - \mu_\theta(s)}{\sigma^2}\nabla \mu_\theta(s)
$$

---
## Approximate the Expected Return
### Average State Value

By $maximise$ weighted average of all state values:

$$
J(\theta) 
= \sum_{s \in S}d^{\pi_\theta}(s)v^{\pi_\theta}(s)
$$

where

- $d^{\pi_\theta}(s)$ is the weight (importance) of each state

---
### Start State Value
- In episodic environments, we may only care about value of start states:

$$
J(\theta) 
= v^{\pi_\theta}(s_0)
$$

---
### Average One-step Reward

By $maximise$ weighted average of all immediate rewards:

$$
J(\theta) 
= \sum_{s \in S}d^{\pi_\theta}(s) 
\sum_{a \in A} \pi_\theta(a \mid s) \, R^a_s
$$

---
### Policy Gradient Theorem

Average State Value and Average One-step Reward are **Equivalent**:

$$
\bar{R}_\pi = (1 - \gamma) \, \bar{v}_\pi
$$

In Gradient Form:

$$
\nabla \bar{R}_\pi \simeq { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; q_{\pi_{\theta}}(s,a) \bigr]}
$$

$$
\nabla \bar{v}_\pi = \frac{1}{1 - \gamma} \nabla \bar{R}_\pi
$$

$$
\nabla v_\pi(s_0) = { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s_0) \; q_{\pi_{\theta}}(s_0,a) \bigr]}
$$


In summary, gradient version of above methods can be simplified to one formula:

$$
\nabla_{\theta} J(\theta) 
= { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; q_{\pi_{\theta}}(s, a) \bigr]}
$$

Then, we can use our familiar MC or value approximators to solve it.

---
### REINFORCE

Derive from MC, we can use return $v_t$ as an unbiased sample of $q_{\pi_{\theta}}(s,a)$:

$$
q_{\pi_{\theta}}(s,a) = v_t
$$

---
### Actor-Critic

#### QAC

Derive from TD, we can use a value approximator to estimate action-value function:

$$
q_{\pi_{\theta}}(s,a) = q(s,a, w)
$$

|            | Actor                              | Critic                             |
| ---------- | ---------------------------------- | ---------------------------------- |
| **Role**   | Policy                             | Policy Evaluation (Value Function) |
| **Input**  | Score                              | State, Reward, Action              |
| **Task**   | Update $\theta$ by Policy Gradient | Update $w$ by Value Learning       |
| **Output** | Action chosen by the policy        | Score (Value / Advantage)          |

---
#### A2C - Advantage actor-critic

Introduce a baseline for reducing variance without changing the expectation[^1]:

$$
\begin{align}
\nabla_{\theta} J(\theta) 
&= { \mathbb{E}_{\pi_{\theta}} 
\bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \;
q_{\pi_{\theta}}(s, a) \bigr]} \\[4pt]
&= { \mathbb{E}_{\pi_{\theta}} 
\bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; 
\left[ q_{\pi_{\theta}}(s, a) - B(s) \right] \bigr]}
\end{align}
$$

- A good (**sub-optimal**) baseline is the state value function $v_{\pi_\theta}(s)$

By applying it, we can get the **Advantage Function**:

$$
A_{\pi_{\theta}}(s,a) =q_{\pi_{\theta}}(s, a) - v_{\pi_\theta}(s)
$$

Then,

$$
\nabla_{\theta} J(\theta) 
= { \mathbb{E}_{\pi_{\theta}} \bigl[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) \; A_{\pi_{\theta}}(s,a) \bigr]}
$$

Derive from TD, we can use **TD error** to **unbiased** estimate such function

$$
\delta_{\pi_{\theta}} = r + \gamma v_{\pi_{\theta}}(s') - v_{\pi_{\theta}}(s)
$$

$$
\begin{align}
\mathbb{E}_{\pi_{\theta}}[\delta_{\pi_{\theta}} \mid s,a]
&= \mathbb{E}_{\pi_{\theta}}[r + \gamma v_{\pi_{\theta}}(s') \mid s,a] - v_{\pi_{\theta}}(s) \\[2pt]
&= q^{\pi_{\theta}}(s,a) - v^{\pi_{\theta}}(s) \\[2pt]
&= A^{\pi_{\theta}}(s,a)
\end{align}
$$

Finally, we in fact use the TD error to compute the policy gradient:

$$
\nabla_{\theta} J(\theta)
= {\mathbb{E}_{\pi_{\theta}} \big[ \nabla_{\theta} \log \pi_{\theta}(s,a)\; \delta_{\pi_{\theta}} \big]}
$$

[^1]: 【第10课-Actor-Critic方法（Part2-Advantage Actor-Critic (A2C)）【强化学习的数学原理】-哔哩哔哩】 https://b23.tv/SlGApOj
