# 8 Monte Carlo
---
## Model-based â†’ Model-free

Model-based

- Learn from **Simulator** of Environment
	- Commonly use MDP to model the Markov Environment

Model-free

- Learn **Experience** from Environment
	- Experience: $E_t = \langle S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, ... \rangle$
- Optimise $V, Q, \pi$ directly

---
## Monte Carlo (MC) Learning

1. Run-to-end in the environment for multiple times
2. Use **Average Actual Return** to **estimate** value function

$$
V(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_t^{(i)}
$$

where

- $N(s)$ - how many times $G_t$ is visited
- When $N(s) \to \infty$, $V(s) \to v_\pi(s)$.

> [!note] Outcomes: Monte Carlo
> 
> - Unbiasedâœ…
> - Can only apply to **episodic** (terminating) Markov environmentâš ï¸

---
## MC Policy Iteration

Directly transfer Policy Iteration into a Monte-Carlo version.

**Algorithm**

- For each iteration:
	1. Policy Evaluation (Predication)
		- For each $(s, a)$:
			- run **sufficiently many** episodes and get average return
			- When $N \to \infty$, $Q(s, a) \to q_\pi(s, a)$
	2. Policy Improvement (Control)
		- Give the Greedy Optimal Policy based on estimated $Q$
			- $a^\ast = \arg\max_a q_\pi(s,a)$

> [!note] Outcomes: MCPI
> 
> - Convergence guaranteeâœ… 
> - Very low Time Efficiency; Not practicalâŒ


---
## MC Exploring Starts

**Generalised Policy Iteration (GPI)**: Repeatedly interleave evaluation and improvement, instead of doing them once.

**Exploring Starts**: Randomly select start points to avoid missing the optimal episode.

**Algorithm**

- Repeat:
	- Start from a random $(s_0, a_0)$
	- For each $(s,a)$ visited:
		- $N(s,a) \gets N(s,a) + 1$
		- Update $Q(s,a) \gets \frac{1}{N(s,a)}(Q(s,a) + (G - Q(s,a))$
		- If $Q$ is updated, update $a^\ast$

> [!note] Outcomes: ES-MC
> 
> - Data is used much more efficiently
> - **Exploring Start** can highly guarantee that every node is visited
> - However difficult in practice (limitation in environment)

### First-visit vs. Every-visit

Different ways to handle **repeated visits** of the same $(s,a)$ pair during an episode:

| Aspect            | First-Visit MC                            | Every-Visit MC                          |
| ----------------- | ----------------------------------------- | --------------------------------------- |
| When?             | Only the *first* time visit               | *Every* time visit                      |
| Pros              | Avoids dependence between multiple visits | Faster convergence in practice          |
| Cons              | Some visits ignored                       | Higher variance                         |
| Suitable Scenario | Long episodes with repeated states        | Short episodes or rare-state situations |

---
## MC + Îµ-greedy

Use a soft policy (e.g., Îµ-greedy) to avoid the need for exploring starts.

$$
\pi(a|s) =
\begin{cases}
1 - (1 - \frac{1}{|\mathcal{A}(s)|}) \, \varepsilon, & a = a^*_k \\[6pt]
\frac{1}{|\mathcal{A}(s)|} \, \varepsilon, & \text{otherwise}
\end{cases}
$$
	
where

- $|\mathcal{A}(s)|$ - number of valid actions of state
- Balance Exploration & Exploitation

> [!note] Outcomes: Îµ-greedy MC
> 
> - Ensures continual exploration by assigning non-zero probability to all actions. âœ…
> - More stable than pure greedy MC because it prevents being trapped in suboptimal actions early. â­ï¸
> - Slight performance trade-off: exploration introduces variance, but improves long-term value estimation and policy quality. ðŸ’­

---