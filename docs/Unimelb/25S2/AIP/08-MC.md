# 8 Monte Carlo Learning
---
## Model-based → Model-free
- Learn from: Principle → Experience

## Monte Carlo (MC) Motivation
- Run-to-end in the environment for multiple times
- Use **average return** to **estimate** value function, *instead of solving $v(n)$ in Model-based RL*

	$$
	V(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_t^{(i)}
	$$
	
	- $N(s)$ - how many times $G_t$ is visited
	- When $N(s) \to \infty$, $V(s) \to v_\pi(s)$.

- Unbiased✅
- Can only apply to **episodic** 有终止状态 Markov environment⚠️

---
## MC Basic
- Use MC motivation to transfer Policy Iteration into a Model-free method
- For each iteration:
	1. Policy Evaluation (Predication)
		- For each $(s, a)$:
			- run **sufficiently many** episodes and get average return
			- When $N \to \infty$, $Q(s, a) \to q_\pi(s, a)$
	2. Policy Improvement (Control)
		- Give the Greedy Optimal Policy based on estimated $Q$
			- $a^\ast = \arg\max_a q_\pi(s,a)$
		
- Convergence guarantee✅ Very low Time Efficiency; Not practical❌


---
## MC Exploring Starts
- Use *Generalised Policy Iteration* (GPI) Framework to improve efficiency
	- In an iteration, instead of do "Policy Evaluation → Policy Improvement" **once**, do the sub-version of it **repeatly**
- For each episode $e$:
	- Randomly pick one $(s, a)$ and start learning (**Exploring Start**)
	- For each $(s,a)$ visited:
		- $N(s,a) \gets N(s,a) + 1$
		- Update $Q(s,a) \gets \frac{1}{N(s,a)}(Q(s,a) + (G - Q(s,a))$
		- If $Q$ is updated, update $a^\ast$
- Data is used much more efficiently
- **Exploring Start** can highly guarantee that every node is visited
	- However difficult in practice (limitation in environment)
#### First-visit vs. Every-visit
- How to deal with repeat visit of same $(s,a)$ in a episode

| Aspect            | First-Visit MC                            | Every-Visit MC                          |
| ----------------- | ----------------------------------------- | --------------------------------------- |
| When?             | Only the *first* time appeared            | *Every* time appeared                   |
| Pros              | Avoids dependence between multiple visits | Faster convergence in practice          |
| Cons              | Some visits ignored                       | Higher variance                         |
| Suitable Scenario | Long episodes with repeated states        | Short episodes or rare-state situations |

---
## MC + $\varepsilon$-greedy
- Use soft policy (e.g. $\varepsilon$-greedy) to remove needs of exploring start

	$$
	\pi(a|s) =
	\begin{cases}
	1 - (1 - \frac{1}{|\mathcal{A}(s)|}) \, \varepsilon, & a = a^*_k \\[6pt]
	\frac{1}{|\mathcal{A}(s)|} \, \varepsilon, & \text{otherwise}
	\end{cases}
	$$
	
	- $|\mathcal{A}(s)|$ - number of valid actions of state
	- Balance Exploration & Exploitation