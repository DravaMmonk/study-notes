# 5 Linear & Logistic Regression

## Hyperplane

A **hyperplane** in $n$ dimensions is a linear surface defined as:

$$
\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n = 0
$$

Used as a **decision boundary** in LR.

---

## Regression Model

Regression Model is **probabilistic discriminative** model:

- It optimally discriminates between features that belong to different classes
 - Unlike NB, it models **$P(y \mid x)$ directly**, not via $P(x \mid y)$

> [!note] Outcomes of Regression Model
> 
> **Pros**:
> 
> - Probabilistic interpretation  
> - Simple and efficient  
> - No strong feature distribution assumptions  
> - Works well with frequency-based features  
> - Often outperforms Naive Bayes  
> 
> **Cons**:
> 
> - Can only learn **linear** decision boundaries  
> - Requires feature scaling  
> - Needs fairly large datasets  
> - Regularisation tuning required  
> - Poor performance when relationship is highly non-linear  

---
## Linear Regression

Linear regression models the relationship:

$$
y = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n
$$

**Assumptions**:

- Targets follow a **linear relationship** with the features  
- Sensitive to **multicollinearity** (highly correlated features)

**Goals**:

- Predict continuous target values  
- Use MLE / least-squares to estimate parameters  

**Optimisation**:

Solve the **Normal Equation** of the parameter vector:

$$\theta = (X^\top X)^{-1}X^\top y$$

> [!tip] Be careful! - Interpolationâœ… | ExtrapolationðŸ™…
> 
> - Only give the result within the relevant range of data
> - May fail for classification when out of bound

---

## Logistic Regression

> [!info] Linear â†’ Logistic
> 
> Linear regression fails for classification because:
> 
> - Output must satisfy $0 \le p(x) \le 1$
> - Linear functions output unbounded values
> 
> Solution: **Log-Odds Transformation**

Define **odds**:

$$
\text{odds}(x) = \frac{p(x)}{1 - p(x)} \in (0, \infty)
$$

Take log:

$$
\log \frac{p(x)}{1 - p(x)} \in (-\infty, +\infty)
$$

Model it as a linear function:

$$
\log \frac{p(x)}{1 - p(x)} = \theta^\top x
$$

Apply inverse transform â†’ **sigmoid**:

$$
p(y=1 \mid x;\theta) = \sigma(\theta^\top x)
= \frac{1}{1 + e^{-\theta^\top x}}
$$

Decision rule â†’ **Thresholding Probability**:

Typically set $\text{threshold} = 0.5$:

$$
\hat{y}=
\begin{cases}
1, & p \ge 0.5 \\
0, & p < 0.5
\end{cases}
$$

- $\theta^\top x > 0$ â†’ class 1  
- $\theta^\top x < 0$ â†’ class 0  
- $\theta^\top x = 0$ â†’ boundary ($p = 0.5$)  

---

## Optimisation

The Bernoulli likelihood:

$$
P(y \mid x;\theta)
= p(x;\theta)^y (1 - p(x;\theta))^{(1-y)}
$$

Negative log-likelihood (logistic loss):

$$
L(\theta)
= -\left[ y\log p + (1 - y)\log(1 - p) \right]
$$

Equivalent vector form over all data:

$$
J(\theta)
= -\sum_{i=1}^m 
\left[
y^{(i)} \log h_\theta(x^{(i)}) +
(1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))
\right]
$$

Gradient:

$$
\nabla_\theta J(\theta)
= \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}
$$

Logistic Regression is **No Closed-Form Solution**.

> [!question]- When Does an Optimisation Problem Have No Closed-Form Solution?
> 
> â†’ When its optimal parameters cannot be expressed using a finite sequence of algebraic operations (e.g. Normal Equation of Linear Regression).
> 
> This happens when:
>
> **1. Parameters appear inside non-linear functions**
>   
> - e.g., $e^{-\theta^\top x}$, $\log(\theta^\top x)$, sigmoid, softmax.
> - Such equations cannot be solved **algebraically**.
>
> **2. The loss function is not quadratic**
> 
> - Quadratic losses (e.g., linear regression) yield matrix-form solutions.
> - Non-quadratic losses (log-loss, cross-entropy) do not.
>
> **3. Parameters are coupled in inseparable ways**  
> 
> - Nonlinear coupling prevents isolating $\theta$ in a closed form.
>
> **4. Regularisation introduces nondifferentiable terms**
> 
> - e.g., L1 (Lasso) causes non-smooth optimisation with no analytic solution.


---

## Regularisation Method

L2 regularisation (Ridge):

$$
J(\theta) + \lambda\|\theta\|_2^2
$$

Adds bias â†’ reduces variance.

---

## Handling Multiclass

### One-vs-Rest (OvR)

Train $K$ binary classifiers:

- Class $k$ vs all other classes  
- Predict using the classifier with highest probability  

Used when $K$ is large.

---

### Softmax Regression (Multinomial LR)

For $K$ classes:

$$
P(y=k \mid x;\Theta)
= \frac{e^{\theta_k^\top x}}
{\sum_{j=1}^K e^{\theta_j^\top x}}
$$

Softmax Loss (cross-entropy):

$$
J(\Theta)
= -\sum_{i=1}^m \sum_{k=1}^K 
1\{y^{(i)} = k\} 
\log P(y^{(i)} = k \mid x^{(i)})
$$

---