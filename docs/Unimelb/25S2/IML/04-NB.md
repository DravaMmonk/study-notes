# 4 Naive Bayes

Naive Bayes is a **probabilistic classifier** based on:

- Maximum Likelihood Estimation (MLE)
- The **Naive** Assumption 

> [!NOTE] The Naive Assumption
> 
> - All features are **conditionally independent** given the class  
> - All features contribute **equally** to prediction  

For a feature vector $x = (x_1, x_2, \ldots, x_n)$, Naive Bayes predicts:

$$
\hat{y} = \arg\max_y P(y \mid x)
$$

Using Bayes’ Rule:

$$
P(y \mid x) = \frac{P(y)P(x \mid y)}{P(x)}
$$

Since $P(x)$ is constant across classes:

$$
\hat{y} = \arg\max_y P(y)\prod_{i=1}^n P(x_i \mid y)
$$

> [!note] Outcomes of NB
> 
> **Pros**:
> 
> - Simple to train; computationally efficient  
> - Scales well to high-dimensional data  
> - Interpretable probabilistic model  
> - Handles missing values naturally  
> - Works surprisingly well even when independence is violated
> 
> **Cons**:
> 
> - A single zero renders many additional meaningful observations irrelevant
> 	- If any term's $\text{likelihoods} = 0$, then the whole $P(y|X) = 0$
> 	- Solution: **Smoothing**

---

## Smoothing Methods

### Epsilon Smoothing
Replace zero probabilities with a tiny $\varepsilon$:

$$
P(x_i \mid y) = \varepsilon
$$

---

### Laplace / Add-$\alpha$ Smoothing

$$
P(x_i \mid y) =
\frac{\text{count}(x_i, y) + \alpha}
{\sum_j \text{count}(x_j, y) + \alpha K}
$$

Where:
- $K$ = number of possible values of attribute $X$  
- $\alpha = 1$ gives **Add-One smoothing**

> [!note] After Smoothing  
> - Avoids zero probabilities  
> - Reduces variance  
> - Adds bias (no longer pure MLE)  
> - Large effect when dataset is small  

---

## Bias–Variance Perspective

- **Variance:** how much predictions vary if training data changes  
- **Bias:** difference between model expectation and true function  

High variance → overfitting  
High bias → underfitting

Smoothing: lower variance, slightly higher bias.

---

## Variants of NB

### Gaussian NB

Assumes:

$$
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma_y^2}}
\exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}\right)
$$

MLE estimates:

$$
\mu_y = \frac{1}{N_y}\sum_{i\in y} x_i
$$

$$
\sigma_y^2 = \frac{1}{N_y}\sum_{i\in y}(x_i - \mu_y)^2
$$

Useful for **continuous features**.

---

### Bernoulli NB

$$
P(x_i = 1 \mid y) = \theta_{iy}
$$

$$
P(x_i = 0 \mid y) = 1 - \theta_{iy}
$$

Useful for **binary features** (bag-of-words).

---

### Categorical NB

$K$ categories → $C$ classes:

$$
P(X = k \mid y) =
\frac{\text{count}(X=k, y) + \alpha}
{\sum_{j=1}^K \text{count}(X=j, y) + K\alpha}
$$

> Standard multinomial MLE with Laplace smoothing.

---

### Log-Transformation

$$
\log P(y \mid x) = \log P(y) + \sum_i \log P(x_i \mid y)
$$

> [!question]- Why log-transfer helps?
> - Prevents floating-point underflow  
> - Converts products into sums  
> - Speeds computation and improves stability  

---