# 3 Decision Trees
---
## Baseline Models

### Zero-R (0-R) Model

- Ignores all input features; predicts using **only the label distribution**.
- **Classification:** predict the majority class  
- **Regression:** predict the mean value  

> [!NOTE] Often used to:
> 
> - Provide a baseline reference for performance comparison  
> - Evaluate whether a more complex model is actually learning anything  

---

### One-R (Decision Stump) Model

- Uses **one single feature** to make predictions  
- The *best* feature is the one that makes **fewest training errors**  
- A **pure node** means 0 misclassification  

> [!NOTE] Often used to:
> 
> - Provide a simple interpretable model  
> - Understand which feature is most predictive  
> - Serve as a weak learner in boosting  

---

## Decision Trees

- Recursively split features to produce purer subsets  
- A perfect tree would have **pure leaves** (entropy = 0), but $complete$ purity is often *unrealistic*  
- In practice, we stop splitting when purity is **above a threshold** $\tau$  

> [!note] Outcomes of Decision Tree
> 
> **Pros**:
> 
> - Intuitive and interpretable  
> - Handles numerical and categorical variables  
> - No need for feature scaling  
> 
> **Cons**:
>  
> - Prone to **overfitting**  
> - Small changes in data → large changes in the tree (unstable)  
> - Needs pruning and sometimes weighting  

---

### Feature Selection Principles

- Prefer **smaller trees** → simpler hypotheses generalise better  
- Very large trees usually indicate overfitting  
- Selecting attributes is equivalent to **selecting hypotheses**  
- Using many possible hypotheses increases risk of fitting noise  

---

### ID3 Algorithm

Finding the *optimal* DT is **NP-hard**.

> [!note] Why Optimal Decision Trees Are NP-Hard
> 
> The number of possible trees grows super-exponentially:
> 
> $$\text{\#Trees} = O((d!)^{n})$$
> 
> where $d = \text{attributes}$, $n = \text{splits}$.
>  
> Exhaustive search is infeasible.
>

Therefore, we use **greedy heuristics**, which produce good but not optimal trees:

- Greedy algorithm selecting the feature with **maximum IG (Information Gain)** at each step  
- Builds the tree top-down

> [!question]- When is the tree stop splitting?
> 
> - If **IG = 0** → no improvement → stop splitting  
> - If node purity > threshold $\tau$ → stop splitting  
> - Attribute selected only if **IG/GR > threshold** $\theta$  
> - Use **post-pruning** to remove unnecessary branches  

> [!NOTE] Outcomes of ID3
> 
> ID3 is the **Core** algorithm of DT Learning
> 
> **Pros**:
> 
> - Fast to train and classify  
> - Highly interpretable (transparent structure)  
> 
> **Cons**:
> 
> - Prone to overfitting  
> - Handles continuous attributes poorly unless discretised  
> - Does not guarantee global optimal tree  
> - Computational steps grow with number of classes  
> - **Over-Branching**: IG is biased toward attributes with many distinct values

---

## Variants of DT

### DT with Numeric Features

- Typically use **binary splits**  
- Select a threshold $t$  
- Search for $t$ that maximises a purity metric (IG or Gini, etc.)  

---

### Regression Trees

- Leaf predicts a numerical value (usually mean of samples)  
- Purity measured using **Mean Squared Error (MSE)** instead of IG/GR  
- Objective: minimise variance in child nodes  

---

### Oblivious Decision Tree

- All nodes at the same depth **must split on the same attribute**  
- More structured and sometimes more robust  

---

### Random Tree

- At each node, consider only a **random subset of attributes**  
- Core idea behind **Random Forests**, improving generalisation  

---