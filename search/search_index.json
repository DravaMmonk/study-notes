{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Drava's Study Notes","text":""},{"location":"#master-of-information-technology-in-the-university-of-melbourne","title":"\ud83c\udf93 Master of Information Technology in the University of Melbourne","text":""},{"location":"#2025-semester-2","title":"\ud83d\uddd3\ufe0f 2025 Semester 2","text":""},{"location":"#ai-planning-for-autonomy-comp90054","title":"\u270f\ufe0f AI Planning for Autonomy - COMP90054","text":""},{"location":"#other","title":"\ud83d\udcbb Other","text":""},{"location":"#reference","title":"\ud83d\udcda Reference","text":""},{"location":"Unimelb/25S2/AIP/1-Search/","title":"1. Search","text":""},{"location":"Unimelb/25S2/AIP/1-Search/#search-space","title":"Search Space","text":"<ul> <li>A set of search states</li> <li>Forward Search (Progression)<ul> <li>search space = world space (represents the current real world)</li> </ul> </li> <li>Backward Search (Regression)<ul> <li>search space = a sets of world spaces (represents all predications of sub-goals)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#example","title":"EXAMPLE","text":"<p>In a robot delivery scene: <pre><code>world_state_of_a_robot = {\n\u00a0 \u00a0 \"position\": (3, 5),\n\u00a0 \u00a0 \"direction\": \"North\",\n\u00a0 \u00a0 \"battery\": 80,\n\u00a0 \u00a0 \"carrying_package\": True\n}\n</code></pre></p> <ul> <li>Progression: \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5c31\u662f\u673a\u5668\u4eba\u5728\u67d0\u4e2a\u4f4d\u7f6e\u3001\u671d\u5411\u3001\u80fd\u91cf\u3001\u8d1f\u8f7d\u7684\u7ec4\u5408\u72b6\u6001\u3002</li> <li>Regression:\u00a0 \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5171\u540c\u7ec4\u6210\u201c\u6211\u4eec\u60f3\u8981\u6240\u6709\u6ee1\u8db3\u4e00\u4e2a\u6761\u4ef6\u7684\u4e16\u754c\u72b6\u6001\u96c6\u5408\u201d\u3002</li> </ul> <pre><code>subgoals = {\"position\": (5, 8), \"carrying_package\": True}\n</code></pre> <p>Common Functions: </p> <ul> <li>\\(s\\) = search states</li> <li>\\(\\mathrm{is}\\_\\mathrm{start}(s)\\) return if the state is the start state of the search space</li> <li>\\(\\mathrm{is}\\_\\mathrm{target}(s)\\) mark if the state is the goal state of the search space</li> <li>\\(\\mathrm{succ}(s)\\) return a list of successors/next states of \\(s\\)</li> <li>Search nodes:\u00a0<ul> <li>\\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(\\mathrm{parent}(\\sigma)\\) where \\(\\sigma\\) was reached</li> <li>\\(\\mathrm{action}(\\sigma)\\) leads from \\(\\mathrm{state}(\\mathrm{parent}(\\sigma))\\) to \\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(g(\\sigma)\\) denotes cost of path from the root to \\(\\sigma\\)</li> <li>The root\u2019s \\(\\mathrm{parent}(\\cdot)\\) and \\(\\mathrm{action}(\\cdot)\\) are undefined</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#search-methods","title":"Search Methods","text":""},{"location":"Unimelb/25S2/AIP/1-Search/#blind-search-vs-informed-search","title":"Blind Search vs. Informed Search","text":"<ul> <li>Blind search not require any input beyond the problem</li> <li>No additional work but rarely effective</li> <li>Informed search requires function \\(h(x)\\) mapping states to estimates their goal distance</li> <li>Effective but lots of work to construct \\(h(x)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#blind-systematic-search-algorithms","title":"Blind Systematic Search Algorithms","text":"<ul> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>Iterative Deepening Search<ul> <li>Do DLS(DFS with depth limited) with continuously increasing depth limited by 1.</li> </ul> </li> <li>Blind Search: Completeness 100%, but poor efficient when doing hard work</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#heuristic-functions","title":"Heuristic Functions","text":"<ul> <li>\\(h(n)\\) - Estimated remaining cost (from current state to goal state)</li> <li>\\(h^*(n)\\) - Real remaining cost</li> <li>proficiency of \\(h(n)\\) <ul> <li>\\(h = h^{\\ast}\\) perfectly informed, \\(h(n) = h^{\\ast}(n) - \\textbf{optimal } A^{\\ast}\\)</li> <li>\\(h = 0\\) no information at all - uniform cost search</li> </ul> </li> </ul> Properties Description Safe \\(h(n) = \\infty\\) iff \\(h^{\\ast}(n) = \\infty\\) Goal-Aware \\(h(\\text{goal}) = 0\\) Admissible \\(h(n) \\le h^*(n)\\) Consistent \\(h(n) \\le c(n,n\u2019) + h(n\u2019)\\) for all possible \\(c(n, n\u2019)\\)"},{"location":"Unimelb/25S2/AIP/1-Search/#relations-of-properties","title":"Relations of properties","text":"<ul> <li>Consistent &amp; Goal-aware \\(\\to\\) Admissible</li> <li>Admissible \\(\\to\\) Safe &amp; Goal-aware</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#informed-systematic-search-algorithms","title":"Informed Systematic Search Algorithms","text":""},{"location":"Unimelb/25S2/AIP/1-Search/#greedy-best-first-search-gbfs","title":"Greedy Best-First Search (GBFS)","text":"<ul> <li>Use priority queue to sort \\(h(n)\\) of each node in ascending order<ul> <li>If \\(h(n) = 0\\), it becomes what fully depends\u00a0on how\u00a0we\u00a0break\u00a0ties</li> </ul> </li> </ul> <pre><code>def greedy_BFS:\n\u00a0 \u00a0 frontier = priority queue ordered by h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <ul> <li>Completeness\u2705for safe heuristics; Optimal\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#a","title":"A*","text":"<ul> <li>Only difference from GBFS:<ul> <li>\\(h(n) \\rightarrow f(n) = g(n) + h(n)\\)</li> </ul> </li> </ul> <pre><code>def a_star:\n\u00a0 \u00a0 frontier = priority queue ordered by f(n) = g(n) + h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(h(start), start, path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 _, current, path = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f = cost(new_path) + h(current)\n\u270f\ufe0f\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(f, successor, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre>"},{"location":"Unimelb/25S2/AIP/1-Search/#re-opening","title":"Re-opening","text":"<ul> <li>a node \\(n\\) is in <code>explored</code> but if we find a cheaper \\(g(n)\\), then we can re-open the <code>explored</code> set and extend this node</li> <li>Not needed if consistent\u2705<ul> <li>\\(g\\) is already cheapest</li> </ul> </li> </ul> <pre><code>def a_star_with_re_opening:\n    frontier = priority queue ordered by f(n) = g(n) + h(n)\n    explored = set\n    path = list\n\u00a0 \u00a0 frontier.add(h(start), start, path)\n\n\u270f\ufe0f  g = dict\n\u270f\ufe0f  g[start] = 0\n\n    while frontier:\n        _, current, path = frontier.pop()\n        if current == goal:\n            return path\n        explored.add(current)\n        for successor in succ(current):\n            new_path = path + action(current, successor)\n            new_g = cost(new_path)\n            if successor not in g or new_g &lt; g[successor]:\n                [successor] = new_g\n                f = g[successor] + h(successor)\n                frontier.add(f, successor, f)\n\n\u270f\ufe0f              if successor in explored:     # Re-opening\n\u270f\ufe0f                  explored.remove(successor)\n    return unsolvable\n</code></pre>"},{"location":"Unimelb/25S2/AIP/1-Search/#weighted-a","title":"Weighted A*","text":"\\[ f_W(n) = g(n) + W \\cdot h(n) \\] Weight Algorithm \\(W \\to 0\\) Digkstra Algorithm \\(W \\to 1\\) A* \\(W &gt; 1\\) Bounded sub-optimal A* \\(W \\to \\infty\\) GBFS <ul> <li>If \\(h\\) is admissible, \\(f_W(n) \\le W \\cdot h(n)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#hill-climbing","title":"Hill-Climbing","text":"<p><pre><code>def hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break \u00a0 \u00a0\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Can only find local maxima - Make sense only if \\(h(n) &gt; 0\\) for all non-goal states</p>"},{"location":"Unimelb/25S2/AIP/1-Search/#enforced-hill-climbing","title":"Enforced Hill-Climbing","text":"<p><pre><code>def enforced_hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subs = n for n in neighbours_of(current) and not in explored\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 best_sub = argmin_h(subs, goal=current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if best_sub and h(best_sub) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path -= action(parent, current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(parent, best_sub)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = best_sub\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Do small range BFS when find local optimal - Can across small gap between local best and global best</p>"},{"location":"Unimelb/25S2/AIP/1-Search/#iterative-deepening-a","title":"Iterative Deepening A*","text":"<ul> <li>IDS + \\(A^*\\): Use f(n) instead of depth to limit IDS<ul> <li>In First Search:\u00a0 f(n) = f(start) = 0 + h(start)</li> <li>Following Searches: f(n) = min_out_of_bound_excess <pre><code>def ida_star:\n\u00a0 \u00a0 bound = f(start)\n\u00a0 \u00a0 while True:\n\u00a0 \u00a0 \u00a0 \u00a0 t = ids(start, bound)\n\u00a0 \u00a0 \u00a0 \u00a0 if t == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return solution\n\u00a0 \u00a0 \u00a0 \u00a0 if t == infinity:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 \u00a0 \u00a0 bound = t\n</code></pre></li> </ul> </li> <li>Dealing with one of \\(A^*\\)\u2019s problem: large queue/closed_set</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#evaluation-of-search-methods","title":"Evaluation of Search Methods","text":""},{"location":"Unimelb/25S2/AIP/1-Search/#guarantees","title":"Guarantees","text":"<ul> <li>Completeness sure to find a solution if there is one</li> <li>Optimality solutions sure be optimal</li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#complexity","title":"Complexity","text":"<ul> <li>Time/Space (Measured in generated states/states cost)</li> <li>Typical state space features governing complexity<ul> <li>Branching factor \\(b\\) how many successors</li> <li>Goal depth \\(d\\) number of actions to reach shallowest goal state</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/1-Search/#summary","title":"Summary","text":"DFS BrFS IDS A* HC IDA* Complete \u274c \u2705 \u2705 \u2705 \u274c \u2705 Optimal \u274c \u2705* \u2705 \u2705* \u274c \u2705* Time \\(\\infty\\) \\(b^d\\) \\(b^d\\) \\(b^d\\) \\(\\infty\\) \\(b^d\\) Space \\(b \\cdot d\\) \\(b^d\\) \\(b \\cdot d\\) \\(b^d\\) \\(b\\) \\(b \\cdot d\\) <ul> <li>\\(b\\) - Branching Factor: sum of number of child nodes of each node</li> <li>\\(d\\) - Solution Depth: \\(minimum\\) depth among all goal nodes</li> <li>DFS cannot handle cyclic graph</li> <li>BFS is optimal only when uniform costs are applied </li> <li>\\(A^*\\) / Iterative Deepening \\(A^*\\) is optimal only when \\(h\\) is admissible (\\(h \\le h^*\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/","title":"2. Planning","text":""},{"location":"Unimelb/25S2/AIP/2-Planning/#problem-solving","title":"Problem Solving","text":""},{"location":"Unimelb/25S2/AIP/2-Planning/#autonomous-behaviour-in-ai","title":"Autonomous Behaviour in AI","text":"<ul> <li>Programming-based</li> <li>Learning-Based</li> <li>Model-Based</li> <li>Approaches not orthogonal</li> <li>Different models yield different types of controllers \u6267\u884c\u52a8\u4f5c\u7684\u7b56\u7565\u6a21\u5757</li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#3-level-solver-ambitions","title":"3-level Solver Ambitions","text":"<ul> <li>Ambition: Write one program to solve all classical search problems<ul> <li>Ambition 1.0 (Problem Solving) Write one program to solve a problem</li> <li>Ambition 2.0 (Problem Generation) Write one program to solve a large class of problems</li> <li>Ambition 3.0 (Meta Problem Solving) Write one program to solve a large class of problems effectively</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#solving-approaches","title":"Solving Approaches","text":""},{"location":"Unimelb/25S2/AIP/2-Planning/#programming-based-approach","title":"Programming-Based Approach","text":"<ul> <li>Pro: Domain-knowledge easy to express</li> <li>Con: Less Flexible (Can\u2019t deal with situations not anticipated by programmer)</li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#learning-based-approach","title":"Learning-Based Approach","text":"<ul> <li>Unsupervised/Reinforced Learning: Use reward &amp; penalise</li> <li>Supervised: Use labelled data</li> <li>Evolutionary: Use original controllers to mutate and recombine to build better controller</li> <li>Pros Dose not require much knowledge in principle\u00a0</li> <li>Cons Slower; Hard to know which features to learn\u00a0</li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#model-based-approach-planning","title":"Model-Based Approach (Planning)","text":"<ul> <li>One model for one specific problem </li> <li>Pros Powerful; Quick; Flexible; Clear; Intelligent; Domain-independent</li> <li>Cons Need a model (sometimes very hard); Efficiency loss</li> <li>Always need trade-off between \u201cAutomatic and general\u201d vs. \u201cManual work but effective\u201d</li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#planning-models","title":"Planning Models","text":""},{"location":"Unimelb/25S2/AIP/2-Planning/#classical-planning-model","title":"Classical Planning Model","text":"<ul> <li>Classical Planning Model = Basic State Model</li> <li> <p>Assumptions</p> <ul> <li>Deterministic</li> <li>Fully Observable</li> <li>Static World</li> <li>Discrete Time &amp; Finite Actions</li> <li>Uniform Cost</li> </ul> </li> <li> <p>Components </p> \\[ M = \\langle S, A, T, I, G \\rangle \\] <ul> <li>\\(S\\) - State spaces </li> <li>\\(A(s)\\) - Actions applicable for \\(s \\in S\\)</li> <li>\\(T\\) - Deterministic Transition Function: \\(s\u2019 = T(A(s), s)\\) shows one successor \\(s\u2019\\) of \\(s\\)</li> <li>\\(I\\) - Initial state</li> <li>\\(G\\) - Goal states</li> <li>Uniform action costs \\(c(A(s), s) = 1\\)</li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>A seq of actions map \\(s_0\\) into \\(g\\)</li> <li>Optimal if total cost to goal is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#conformant-planning-model","title":"Conformant Planning Model","text":"<ul> <li>\\(\\approx\\) Brute Force</li> <li>Components (Diff with classic)<ul> <li>Initial State \u2192 A set of possible initial states</li> <li>Deterministic Transition Function \u2192 Non-deterministic</li> </ul> </li> <li>Outcomes<ul> <li>No Observation - No new info; must pre-planning</li> <li>Goal Guarantee</li> <li>Rarely optimal<ul> <li>Must map any possible \\(s_0\\) to \\(g\\), too much unnecessary work</li> <li>Sensitive to Worst/Special case </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#markov-decision-processes-mdps","title":"Markov Decision Processes (MDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li> <p>Markov Property: the next state only depends on the current state and actions.</p> \\[ P(s_{t+1} \\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots) = P(s_{t+1} \\mid s_t,a_t)  \\] </li> <li> <p>Fully Observable</p> </li> <li>Discrete Time &amp; Finite Actions</li> <li>Fixed Transition Probabilities</li> </ul> </li> <li> <p>Components (Diff with conformant)     $$     M = \\langle S,A,T,R,\\gamma \\rangle     $$</p> <ul> <li>Introduce the Transition Probability Function:     $$     T(s, a, s') = P(s' \\mid s, a)     $$</li> <li>Introduce Reward Function \\(R\\) and Discount Factor \\(\\gamma\\):<ul> <li>For estimating the value of each state</li> </ul> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map states to actions</li> <li>Optimal if total expected cost to goal is \\(minimum\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/2-Planning/#partially-observable-mdps-pomdps","title":"Partially Observable MDPs (POMDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li>Markov Property</li> <li>Limited Observable</li> </ul> </li> <li> <p>Components (Diff with MDPs)</p> <ul> <li> <p>Introduce Sensor Model (based on Probability Distribution)</p> \\[ \\mathrm{b}(s_t) = P(s_t \\mid \\text{historical actions and observations}) \\] </li> <li> <p>Initial/Goal States \u2192 Belief States \\(\\mathrm{b}(s_0)\\) and \\(\\mathrm{b}(g)\\)</p> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map belief states into actions</li> <li>Optimal if total expected cost from \\(\\mathrm{b}(s_0)\\) to \\(\\mathrm{b}(g)\\) is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/","title":"3. Complexity","text":""},{"location":"Unimelb/25S2/AIP/3-Complexity/#savitchs-theorem","title":"Savitch\u2019s Theorem","text":"<ul> <li>NPSPACE = PSPACE<ul> <li>Non-determinism won't make solvers more powerful.</li> </ul> </li> <li>PSPACE-complete<ul> <li>The problem is at least hard as any Polynomial time problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#goals-definition","title":"Goals Definition","text":"<ul> <li>To make a Satisficing planning:<ul> <li>\\(\\mathrm{PlanEx}(P) =\\) <code>TRUE</code> - Existence of a plan for problem \\(P\\).</li> </ul> </li> <li>To make a Optimal planning:<ul> <li>\\(\\mathrm{PlanLen}(P) \\le B\\)  - Existence of a plan which length is at most \\(B\\).</li> </ul> </li> <li>\\(\\mathrm{PlanEx}\\) and \\(\\mathrm{PlanLen}\\) is PSPACE-complete<ul> <li>In practice, optimal planning is almost never easy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#strips-planning-problem","title":"STRIPS planning problem","text":""},{"location":"Unimelb/25S2/AIP/3-Complexity/#strips-stanford-research-institute-problem-solver","title":"STRIPS = STanford Research Institute Problem Solver","text":"<ul> <li>Design Goals:<ul> <li>Specification - concise model description</li> <li>Computation - reveal useful information/structure for heuristics</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#components","title":"Components","text":"\\[ P = \\langle P,A,I,G \\rangle \\] <ul> <li>\\(P\\) - All Possible Predicates</li> <li>\\(A\\) - Actions</li> <li>\\(I\\) - Initial States: Predicates initially to be <code>TRUE</code></li> <li>\\(G\\) - Goal Conditions: Predicates Finally need to be <code>TRUE</code></li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#action","title":"Action","text":"\\[ a \\in A, a = \\langle Pre(a), Add(a), Del(a) \\rangle \\] <ul> <li>\\(Pre(a)\\) - Preconditions need to be <code>TRUE</code> before executing the action</li> <li>\\(Add(a)\\) - Add Effects: Preconditions set to be<code>TRUE</code> after execution</li> <li>\\(Del(a)\\) - Delete Effects: Preconditions set to be <code>FALSE</code> after execution</li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#result-of-applying-actions","title":"Result of applying actions","text":"\\[ Result(s, a) = (s - Del(a)) \\cup Add(a) \\] <ul> <li>which could also be seemed as a transition function</li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#goal","title":"Goal","text":"<ul> <li>Find a list of actions \\([a_1,a_2,a_3,\\ldots]\\) which satisfies:     $$     Result(Result(\\ldots Result(I,a_1\u200b) \\ldots ,a_n\u22121\u200b),a_n\u200b) \\models G     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/3-Complexity/#properties","title":"Properties","text":"<ul> <li>It turns a \"solving\" problem into a \"search\" problem</li> <li>Still PSPACE-complete, but \"approximation\" is able to be applied<ul> <li>Explicit Search \u663e\u5f0f\u641c\u7d22<ul> <li>e.g. Blind/Heuristic search</li> <li>Not effective</li> </ul> </li> <li>Near Decomposition \u8fd1\u4f3c\u5206\u89e3<ul> <li>e.g. Relaxation: similar to Decrease/Divide &amp; Conquer</li> <li>Maybe fail</li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/","title":"4. Relaxation","text":""},{"location":"Unimelb/25S2/AIP/4-Relaxation/#relaxation","title":"Relaxation","text":"<ul> <li>A method to compute heuristic functions \\(h(n)\\)<ul> <li>Defines a transformation for simplifying the STRIPS problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#components","title":"Components","text":"\\[ \\mathcal{R} = (\\mathcal{P'}, r, h'^*) \\] <ul> <li>\\(\\mathcal{P}'\\) - the class of the simpler problem </li> <li>\\(r\\) - transformer turning the original problem into a simplified one</li> <li>\\(h'^*(n)\\) - Perfect heuristic function of the simplified problem </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#properties","title":"Properties","text":"<p>The Relaxation is - Native - if \\(\\mathcal{P'} \\subseteq \\mathcal{P}\\) and  \\(h'^*(n) = h(n)\\) - Efficiently Constructible - if a polynomial \\(r\\) exists - Efficiently Computable - if a polynomial \\(h\u2019(n)\\) exists</p>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#examples","title":"Examples","text":"<ol> <li>Route-Finding<ul> <li>Relaxation<ul> <li>Route-find as a bird (ignoring the road)</li> </ul> </li> <li>Outcome<ul> <li>Road Distance \u2192 Manhattan distance, Euclidean distance, etc.</li> </ul> </li> <li>Native\u274c Efficiently constructible\u2705 Efficiently computable\u2705</li> </ul> </li> <li>Goal-Counting<ul> <li>Relaxation<ul> <li>Assume we can achieve each goal directly</li> </ul> </li> <li>Outcome<ul> <li>Tasks \u2190 No precondition and delete</li> </ul> </li> <li>Admissible but still NP-hard</li> <li>Native\u2705 Efficiently constructible\u2705 Efficiently computable\u274c</li> </ul> </li> </ol>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#still-inefficiency","title":"Still Inefficiency?","text":"<ul> <li>Approximate \\(h\u2019^*\\)</li> <li>Re-design \\(h\u2019^*\\) in a way so that it will typically be feasible<ul> <li>Critical path heuristics</li> <li>Delete relaxation \u2190 wide-spread for satisficing planning</li> <li>Abstractions</li> <li>Landmarks</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#delete-relaxation","title":"Delete Relaxation","text":"<ul> <li>Apply \\(Del(a) = \\emptyset, \\forall a \\in A\\)<ul> <li>which makes \"once\"  <code>TRUE</code> predications remain <code>TRUE</code> \"forever\"</li> </ul> </li> <li>\\(a^+\\) - Actions after delete relaxation</li> <li>Optimal Delete Relaxation is Admissible \\(h^+(n) \\le h^*(n)\\)<ul> <li>(but find an optimal solution is still NP-hard)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#dominance","title":"Dominance","text":"<ul> <li>\\(s' \\supseteq s\\) - \\(s'\\) dominate \\(s\\)<ul> <li>If \\(s\\) is a goal state, then \\(s'\\) must be a goal state.</li> <li>If \\(a^+\\) is applicable in \\(s\\), then \\(a^+\\) must be applicable in \\(s'\\).</li> </ul> </li> <li>\\(Result(s, a^+)\\) dominates both \\(s\\) and \\(Result(s, a)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#optimal-delete-relaxation-heuristic-h","title":"Optimal Delete Relaxation Heuristic (\\(h^+\\))","text":"<pre><code>def h_plus(s, G, A):\n    if G in s:\n        return 0\n\n    open = [s]\n    cost[s] = 0\n    cost[others] = inf\n    best = inf\n\n    while open:\n        cur_state = pop(open)\n        if G in cur_state:\n            best = min(best, cost[cur_state])\n            continue\n\n        for a in A:\n            if pre(a) in current:\n                next_state = current + add(a)\n                if cost[next_state] &gt; cost[cur_state] + 1:\n                    cost[next_state] = cost[current] + 1\n                    push(open, next_state)\n\n    if best == inf:\n        return unsolvable\n    else:\n        return best\n</code></pre> <ul> <li>Native relaxation\u2705</li> <li>Safe\u2705, goal-aware\u2705, admissible guarantee\u2705</li> <li>Efficiently constructible\u2705 Polytime</li> <li>Efficiently computable\u274c<ul> <li>\\(\\mathrm{PlanOpt^+} = \\sum h^+\\) still NP-hard</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<ul> <li>Application based on \\(h^+\\)</li> <li>Distance estimate during the iterations in using shortest-distance algorithm</li> <li>Initially set start point \\(= 0\\); others \\(= \\infty\\).</li> <li>Relax every edge (One iteration = all neighbour unexplored edges from one point)</li> <li>Use a table to note estimate length of shortest path from start to current point</li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#example","title":"Example","text":"<ul> <li>A\u2192B(+4); A\u2192C(+5); B\u2192C(-2)</li> </ul> Vertex Initialisation Round 1 Round 2 A 0 0 0 B \u221e 4 4 C \u221e 5 2 (= 4 - 2)"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#h-approximation-methods","title":"\\(h^+\\)-Approximation Methods","text":""},{"location":"Unimelb/25S2/AIP/4-Relaxation/#plan-existence-checking","title":"Plan Existence Checking","text":"<ul> <li> <p>based on Fast Forward Heuristic (\\(h_{FF}\\))</p> <ul> <li>Expand the current state based on BrFS <pre><code>def is_plan_existence(s, G, A):\n    if G in s:\n        return TRUE\n\n    reached = s\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return FALSE\n        reached = new_facts\n\n    return TRUE\n</code></pre></li> </ul> </li> <li> <p>Sound, complete, Terminates in polytime\u2705</p> <ul> <li>\\(\\mathrm{PlanEx}^+\\) now becomes a polytime problem</li> </ul> </li> <li>Safe\u2705, goal-aware\u2705, admissible\u274c (Existence Check Only, usually far from optimal)</li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#max-heuristic-hmax","title":"Max Heuristic (\\(h^{max}\\))","text":"<pre><code>def h_max(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n                # a is reachable\n                new_cost = 1 + max(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n    return max(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime<ul> <li>However, sometimes maybe too optimistic\u26a0\ufe0f</li> </ul> </li> <li>Optimistic Estimation: admissible\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#additive-heuristic-hadd","title":"Additive Heuristic (\\(h^{add}\\))","text":"<pre><code>def h_sum(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n\u270f\ufe0f              new_cost = 1 + sum(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n\u270f\ufe0f  return sum(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime</li> <li>Pessimistic Estimation: admissible\u274c informative\u2705 </li> <li>Overcounts by ignoring positive interactions, i.e. shared sub-plans<ul> <li>May result in \\(dramatic\\) over-estimates of \\(h^*\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#relaxed-plans","title":"Relaxed Plans","text":""},{"location":"Unimelb/25S2/AIP/4-Relaxation/#best-supporter-function-bsp","title":"Best Supporter Function \\(bs(p)\\)","text":"<ul> <li>Input: A predication \\(p\\)</li> <li>Output: The \\(cheapest\\) action \\(a\\) which make this predication <code>TRUE</code></li> <li>Prerequisites<ul> <li>\\(p \\in add(a)\\) iff \\(bs(p) = a\\)</li> <li>\\(bs(\\cdot)\\) is closed<ul> <li>\\(bs(p)\\) is defined for every \\(p \\in (P \\backslash s)\\) that has a path to a goal \\(g \\in G\\)</li> </ul> </li> <li>\\(bs(\\cdot)\\) is well-bounded<ul> <li>Support Graph is acyclic </li> </ul> </li> </ul> </li> <li>If a relaxed plan exists, the closed well-founded \\(bs(\\cdot)\\) definitely exists.<ul> <li>There is a relaxed path from \\(I\\) to \\(G\\)</li> <li>Every \\(g\\) has at least one supporter, so as its subgoals</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#helpful-actions","title":"Helpful Actions","text":"<ul> <li>An action is helpful iff:<ul> <li>it is applicable in the current state (\\(pre(a) \\subseteq s\\))</li> <li>it is contained in the final plan</li> </ul> </li> <li>Expanding only helpful actions does not guarantee completeness.</li> </ul>"},{"location":"Unimelb/25S2/AIP/4-Relaxation/#relaxed-plan-heuristic-h_ff","title":"Relaxed Plan Heuristic (\\(h_{FF}\\))","text":"<ul> <li> <p>Fast Forward Expansion + Greedy Backward Extraction</p> <ul> <li>For each goal state \\(g \\in G\\):<ul> <li>Find the cheapest \\(a = bs(g)\\)</li> <li>Add this action to the plan</li> <li>repeat on \\(bs(pre(a))\\) <pre><code>def h_FF(I, G, A):\n    if G in I:\n        return 0\n\n    # Forward Expansion - BFS\n    reached = I\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return unsolvable\n        reached = new_facts\n\n    # Backward Extraction - Greedy\n    plan = []\n    subgoals = G\n    while subgoals not in I:\n        new_subgoals = []\n        for g in subgoals:\n            pick a s.t. g in add(a) and pre(a) in reached\n            plan += {a}\n            new_subgoals += pre(a)\n        subgoals = new_subgoals\n\n    return len(plan)\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Same theoretical properties as \\(h^{add}\\) but better in practice</p> <ul> <li>Overcount sub-plans shared by different sub-goals</li> <li>Best Supporter is greedily chosen and sub-optimal</li> <li>In practice, \\(h_{FF}\\) typically does not over-estimate \\(h^*\\), or not by a large amount.</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/","title":"5. Exploration","text":""},{"location":"Unimelb/25S2/AIP/5-Exploration/#novelty","title":"Novelty","text":"<ul> <li>The size of the smallest subset of \\(P \\subseteq s\\), such that \\(s\\) is the first state that makes \\(P\\) <code>TRUE</code> during the search</li> <li>Width: The size of the smallest subset of \\(P\\) needed to be considered to achieve the goal</li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#example","title":"EXAMPLE","text":"\\[ p \\to (p, q) \\to (q, r) \\to (p, r) \\to (p, q, r) \\] Current State Smallest New Subset Novelty \\(p\\) \\(p\\) 1 \\(p, q\\) \\(q\\) 1 \\(q, r\\) \\(r\\) 1 \\(p, r\\) \\(p, r\\) 2 \\(p, q, r\\) \\(p, q\\) 2"},{"location":"Unimelb/25S2/AIP/5-Exploration/#width-based-planning","title":"Width-Based Planning","text":""},{"location":"Unimelb/25S2/AIP/5-Exploration/#iterated-width-iw","title":"Iterated Width (IW)","text":"<ul> <li>\\(IW(k) =\\) BFS on \\((Q \\; \\backslash \\; s), \\mathrm{novelty}(s) &gt; k\\)<ul> <li>\\(IW(1) =\\) There is new \\(p\\) appearing in every step in search</li> </ul> </li> <li>\\(IW\\) Algorithm<ul> <li>A sequence of calls \\(IW(k), k=1,2,3,\\ldots\\), until the problem solved or \\(k &gt; len(\\bigcup P)\\) (return <code>unsolvable</code>). </li> <li>The \\(minimum\\) \\(k\\) is the \\(\\text{Width}\\) of the problem</li> </ul> </li> <li>Outcomes<ul> <li>Simple and Blind<ul> <li>Even don't need to know \\(G\\) </li> </ul> </li> <li>However performs pretty well in practice<ul> <li>\\(IW(k \\le 2)\\) can solve 88.3% IPC problems with single goals.</li> <li>Most classical problem (e.g. Blocks, Logistics, Gripper, n-puzzle) have a bounded width independent of problem size and initial situation</li> </ul> </li> <li>Fast \\(\\mathrm{O}(n^k)\\)</li> <li>Optimal if in uniform cost</li> <li>Complete</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#serialised-iterated-width-siw","title":"Serialised Iterated Width (SIW)","text":"<ul> <li> <p>Use \\(IW\\) for decomposing problem and solving sub-problems individually <pre><code>def SIW(s, G):\n    state = s\n    plan = []\n    for g in serialize(G):\n        subplan = IW(state, goal=g)\n        plan += subplan\n        state = Result(subplan, state)\n    return plan\n</code></pre></p> </li> <li> <p>Better performance in Joint Goals problem</p> <ul> <li>(Multi goals but similar approaches)</li> </ul> </li> <li>\u24c1 Goals should be easy to serialise and have low width</li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#balancing-exploration-exploitation","title":"Balancing Exploration &amp; Exploitation","text":"<ul> <li>Exploitation: Trusting your heuristic function<ul> <li>State-based Satisfying Planning Often Rely on:<ul> <li>heuristics derived from problem</li> <li>plugged into Greedy Best-First Search (GBFS)  </li> <li>extensions (e.g. helpful actions and landmarks)  </li> </ul> </li> <li>Often gets stuck in local minima<ul> <li>poly + sub-optimal or optimal + NP-hard</li> </ul> </li> </ul> </li> <li>Exploration: Searching for Novelty<ul> <li>Novelty leads to much better performance in practice</li> <li>Can be model-free (No Rely/Assumption)</li> <li>Required for optimal behaviour (in RL and MTCS)</li> </ul> </li> <li>A good agent need to balance the Exploration and Exploitation</li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#best-first-width-search-bfws","title":"Best-First Width Search (BFWS)","text":"<ul> <li>Do BFS (Priority Queue) on a sequence of measures:     $$     BFWS(f) \\text{ for } f= \\langle w,f_1,f_2,\\ldots \\rangle     $$<ul> <li>\\(w\\) - Novelty-Measure</li> <li>\\(f_i\\) - tie breaker</li> </ul> </li> <li>Much more efficient than GBFS</li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#models-to-simulators","title":"Models to Simulators","text":""},{"location":"Unimelb/25S2/AIP/5-Exploration/#models","title":"Models","text":"<ul> <li>Nowadays, models become more powerful with helps of declarative programming<ul> <li>Expressive language features easily supported</li> <li>Development of external development tools</li> <li>Fully-Black-Box procedures for higher-level abstraction and decomposing problems</li> </ul> </li> <li>However, Declarative Languages also have their downsides:<ul> <li>Model \\(\\ne\\) Language<ul> <li>Many problems fit Classical Planning model, but hard to express in PDDL</li> </ul> </li> </ul> </li> <li>We need for planners that work without complete declarative representations</li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#simulators","title":"Simulators","text":"<ul> <li>= Models but no \\(pre(a)\\) and \\(add(a)\\) for presenting \\(a\\)</li> <li>Outcomes<ul> <li>At the same level of efficiency as classic models</li> <li>Open up exciting possibilities for modelling beyond PDDL</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#simulated-bfws-framework","title":"Simulated BFWS Framework","text":"<ul> <li>Framework<ul> <li>Get communication across researchers and to build on each others\u2019 work</li> </ul> </li> <li>Approaches<ul> <li>Get \"states\" from the simulator</li> <li>The optimal combination is \\(BFWS(\\langle w_h(s), h(s) \\rangle)\\) <ul> <li>\\(w_h(s) =\\) \\(s'\\) which has smallest novelty and \\(h(s') = h(s)\\)</li> </ul> </li> </ul> </li> <li>BFWS is the first planners using simulators</li> <li>Challenges of Width-Based Planning over Simulators<ul> <li>Non-linear dynamics  </li> <li>Perturbation in flight controls  </li> <li>Partial observability  </li> <li>Uncertainty about opponent strategy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/5-Exploration/#arcade-learning-environment","title":"Arcade Learning Environment","text":"<ul> <li>A simple object-oriented framework to develop AI agents for Atari 2600 games<ul> <li>Deterministic</li> <li>Initial state fully known</li> </ul> </li> <li>Performance of \\(IW(1)\\) <ul> <li>better in 34/54 games than 2BFS</li> <li>better in 31/54 games than UCT</li> <li>better in 45/49 games than DeepMind (RL method)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/","title":"6. Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#reinforced-learning","title":"Reinforced Learning","text":"<ul> <li>Policy + Reward + Trial-and-error interaction</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#planning-vs-learning","title":"Planning vs Learning","text":"Dimension Planning Learning Environment Model Model is known Model is unknown Learning Mode Offline computation Online trial-and-error Agent\u2013Environment Interaction No interaction with the real environment; uses internal simulator Must act in the real environment to gather experience Policy Improvement Through search, deliberation,  planning and introspection Through reward-driven learning Suitable Scenarios A precise model exists and simulation is cheap The model is unknown or hard to specify Advantages Safe, interpretable, no real-world risk Adaptive, risky working in complex/unknown environments Disadvantages Requires accurate model; modeling may be expensive Requires exploration; may be costly or risky Example - Atari Game Agent can query emulator for perfect model (source code) Agent can only sees pixels and scores on the screen \u2192 trial-and-error gameplay is necessary"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#rl-vs-planning-vs-other-ml","title":"RL vs. Planning vs. Other ML","text":"Dimension Reinforcement Learning Automated Planning Other ML Action Outcomes Non-deterministic \u2014 actions lead to probabilistic transitions Deterministic \u2014 outcome fully known from model Usually not modelled as sequential decisions Environment Representation Probabilistic model of states, transitions, and rewards Symbolic or logical model (e.g., STRIPS) No explicit environment Learning Signal Reward Signal - Feedback from environment Predefined goal or planner objective Labels or self-structures Data Structure Sequential / Time series (non-i.i.d.) Discrete steps in a planning domain Often i.i.d. samples (independent &amp; identically distributed) Search &amp; Optimisation Trail-and-error search + Reward-driven state-space search + Predefined policy Gradient-based or statistical fitting Credit Assignment Required \u2014 reward may be delayed over time Not relevant \u2014 goal known a priori Not Required - no delay"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#example-common-applications","title":"Example - Common Applications","text":"<ul> <li>Making a humanoid robot walk</li> <li>Fine tuning LLMs using human/AI feedback</li> <li>Optimising operating system routines</li> <li>Controlling a power station</li> <li>Managing an investment portfolio</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#reinforced-learning-process","title":"Reinforced Learning Process","text":"<pre><code>flowchart LR\n    subgraph Env[Environment]\n        S[(State)]\n        R[(Reward)]\n    end\n\n    subgraph Agent[Agent]\n        P[\"Policy \u03c0(a|s)\"]\n        V[\"Value Function V(s)\"]\n    end\n\n    S --&gt;|\"(1) State s\"| P\n    P --&gt;|\"(2) Action a\"| Env\n    Env --&gt;|\"(3) Reward r, Next State s'\"| V\n    V --&gt;|\"(4) Update Policy\"| P\n</code></pre>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#environments","title":"Environments","text":""},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#state-s-p","title":"State - \\(S, P\\)","text":"<ul> <li> <p>All RL Algorithms assume that State is Markov:     $$     P(s' \\mid s + H(s), a) = P(s' \\mid s, a)     $$</p> <ul> <li>Once \\(S\\) is known, \\(H\\) can be thrown away</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#reward-r","title":"Reward - \\(R\\)","text":"<ul> <li>A scalar feedback signal</li> <li>Indicates how well agent is doing at one step</li> <li>Reward Hypothesis<ul> <li>All\u00a0goals can be described by the \\(maximisation\\) of expected cumulative reward</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#agents","title":"Agents","text":"<ul> <li>Prediction: Evaluate the future rewards of state-actions<ul> <li>\u2192 Value Function</li> </ul> </li> <li>Control: Find the optimal policy<ul> <li>\u2192 Policy Function</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#model-p-r","title":"Model - \\(P, R\\)","text":"<ul> <li>An internal simulator for predicting what the environment will do next<ul> <li>Transition Model: \\(P(s' \\mid s,a)\\)</li> <li>Reward Model: \\(R(s,a) = \\mathbb{E}\\left[\\, R_{t+1} \\mid s_t, a \\,\\right]\\)</li> </ul> </li> <li>Simulation is NOT necessary for RL <ul> <li>\u2192 Model-based/Model-free</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#value-function-v-q","title":"Value Function - \\(V, Q\\)","text":"<ul> <li>Define and predict values of (current and future) states<ul> <li>by using the expectation of rewards</li> </ul> </li> <li> <p>State-value Function</p> <ul> <li>Return the value of current state     $$     V(s_t) = \\mathbb{E}\\big[G_t \\mid s_t\\big]     $$</li> </ul> </li> <li> <p>State-action-value Function</p> <ul> <li>Return the value of the current state with a deterministic action applied     $$     Q_\\pi(s_t,a) = \\mathbb{E}_\\pi \\big[\\, G_t \\mid s_t,\\, a \\,\\big]     $$</li> </ul> </li> <li> <p>\\(G_t\\) - the total discounted reward from time-step \\(t\\)     $$     G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}     $$</p> </li> <li> <p>Value function is NOT necessary for RL </p> <ul> <li>\u2192 Value-based Model/ Policy-based Model</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#discount-factor-gamma","title":"Discount Factor\u00a0- \\(\\gamma\\)","text":"Gamma (\\(\\gamma \\ge 0\\)) Behaviour \\(\\gamma = 0\\) Greedy \\(\\gamma \\to 0\\) Myopic \\(\\gamma \\to 1\\) Far-sighted \\(\\gamma = 1\\) Guarantee only if all sequence terminate"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#why-discounting-is-used","title":"Why Discounting is Used","text":"<ul> <li> <p>Technical Reasons</p> <ul> <li>Makes modelling and computation easier (Bellman equations converge cleanly)</li> <li>Prevents infinite returns in cyclic Markov processes</li> <li>Reflects uncertainty about far-future outcomes</li> </ul> </li> <li> <p>Realistic Reasons</p> <ul> <li>In financial settings, immediate rewards can be reinvested (time value of money)</li> <li>Human and animal behaviour shows preference for immediate rewards over delayed rewards</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#policy-pi","title":"Policy - \\(\\pi\\)","text":"<ul> <li>Fully defines agent's behaviour</li> <li>Stationary (Time-independent) - Only relies on the current state</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#implicit-policy","title":"Implicit Policy","text":"\\[ a = \\arg\\max_{a} Q(s, a) = \\arg\\max_{a} \\sum^{s'} P(s,a,s') \\cdot V(s') \\] <ul> <li>Compare expectations of all valid actions \\(\\sim\\) Brute Force</li> <li>Common use in model-based RL</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#deterministic-policy","title":"Deterministic Policy","text":"\\[ \\pi(s) = a \\] <ul> <li>MRP = MDP + restricted by the deterministic policy</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#stochastic-policy","title":"Stochastic Policy","text":"\\[ \\pi(a \\mid s) = P(a \\mid s) \\] <ul> <li>Choose the optimal action or random one with a small probability</li> <li>Ability of exploration; Robustness\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#policy-gradient","title":"Policy Gradient","text":"<ul> <li> <p>Maintain a set of parameters \\(\\theta\\) for \\(maximising\\) future rewards $$ J(\\theta) = \\mathbb{E}(G) = \\sum_{\\tau} P(\\tau \\mid \\theta) \\, G(\\tau) $$</p> <ul> <li>\\(\\tau\\) - A full path from initial state to the goal</li> </ul> </li> </ul> \\[ \\begin{aligned} \\nabla J(\\theta) &amp;= \\sum_{\\tau} \\nabla P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(definition of expected return)} \\\\[4pt] &amp;= \\sum_{\\tau} P(\\tau \\mid \\theta) \\, \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(apply log-derivative trick)} \\\\[4pt] &amp;= \\mathbb{E}_{\\tau \\sim P(\\tau \\mid \\theta)} \\big[ \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) \\big] &amp;&amp; \\text{(convert sum to expectation)} \\\\[4pt] &amp;= \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla \\log \\pi_\\theta(a \\mid s) \\, G(\\tau) \\big] &amp;&amp; \\text{(expand trajectory likelihood)} \\end{aligned} \\]"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#balance-exploration-exploitation-in-policy-design","title":"Balance Exploration &amp; Exploitation in Policy Design","text":"<ul> <li>Exploitation - select currently-known best action</li> <li>Exploration - try a new action</li> <li> <p>Trade-off Strategies</p> <ul> <li>\u03b5-greedy <ul> <li>most time exploitation (\\(P = 1 - \\varepsilon\\)), little time exploration (\\(\\varepsilon\\))</li> </ul> </li> <li> <p>softmax </p> <ul> <li>set a probability to each action and a temperature \\(\\tau\\) to control\u00a0randomising     $$     P(a) = \\frac{\\exp(Q(a) / \\tau)}{\\sum_{a'} \\exp(Q(a') / \\tau)}     $$</li> </ul> </li> <li> <p>Upper Confidence Bound (UCB) </p> <ul> <li> <p>exploitation + exploration bonus     $$     a = \\arg\\max_a \\left( Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right)     $$</p> <ul> <li>\\(c\\) - importance of exploration (parameter)</li> <li>\\(\\ln{t}\\) - explore more when time goes</li> <li>\\(N(a)\\) - not try too many times on one same action</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#example-maze","title":"Example - MAZE","text":"<ul> <li>\\(S\\) - Agent's possible locations</li> <li>\\(A\\) - Step directions \\(\\mathtt{N, E, S, W}\\)</li> <li>\\(R\\) - \\(-1\\) per time-step (encourage short-path solution)</li> <li>\\(V(s)\\) - the expected return of following the policy from each \\(s\\) <ul> <li>closer to \\(g\\) - \\(V(s)\\) \u2934</li> <li>Farther away - \\(V(S)\\) \u2935</li> </ul> </li> <li>\\(\\pi(s)\\) - Best action = Best \\(V(s')\\)</li> <li>Model<ul> <li>Transition model \\(P_{ss'}^a\\) - how each action changes the state.</li> <li>Reward model \\(R_{s}^a\\) - immediate reward from each state (same for all \\(a\\)).</li> <li>The model can be imperfect but supports planning and prediction.</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/6-Reinforced-Learning/#categories-of-rl-algorithms","title":"Categories of RL Algorithms","text":"<ul> <li>Model?<ul> <li>Model Based - Simulate environment to gain first experience\u00a0</li> <li>Model Free - Gain experience from real interaction directly</li> </ul> </li> <li>Value-or-Policy?<ul> <li>Value Based - Implicit Policy</li> <li>Policy Based - No Value Function = Policy Gradient</li> <li>Actor-Critic - Policy Based \u6f14\u5458 + Value Based \u8bc4\u8bba\u5bb6</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/","title":"7. Model-based Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#markov-decision-process","title":"Markov Decision Process","text":"<ul> <li>MDP - General Simulator of the environment for Model-based RL</li> <li>Almost all RL problems can be formalised as MDPs</li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#markov-process","title":"Markov process","text":"<ul> <li> <p>A memoryless\u00a0random process      $$     M = \\langle S, P \\rangle     $$</p> <ul> <li>\\(S\\) - a finite set of states</li> <li>\\(P\\) - a state transition probability matrix, which maps every two states in \\(S\\)<ul> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>Each row of \\(P\\) sums to 1</li> <li>\ud83c\udd44 For showing actions validations and non-deterministic</li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#markov-reward-processes","title":"Markov Reward Processes","text":"<ul> <li> <p>A Markov chain with reward values:     $$     M = \\langle S, P, R, \\gamma \\rangle     $$</p> <ul> <li>\\(R\\) - the reward function<ul> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> <li>\\(\\gamma\\) - discount factor</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#markov-decision-process_1","title":"Markov Decision Process","text":"<ul> <li> <p>Introduce agency \u80fd\u52a8\u6027 in terms of actions:     $$     M = \\langle S, A, P, R, \\gamma \\rangle     $$</p> <ul> <li>\\(A\\) - a finite set of actions</li> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#solving-a-mdp-problem","title":"Solving a MDP Problem","text":"<ul> <li>Solve MDP</li> <li>= Give a policy to find the best behaviour of the current state</li> <li>= Find an action with \\(maximum\\) action-value</li> <li>= Optimise the Value Function</li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#optimal-value-function-v_asts-q_asts-a","title":"Optimal Value Function - \\(v_\\ast(s)\\), \\(q_\\ast(s, a)\\)","text":"<ul> <li> <p>Optimal State-value Function $$ v_\\ast(s) = \\max_\\pi v_\\pi(s) $$</p> </li> <li> <p>Optimal State-action-value Function $$ q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a) $$</p> </li> </ul>"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#bellman-equation","title":"Bellman Equation","text":"<ul> <li> <p>A method for recursively handling with \\(v(s)\\) evaluation and optimisation         $$ G_t = R_{t+1} + \\gamma G_{t+1} $$</p> <ul> <li>Based on \"look-ahead then back-up\" computational mechanism<ul> <li>Look-ahead: to the next step \u2192 \\(r, s'\\)</li> <li>Back-up: to the current state \u2192 \\(v(s) = f(r,s')\\)</li> </ul> </li> </ul> </li> </ul> Categories Formula Expectation $$ v_\\pi(s) = \\mathbb{E}\\left[r + \\gamma v_\\pi(s')\\right] $$ $$ q_\\pi(s,a) = \\mathbb{E}\\left[r + \\gamma\\mathbb{E} \\left[q_\\pi(s',a')\\right]\\right] $$ Expended $$ v_\\pi(s) = \\sum_a \\pi(a\\mid s)\\sum_{s',r} p(s',r\\mid s,a)\\bigl[r + \\gamma v_\\pi(s')\\bigr] $$ $$ q_\\pi(s,a) = \\sum_{s',r} p(s',r\\mid s,a)\\left[r + \\gamma \\sum_{a'} \\pi(a'\\mid s') q_\\pi(s',a')\\right] $$ Optimality $$ v_\\ast(s) = \\max_a \\mathbb{E}\\left[\\, r + \\gamma v_\\ast(s') \\,\\right] $$ $$ q_\\ast(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a'} q_\\ast(s',a')\\right] $$ Expended $$ v_\\ast(s) = \\max_{a} \\sum_{s'} P(s' \\mid s,a)\\left[ R(s,a,s') + \\gamma v_\\ast(s') \\right] $$ $$ q_\\ast(s,a) = \\sum_{s',r} p(s',r\\mid s,a)\\left[r + \\gamma \\max_{a'} q_\\ast(s',a')\\right] $$"},{"location":"Unimelb/25S2/AIP/7-Model-based%20RL/#solving-the-bellman-equation","title":"Solving the Bellman Equation","text":"<ul> <li> <p>Solving the Bellman Expectation Equation</p> <ul> <li> <p>Directly Compute \\(O(n^3)\\) $$ v = R + \\gamma Pv \\to v = (I - \\gamma P)^{-1}R $$</p> <ul> <li>only possible for small \\(P\\) matrix</li> </ul> </li> <li> <p>Dynamic Programming</p> </li> <li>Monte-Carlo Evaluation</li> <li>Temporal-Difference Learning</li> </ul> </li> <li> <p>Solving the Bellman Optimality Equation</p> <ul> <li>No closed form solution</li> <li>Value Iteration</li> <li>Policy Iteration</li> <li>Q-learning</li> <li>SARSA</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/","title":"8. Model Free Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#monte-carlo-mc-learning","title":"Monte Carlo (MC) Learning","text":"<ul> <li>Set the state as initial state and run-to-end in the environment for multiple times</li> <li>Calculate the average reward of them: value = mean return</li> <li>Note: can only apply to episodic MDPs \uff08\u6709\u7ec8\u6b62\u72b6\u6001\u7684Markov Decision Process)</li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#mc-prediction","title":"MC Prediction","text":""},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#classical-policy-evaluation","title":"Classical Policy Evaluation","text":"<ul> <li> <p>Compute value-function by using empirical mean instead of expectation     $$     V(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)}     $$</p> <ul> <li>\\(N(s)\\) - how many times \\(G_t\\) is visited</li> <li>when \\(N(s) \\to \\infty\\), \\(V(s) \\to v_\\pi(s)\\).</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#first-visit-vs-every-visit","title":"First-Visit vs. Every-Visit","text":"<ul> <li>In each episode, when the state will be counted and updated</li> </ul> Aspect First-Visit MC Every-Visit MC When? Only the first time appeard Every time appeard Pros Avoids dependence between multiple visits Faster convergence in practice Cons Some visits ignored Higher variance Suitable Scenario Long episodes with repeated states Short episodes or rare-state situations"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#alpha-style-prediction","title":"\\(\\alpha\\)-style prediction","text":"<ul> <li> <p>In practice, we often use learning rate \\(\\alpha\\) to replace strict mean:     $$     V(s) \\gets V(s) + \\alpha(G_t - V(s))     $$</p> <ul> <li>online learning supported</li> <li>better performance in non-stationary environment</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#mc-control","title":"MC Control","text":""},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<ul> <li>Key Mechanisms: bootstrap + sample<ul> <li>Sampling: an \"1-step experience\" (TD(0))<ul> <li>\\(e = \\{S_t \\to A_t \\to R_{t+1}, S_{t+1}\\}\\)</li> </ul> </li> <li>Bootstrapping: use estimation of future values to update the current value</li> </ul> </li> </ul> \\[ \\text{TD-target} = \\text{sample} + \\gamma \\cdot \\text{bootstrap} \\]"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#td-prediction","title":"TD Prediction","text":""},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#td0","title":"TD(0)","text":"<ul> <li> <p>Look 1-step $$ V(S_t) \\gets V(s_t) + \\alpha(R_{t+1} + \\gamma V(s_{t+1})-V(s_t)) $$</p> </li> <li> <p>\\(R_{t+1}\\) - real reward</p> </li> <li>\\(\\delta = R_{t+1} + \\gamma V(s_{t+1})-V(s_t)\\) - TD Error</li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#tdn","title":"TD(n)","text":"<ul> <li>Look more steps into the future $$ G^{(n)}t = R{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + {{\\gamma^n V(S_{t+n})}} $$</li> </ul> \\[ \\begin{align*} \\color{red}{n} &amp; \\color{red}{= 1}\\ \\text{(TD(0))}      &amp; G_t^{(1)}      &amp; = R_{t+1} + \\gamma V(S_{t+1})\\\\[2pt] \\color{red}{n} &amp; \\color{red}{= 2}                   &amp; G_t^{(2)}      &amp; = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})\\\\[2pt] \\color{red}{\\vdots} &amp;                   &amp; \\vdots         &amp; \\\\[2pt] \\color{red}{n} &amp; \\color{red}{= \\infty}\\ \\text{(MC)} &amp; G_t^{(\\infty)} &amp; = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_T \\end{align*} \\] <ul> <li> <p>Update in direction of error ([#\\(\\alpha\\)-style prediction]) $$ V(S_t) \\;\\leftarrow\\; V(S_t) + \\alpha \\big({{G_t^{(n)} - V(S_t)}} \\big) $$</p> </li> <li> <p>Root Mean Square (RMS) Errors \u5747\u65b9\u5dee\u6839 \u2190 \ud835\udefc; online/offline updates (\ud835\udefc \u2192 0 or \ud835\udefc \u2192 1)</p> </li> <li>RMS Errors \u2192 different optimal choices of n</li> <li>Small n \u2192 More rely on prediction, faster but higher bias</li> <li>Large n \u2192 More rely on exploitation, more precise but higher variance</li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#td","title":"TD(\ud835\udf40)","text":"<ul> <li> <p>Average n-Steps Returns</p> </li> <li> <p>G(t; \ud835\udf06) = (1 - \ud835\udf06) \u00b7 \ud835\udef4 \ud835\udf06\u207f\u207b\u00b9 G(t; n)</p> </li> <li> <p>Using weight (1 - \ud835\udf06) \u00b7 \ud835\udf06\u207f\u207b\u00b9 to balance short-term and long-term rewards</p> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#eligibility-traces","title":"Eligibility Traces","text":"<ul> <li>Simplify calculation of G(t; \ud835\udf40**)</li> <li> <p>Define a trace E(t; s) of every state = How many time visited and how far from current state</p> <ul> <li>E(t; s) \u2009=\u2009 \ud835\udefe\ud835\udf06E(t-1; s) + 1 if S(t)\u2009=\u2009s else 0</li> </ul> </li> <li> <p>If we visit the state, it\u2019s trace goes up suddenly (by 1), if we don\u2019t visit it, it falls down continuously</p> <ul> <li>V(s) \u2190 V(s) + \ud835\udefc \u00b7 \ud835\udeff \u00b7 E(t; s)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#forward-vs-backward-view","title":"Forward vs. Backward View","text":"<ul> <li>Original way calculating G(t; \ud835\udf06) is a Forward View of \ud835\udf06-Reward, we have to wait episodes terminate so that we can calculate all G(t; n) (offline)</li> <li>Eligibility Traces is a Backward View of \ud835\udf06-Reward, we update prediction of state step by step(online)<ul> <li>Backward Propagation - Use Eligibility Traces to propagate TD error</li> <li>state visited more times\u00a0 = more responsible on current TD error</li> </ul> </li> <li>Forward View = Backward View (in Batch/offline update)</li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#mc-vs-td","title":"MC vs. TD","text":"Aspect Monte-Carlo (MC) Temporal-Difference (TD) Learning timing Offline - Must wait until the episode ends Online - Learn after each step Assumption Needed episodic (terminating) environment only both episodic and continuing Target used \\(G_t\\) \\(R_{t+1} + \\gamma V(S_{t+1}\\)) Exploits Markov property? \u274c \u2705 Bias Unbiased (return is true target) Biased (bootstraps using V(s\u2032)) Variance High (Every-visit) Low B-V Trade-off Bias = 0, Variance \u2191 Bias \u2191, Variance \u2193 Efficiency in Markov env \u26a0\ufe0fLess efficient \u2705More efficient Efficiency in non-Markov or PMDP env \u2705More efficient with backup learning \u26a0\ufe0fLess efficient Optimisation in limited experience (Batch) Average Return = minimise MSE Fit the most likelihood Markov model that best explains the data"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#example-driving-home","title":"Example - Driving Home","text":"State Elapsed Time (min) Predicted Time to Go Predicted Total Time MC Update (After Back Home) MC New Predicated Time TD(0) Update - \\(\\gamma = 1, \\alpha = 0.5\\) TD New Predicated Time leaving office 0 30 30 43 43-0=43 30+0.5(5+35-30)=35 35-0=35 reach car, raining 5 35 40 43 43-5=38 40+0.5(20+15-40)=37.5 37.5-5=32.5 exit highway 20 15 35 43 43-20=23 35+0.5(30+10-35)=37.5 37.5-20=17.5 behind truck 30 10 40 43 43-30=13 40+0.5(40+3-40)=41.5 41.5-30=11.5 home street 40 3 43 43 43-40=3 43+0.5(43+0-43)=43 43-40=3 arrive home 43 0 43 43 43-43=0 43 43-43=0 ##### Example - BATCH <ul> <li>2-states: \\(A\\), \\(B\\)</li> <li>Reward Signal = \\(\\{0, 1\\}\\)</li> <li>Sampled Episodes:<ul> <li>\\(A \\to 0\\), \\(B \\to 0\\), \\(B \\to 1\\), \\(B \\to 1\\), \\(B \\to 1\\), \\(B \\to 1\\), \\(B \\to 1\\), \\(B \\to 1\\), \\(B \\to 0\\)</li> </ul> </li> <li>MC Update:<ul> <li>\\(V(B) = 6/8 = 0.75\\)</li> <li>\\(V(A) = 0 / 1 = 0\\)</li> </ul> </li> <li>TD(0) Update:<ul> <li>\\(V(B) \\to0.75\\)</li> <li>\\(V(A) \\to V(B) \\to 0.75\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/8-Model-Free-RL/#model-based-vs-model-free","title":"Model-based vs. Model-free","text":"Aspect Backup Bootstrap? Sampling? Pros Cons MC Full return \u274cOnly real return \u2705Whole episode Unbiased; simple; intuitive High Variance; episodic needed; slow convergence TD(0) Next Step \u2705Estimate by next state V \u27051-Step Low variance; online Biased; se Classical DP/Tree Search Full Expectation \u2705model-based bootstrapping \u274cAll branches Precise; Fast convergence; High Controllability Model needed; Low Generalisation"}]}