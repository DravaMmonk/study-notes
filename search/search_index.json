{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Drava's Study Notes","text":""},{"location":"#master-of-information-technology-in-the-university-of-melbourne","title":"\ud83c\udf93 Master of Information Technology in the University of Melbourne","text":""},{"location":"#2025-semester-2","title":"\ud83d\uddd3\ufe0f 2025 Semester 2","text":""},{"location":"#ai-planning-for-autonomy-comp90054","title":"\u270f\ufe0f AI Planning for Autonomy - COMP90054","text":""},{"location":"#other","title":"\ud83d\udcbb Other","text":""},{"location":"#reference","title":"\ud83d\udcda Reference","text":""},{"location":"Unimelb/25S2/AIP/01-Search/","title":"1 Search","text":""},{"location":"Unimelb/25S2/AIP/01-Search/#search-space","title":"Search Space","text":"<ul> <li>A set of search states</li> <li>Forward Search (Progression)<ul> <li>search space = world space (represents the current real world)</li> </ul> </li> <li>Backward Search (Regression)<ul> <li>search space = a sets of world spaces (represents all predications of sub-goals)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#example","title":"EXAMPLE","text":"<p>In a robot delivery scene: <pre><code>world_state_of_a_robot = {\n\u00a0 \u00a0 \"position\": (3, 5),\n\u00a0 \u00a0 \"direction\": \"North\",\n\u00a0 \u00a0 \"battery\": 80,\n\u00a0 \u00a0 \"carrying_package\": True\n}\n</code></pre></p> <ul> <li>Progression: \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5c31\u662f\u673a\u5668\u4eba\u5728\u67d0\u4e2a\u4f4d\u7f6e\u3001\u671d\u5411\u3001\u80fd\u91cf\u3001\u8d1f\u8f7d\u7684\u7ec4\u5408\u72b6\u6001\u3002</li> <li>Regression:\u00a0 \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5171\u540c\u7ec4\u6210\u201c\u6211\u4eec\u60f3\u8981\u6240\u6709\u6ee1\u8db3\u4e00\u4e2a\u6761\u4ef6\u7684\u4e16\u754c\u72b6\u6001\u96c6\u5408\u201d\u3002</li> </ul> <pre><code>subgoals = {\"position\": (5, 8), \"carrying_package\": True}\n</code></pre> <p>Common Functions: </p> <ul> <li>\\(s\\) = search states</li> <li>\\(\\mathrm{is}\\_\\mathrm{start}(s)\\) return if the state is the start state of the search space</li> <li>\\(\\mathrm{is}\\_\\mathrm{target}(s)\\) mark if the state is the goal state of the search space</li> <li>\\(\\mathrm{succ}(s)\\) return a list of successors/next states of \\(s\\)</li> <li>Search nodes:\u00a0<ul> <li>\\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(\\mathrm{parent}(\\sigma)\\) where \\(\\sigma\\) was reached</li> <li>\\(\\mathrm{action}(\\sigma)\\) leads from \\(\\mathrm{state}(\\mathrm{parent}(\\sigma))\\) to \\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(g(\\sigma)\\) denotes cost of path from the root to \\(\\sigma\\)</li> <li>The root\u2019s \\(\\mathrm{parent}(\\cdot)\\) and \\(\\mathrm{action}(\\cdot)\\) are undefined</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#search-methods","title":"Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search/#blind-search-vs-informed-search","title":"Blind Search vs. Informed Search","text":"<ul> <li>Blind search not require any input beyond the problem</li> <li>No additional work but rarely effective</li> <li>Informed search requires function \\(h(x)\\) mapping states to estimates their goal distance</li> <li>Effective but lots of work to construct \\(h(x)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#blind-systematic-search-algorithms","title":"Blind Systematic Search Algorithms","text":"<ul> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>Iterative Deepening Search<ul> <li>Do DLS(DFS with depth limited) with continuously increasing depth limited by 1.</li> </ul> </li> <li>Blind Search: Completeness 100%, but poor efficient when doing hard work</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#heuristic-functions","title":"Heuristic Functions","text":"<ul> <li>\\(h(n)\\) - Estimated remaining cost (from current state to goal state)</li> <li>\\(h^*(n)\\) - Real remaining cost</li> <li>proficiency of \\(h(n)\\) <ul> <li>\\(h = h^{\\ast}\\) perfectly informed, \\(h(n) = h^{\\ast}(n) - \\textbf{optimal } A^{\\ast}\\)</li> <li>\\(h = 0\\) no information at all - uniform cost search</li> </ul> </li> </ul> Properties Description Safe \\(h(n) = \\infty\\) iff \\(h^{\\ast}(n) = \\infty\\) Goal-Aware \\(h(\\text{goal}) = 0\\) Admissible \\(h(n) \\le h^*(n)\\) Consistent \\(h(n) \\le c(n,n\u2019) + h(n\u2019)\\) for all possible \\(c(n, n\u2019)\\)"},{"location":"Unimelb/25S2/AIP/01-Search/#relations-of-properties","title":"Relations of properties","text":"<ul> <li>Consistent &amp; Goal-aware \\(\\to\\) Admissible</li> <li>Admissible \\(\\to\\) Safe &amp; Goal-aware</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#informed-systematic-search-algorithms","title":"Informed Systematic Search Algorithms","text":""},{"location":"Unimelb/25S2/AIP/01-Search/#greedy-best-first-search-gbfs","title":"Greedy Best-First Search (GBFS)","text":"<ul> <li>Use priority queue to sort \\(h(n)\\) of each node in ascending order<ul> <li>If \\(h(n) = 0\\), it becomes what fully depends\u00a0on how\u00a0we\u00a0break\u00a0ties</li> </ul> </li> </ul> <pre><code>def greedy_BFS:\n\u00a0 \u00a0 frontier = priority queue ordered by h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <ul> <li>Completeness\u2705for safe heuristics; Optimal\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#a","title":"A*","text":"<ul> <li>Only difference from GBFS:<ul> <li>\\(h(n) \\rightarrow f(n) = g(n) + h(n)\\)</li> </ul> </li> </ul> <pre><code>def a_star:\n\u00a0 \u00a0 frontier = priority queue ordered by f(n) = g(n) + h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(h(start), start, path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 _, current, path = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f = cost(new_path) + h(current)\n\u270f\ufe0f\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(f, successor, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre>"},{"location":"Unimelb/25S2/AIP/01-Search/#re-opening","title":"Re-opening","text":"<ul> <li>a node \\(n\\) is in <code>explored</code> but if we find a cheaper \\(g(n)\\), then we can re-open the <code>explored</code> set and extend this node</li> <li>Not needed if consistent\u2705<ul> <li>\\(g\\) is already cheapest</li> </ul> </li> </ul> <pre><code>def a_star_with_re_opening:\n    frontier = priority queue ordered by f(n) = g(n) + h(n)\n    explored = set\n    path = list\n\u00a0 \u00a0 frontier.add(h(start), start, path)\n\n\u270f\ufe0f  g = dict\n\u270f\ufe0f  g[start] = 0\n\n    while frontier:\n        _, current, path = frontier.pop()\n        if current == goal:\n            return path\n        explored.add(current)\n        for successor in succ(current):\n            new_path = path + action(current, successor)\n            new_g = cost(new_path)\n            if successor not in g or new_g &lt; g[successor]:\n                [successor] = new_g\n                f = g[successor] + h(successor)\n                frontier.add(f, successor, f)\n\n\u270f\ufe0f              if successor in explored:     # Re-opening\n\u270f\ufe0f                  explored.remove(successor)\n    return unsolvable\n</code></pre>"},{"location":"Unimelb/25S2/AIP/01-Search/#weighted-a","title":"Weighted A*","text":"\\[ f_W(n) = g(n) + W \\cdot h(n) \\] Weight Algorithm \\(W \\to 0\\) Digkstra Algorithm \\(W \\to 1\\) A* \\(W &gt; 1\\) Bounded sub-optimal A* \\(W \\to \\infty\\) GBFS <ul> <li>If \\(h\\) is admissible, \\(f_W(n) \\le W \\cdot h(n)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#hill-climbing","title":"Hill-Climbing","text":"<p><pre><code>def hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break \u00a0 \u00a0\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Can only find local maxima - Make sense only if \\(h(n) &gt; 0\\) for all non-goal states</p>"},{"location":"Unimelb/25S2/AIP/01-Search/#enforced-hill-climbing","title":"Enforced Hill-Climbing","text":"<p><pre><code>def enforced_hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subs = n for n in neighbours_of(current) and not in explored\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 best_sub = argmin_h(subs, goal=current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if best_sub and h(best_sub) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path -= action(parent, current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(parent, best_sub)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = best_sub\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Do small range BFS when find local optimal - Can across small gap between local best and global best</p>"},{"location":"Unimelb/25S2/AIP/01-Search/#iterative-deepening-a","title":"Iterative Deepening A*","text":"<ul> <li>IDS + \\(A^*\\): Use f(n) instead of depth to limit IDS<ul> <li>In First Search:\u00a0 f(n) = f(start) = 0 + h(start)</li> <li>Following Searches: f(n) = min_out_of_bound_excess <pre><code>def ida_star:\n\u00a0 \u00a0 bound = f(start)\n\u00a0 \u00a0 while True:\n\u00a0 \u00a0 \u00a0 \u00a0 t = ids(start, bound)\n\u00a0 \u00a0 \u00a0 \u00a0 if t == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return solution\n\u00a0 \u00a0 \u00a0 \u00a0 if t == infinity:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 \u00a0 \u00a0 bound = t\n</code></pre></li> </ul> </li> <li>Dealing with one of \\(A^*\\)\u2019s problem: large queue/closed_set</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#evaluation-of-search-methods","title":"Evaluation of Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search/#guarantees","title":"Guarantees","text":"<ul> <li>Completeness sure to find a solution if there is one</li> <li>Optimality solutions sure be optimal</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#complexity","title":"Complexity","text":"<ul> <li>Time/Space (Measured in generated states/states cost)</li> <li>Typical state space features governing complexity<ul> <li>Branching factor \\(b\\) how many successors</li> <li>Goal depth \\(d\\) number of actions to reach shallowest goal state</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search/#summary","title":"Summary","text":"DFS BrFS IDS A* HC IDA* Complete \u274c \u2705 \u2705 \u2705 \u274c \u2705 Optimal \u274c \u2705* \u2705 \u2705* \u274c \u2705* Time \\(\\infty\\) \\(b^d\\) \\(b^d\\) \\(b^d\\) \\(\\infty\\) \\(b^d\\) Space \\(b \\cdot d\\) \\(b^d\\) \\(b \\cdot d\\) \\(b^d\\) \\(b\\) \\(b \\cdot d\\) <ul> <li>\\(b\\) - Branching Factor: sum of number of child nodes of each node</li> <li>\\(d\\) - Solution Depth: \\(minimum\\) depth among all goal nodes</li> <li>DFS cannot handle cyclic graph</li> <li>BFS is optimal only when uniform costs are applied </li> <li>\\(A^*\\) / Iterative Deepening \\(A^*\\) is optimal only when \\(h\\) is admissible (\\(h \\le h^*\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/","title":"2 Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning/#problem-solving","title":"Problem Solving","text":""},{"location":"Unimelb/25S2/AIP/02-Planning/#autonomous-behaviour-in-ai","title":"Autonomous Behaviour in AI","text":"<ul> <li>Programming-based</li> <li>Learning-Based</li> <li>Model-Based</li> <li>Approaches not orthogonal</li> <li>Different models yield different types of controllers \u6267\u884c\u52a8\u4f5c\u7684\u7b56\u7565\u6a21\u5757</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#3-level-solver-ambitions","title":"3-level Solver Ambitions","text":"<ul> <li>Ambition: Write one program to solve all classical search problems<ul> <li>Ambition 1.0 (Problem Solving) Write one program to solve a problem</li> <li>Ambition 2.0 (Problem Generation) Write one program to solve a large class of problems</li> <li>Ambition 3.0 (Meta Problem Solving) Write one program to solve a large class of problems effectively</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#solving-approaches","title":"Solving Approaches","text":""},{"location":"Unimelb/25S2/AIP/02-Planning/#programming-based-approach","title":"Programming-Based Approach","text":"<ul> <li>Pro: Domain-knowledge easy to express</li> <li>Con: Less Flexible (Can\u2019t deal with situations not anticipated by programmer)</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#learning-based-approach","title":"Learning-Based Approach","text":"<ul> <li>Unsupervised/Reinforced Learning: Use reward &amp; penalise</li> <li>Supervised: Use labelled data</li> <li>Evolutionary: Use original controllers to mutate and recombine to build better controller</li> <li>Pros Dose not require much knowledge in principle\u00a0</li> <li>Cons Slower; Hard to know which features to learn\u00a0</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#model-based-approach-planning","title":"Model-Based Approach (Planning)","text":"<ul> <li>One model for one specific problem </li> <li>Pros Powerful; Quick; Flexible; Clear; Intelligent; Domain-independent</li> <li>Cons Need a model (sometimes very hard); Efficiency loss</li> <li>Always need trade-off between \u201cAutomatic and general\u201d vs. \u201cManual work but effective\u201d</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#planning-models","title":"Planning Models","text":""},{"location":"Unimelb/25S2/AIP/02-Planning/#classical-planning-model","title":"Classical Planning Model","text":"<ul> <li>Classical Planning Model = Basic State Model</li> <li> <p>Assumptions</p> <ul> <li>Deterministic</li> <li>Fully Observable</li> <li>Static World</li> <li>Discrete Time &amp; Finite Actions</li> <li>Uniform Cost</li> </ul> </li> <li> <p>Components </p> \\[ M = \\langle S, A, T, I, G \\rangle \\] <ul> <li>\\(S\\) - State spaces </li> <li>\\(A(s)\\) - Actions applicable for \\(s \\in S\\)</li> <li>\\(T\\) - Deterministic Transition Function: \\(s\u2019 = T(A(s), s)\\) shows one successor \\(s\u2019\\) of \\(s\\)</li> <li>\\(I\\) - Initial state</li> <li>\\(G\\) - Goal states</li> <li>Uniform action costs \\(c(A(s), s) = 1\\)</li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>A seq of actions map \\(s_0\\) into \\(g\\)</li> <li>Optimal if total cost to goal is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#conformant-planning-model","title":"Conformant Planning Model","text":"<ul> <li>\\(\\approx\\) Brute Force</li> <li>Components (Diff with classic)<ul> <li>Initial State \u2192 A set of possible initial states</li> <li>Deterministic Transition Function \u2192 Non-deterministic</li> </ul> </li> <li>Outcomes<ul> <li>No Observation - No new info; must pre-planning</li> <li>Goal Guarantee</li> <li>Rarely optimal<ul> <li>Must map any possible \\(s_0\\) to \\(g\\), too much unnecessary work</li> <li>Sensitive to Worst/Special case </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#markov-decision-processes-mdps","title":"Markov Decision Processes (MDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li> <p>Markov Property: the next state only depends on the current state and actions.</p> \\[ P(s_{t+1} \\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots) = P(s_{t+1} \\mid s_t,a_t)  \\] </li> <li> <p>Fully Observable</p> </li> <li>Discrete Time &amp; Finite Actions</li> <li>Fixed Transition Probabilities</li> </ul> </li> <li> <p>Components (Diff with conformant)     $$     M = \\langle S,A,T,R,\\gamma \\rangle     $$</p> <ul> <li>Introduce the Transition Probability Function:     $$     T(s, a, s') = P(s' \\mid s, a)     $$</li> <li>Introduce Reward Function \\(R\\) and Discount Factor \\(\\gamma\\):<ul> <li>For estimating the value of each state</li> </ul> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map states to actions</li> <li>Optimal if total expected cost to goal is \\(minimum\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning/#partially-observable-mdps-pomdps","title":"Partially Observable MDPs (POMDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li>Markov Property</li> <li>Limited Observable</li> </ul> </li> <li> <p>Components (Diff with MDPs)</p> <ul> <li> <p>Introduce Sensor Model (based on Probability Distribution)</p> \\[ \\mathrm{b}(s_t) = P(s_t \\mid \\text{historical actions and observations}) \\] </li> <li> <p>Initial/Goal States \u2192 Belief States \\(\\mathrm{b}(s_0)\\) and \\(\\mathrm{b}(g)\\)</p> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map belief states into actions</li> <li>Optimal if total expected cost from \\(\\mathrm{b}(s_0)\\) to \\(\\mathrm{b}(g)\\) is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/","title":"3 Complexity","text":""},{"location":"Unimelb/25S2/AIP/03-Complexity/#savitchs-theorem","title":"Savitch\u2019s Theorem","text":"<ul> <li>NPSPACE = PSPACE<ul> <li>Non-determinism won't make solvers more powerful.</li> </ul> </li> <li>PSPACE-complete<ul> <li>The problem is at least hard as any Polynomial time problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#goals-definition","title":"Goals Definition","text":"<ul> <li>To make a Satisficing planning:<ul> <li>\\(\\mathrm{PlanEx}(P) =\\) <code>TRUE</code> - Existence of a plan for problem \\(P\\).</li> </ul> </li> <li>To make a Optimal planning:<ul> <li>\\(\\mathrm{PlanLen}(P) \\le B\\)  - Existence of a plan which length is at most \\(B\\).</li> </ul> </li> <li>\\(\\mathrm{PlanEx}\\) and \\(\\mathrm{PlanLen}\\) is PSPACE-complete<ul> <li>In practice, optimal planning is almost never easy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#strips-planning-problem","title":"STRIPS planning problem","text":""},{"location":"Unimelb/25S2/AIP/03-Complexity/#strips-stanford-research-institute-problem-solver","title":"STRIPS = STanford Research Institute Problem Solver","text":"<ul> <li>Design Goals:<ul> <li>Specification - concise model description</li> <li>Computation - reveal useful information/structure for heuristics</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#components","title":"Components","text":"\\[ P = \\langle P,A,I,G \\rangle \\] <ul> <li>\\(P\\) - All Possible Predicates</li> <li>\\(A\\) - Actions</li> <li>\\(I\\) - Initial States: Predicates initially to be <code>TRUE</code></li> <li>\\(G\\) - Goal Conditions: Predicates Finally need to be <code>TRUE</code></li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#action","title":"Action","text":"\\[ a \\in A, a = \\langle Pre(a), Add(a), Del(a) \\rangle \\] <ul> <li>\\(Pre(a)\\) - Preconditions need to be <code>TRUE</code> before executing the action</li> <li>\\(Add(a)\\) - Add Effects: Preconditions set to be<code>TRUE</code> after execution</li> <li>\\(Del(a)\\) - Delete Effects: Preconditions set to be <code>FALSE</code> after execution</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#result-of-applying-actions","title":"Result of applying actions","text":"\\[ Result(s, a) = (s - Del(a)) \\cup Add(a) \\] <ul> <li>which could also be seemed as a transition function</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#goal","title":"Goal","text":"<ul> <li>Find a list of actions \\([a_1,a_2,a_3,\\ldots]\\) which satisfies:     $$     Result(Result(\\ldots Result(I,a_1\u200b) \\ldots ,a_n\u22121\u200b),a_n\u200b) \\models G     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Complexity/#properties","title":"Properties","text":"<ul> <li>It turns a \"solving\" problem into a \"search\" problem</li> <li>Still PSPACE-complete, but \"approximation\" is able to be applied<ul> <li>Explicit Search \u663e\u5f0f\u641c\u7d22<ul> <li>e.g. Blind/Heuristic search</li> <li>Not effective</li> </ul> </li> <li>Near Decomposition \u8fd1\u4f3c\u5206\u89e3<ul> <li>e.g. Relaxation: similar to Decrease/Divide &amp; Conquer</li> <li>Maybe fail</li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/","title":"4 Relaxation","text":""},{"location":"Unimelb/25S2/AIP/04-Relaxation/#relaxation","title":"Relaxation","text":"<ul> <li>A method to compute heuristic functions \\(h(n)\\)<ul> <li>Defines a transformation for simplifying the STRIPS problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#components","title":"Components","text":"\\[ \\mathcal{R} = (\\mathcal{P'}, r, h'^*) \\] <ul> <li>\\(\\mathcal{P}'\\) - the class of the simpler problem </li> <li>\\(r\\) - transformer turning the original problem into a simplified one</li> <li>\\(h'^*(n)\\) - Perfect heuristic function of the simplified problem </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#properties","title":"Properties","text":"<p>The Relaxation is - Native - if \\(\\mathcal{P'} \\subseteq \\mathcal{P}\\) and  \\(h'^*(n) = h(n)\\) - Efficiently Constructible - if a polynomial \\(r\\) exists - Efficiently Computable - if a polynomial \\(h\u2019(n)\\) exists</p>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#examples","title":"Examples","text":"<ol> <li>Route-Finding<ul> <li>Relaxation<ul> <li>Route-find as a bird (ignoring the road)</li> </ul> </li> <li>Outcome<ul> <li>Road Distance \u2192 Manhattan distance, Euclidean distance, etc.</li> </ul> </li> <li>Native\u274c Efficiently constructible\u2705 Efficiently computable\u2705</li> </ul> </li> <li>Goal-Counting<ul> <li>Relaxation<ul> <li>Assume we can achieve each goal directly</li> </ul> </li> <li>Outcome<ul> <li>Tasks \u2190 No precondition and delete</li> </ul> </li> <li>Admissible but still NP-hard</li> <li>Native\u2705 Efficiently constructible\u2705 Efficiently computable\u274c</li> </ul> </li> </ol>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#still-inefficiency","title":"Still Inefficiency?","text":"<ul> <li>Approximate \\(h\u2019^*\\)</li> <li>Re-design \\(h\u2019^*\\) in a way so that it will typically be feasible<ul> <li>Critical path heuristics</li> <li>Delete relaxation \u2190 wide-spread for satisficing planning</li> <li>Abstractions</li> <li>Landmarks</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#delete-relaxation","title":"Delete Relaxation","text":"<ul> <li>Apply \\(Del(a) = \\emptyset, \\forall a \\in A\\)<ul> <li>which makes \"once\"  <code>TRUE</code> predications remain <code>TRUE</code> \"forever\"</li> </ul> </li> <li>\\(a^+\\) - Actions after delete relaxation</li> <li>Optimal Delete Relaxation is Admissible \\(h^+(n) \\le h^*(n)\\)<ul> <li>(but find an optimal solution is still NP-hard)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#dominance","title":"Dominance","text":"<ul> <li>\\(s' \\supseteq s\\) - \\(s'\\) dominate \\(s\\)<ul> <li>If \\(s\\) is a goal state, then \\(s'\\) must be a goal state.</li> <li>If \\(a^+\\) is applicable in \\(s\\), then \\(a^+\\) must be applicable in \\(s'\\).</li> </ul> </li> <li>\\(Result(s, a^+)\\) dominates both \\(s\\) and \\(Result(s, a)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#optimal-delete-relaxation-heuristic-h","title":"Optimal Delete Relaxation Heuristic (\\(h^+\\))","text":"<pre><code>def h_plus(s, G, A):\n    if G in s:\n        return 0\n\n    open = [s]\n    cost[s] = 0\n    cost[others] = inf\n    best = inf\n\n    while open:\n        cur_state = pop(open)\n        if G in cur_state:\n            best = min(best, cost[cur_state])\n            continue\n\n        for a in A:\n            if pre(a) in current:\n                next_state = current + add(a)\n                if cost[next_state] &gt; cost[cur_state] + 1:\n                    cost[next_state] = cost[current] + 1\n                    push(open, next_state)\n\n    if best == inf:\n        return unsolvable\n    else:\n        return best\n</code></pre> <ul> <li>Native relaxation\u2705</li> <li>Safe\u2705, goal-aware\u2705, admissible guarantee\u2705</li> <li>Efficiently constructible\u2705 Polytime</li> <li>Efficiently computable\u274c<ul> <li>\\(\\mathrm{PlanOpt^+} = \\sum h^+\\) still NP-hard</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<ul> <li>Application based on \\(h^+\\)</li> <li>Distance estimate during the iterations in using shortest-distance algorithm</li> <li>Initially set start point \\(= 0\\); others \\(= \\infty\\).</li> <li>Relax every edge (One iteration = all neighbour unexplored edges from one point)</li> <li>Use a table to note estimate length of shortest path from start to current point</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#example","title":"Example","text":"<ul> <li>A\u2192B(+4); A\u2192C(+5); B\u2192C(-2)</li> </ul> Vertex Initialisation Round 1 Round 2 A 0 0 0 B \u221e 4 4 C \u221e 5 2 (= 4 - 2)"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#h-approximation-methods","title":"\\(h^+\\)-Approximation Methods","text":""},{"location":"Unimelb/25S2/AIP/04-Relaxation/#plan-existence-checking","title":"Plan Existence Checking","text":"<ul> <li> <p>based on Fast Forward Heuristic (\\(h_{FF}\\))</p> <ul> <li>Expand the current state based on BrFS <pre><code>def is_plan_existence(s, G, A):\n    if G in s:\n        return TRUE\n\n    reached = s\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return FALSE\n        reached = new_facts\n\n    return TRUE\n</code></pre></li> </ul> </li> <li> <p>Sound, complete, Terminates in polytime\u2705</p> <ul> <li>\\(\\mathrm{PlanEx}^+\\) now becomes a polytime problem</li> </ul> </li> <li>Safe\u2705, goal-aware\u2705, admissible\u274c (Existence Check Only, usually far from optimal)</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#max-heuristic-hmax","title":"Max Heuristic (\\(h^{max}\\))","text":"<pre><code>def h_max(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n                # a is reachable\n                new_cost = 1 + max(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n    return max(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime<ul> <li>However, sometimes maybe too optimistic\u26a0\ufe0f</li> </ul> </li> <li>Optimistic Estimation: admissible\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#additive-heuristic-hadd","title":"Additive Heuristic (\\(h^{add}\\))","text":"<pre><code>def h_sum(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n\u270f\ufe0f              new_cost = 1 + sum(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n\u270f\ufe0f  return sum(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime</li> <li>Pessimistic Estimation: admissible\u274c informative\u2705 </li> <li>Overcounts by ignoring positive interactions, i.e. shared sub-plans<ul> <li>May result in \\(dramatic\\) over-estimates of \\(h^*\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#relaxed-plans","title":"Relaxed Plans","text":""},{"location":"Unimelb/25S2/AIP/04-Relaxation/#best-supporter-function-bsp","title":"Best Supporter Function \\(bs(p)\\)","text":"<ul> <li>Input: A predication \\(p\\)</li> <li>Output: The \\(cheapest\\) action \\(a\\) which make this predication <code>TRUE</code></li> <li>Prerequisites<ul> <li>\\(p \\in add(a)\\) iff \\(bs(p) = a\\)</li> <li>\\(bs(\\cdot)\\) is closed<ul> <li>\\(bs(p)\\) is defined for every \\(p \\in (P \\backslash s)\\) that has a path to a goal \\(g \\in G\\)</li> </ul> </li> <li>\\(bs(\\cdot)\\) is well-bounded<ul> <li>Support Graph is acyclic </li> </ul> </li> </ul> </li> <li>If a relaxed plan exists, the closed well-founded \\(bs(\\cdot)\\) definitely exists.<ul> <li>There is a relaxed path from \\(I\\) to \\(G\\)</li> <li>Every \\(g\\) has at least one supporter, so as its subgoals</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#helpful-actions","title":"Helpful Actions","text":"<ul> <li>An action is helpful iff:<ul> <li>it is applicable in the current state (\\(pre(a) \\subseteq s\\))</li> <li>it is contained in the final plan</li> </ul> </li> <li>Expanding only helpful actions does not guarantee completeness.</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Relaxation/#relaxed-plan-heuristic-h_ff","title":"Relaxed Plan Heuristic (\\(h_{FF}\\))","text":"<ul> <li> <p>Fast Forward Expansion + Greedy Backward Extraction</p> <ul> <li>For each goal state \\(g \\in G\\):<ul> <li>Find the cheapest \\(a = bs(g)\\)</li> <li>Add this action to the plan</li> <li>repeat on \\(bs(pre(a))\\) <pre><code>def h_FF(I, G, A):\n    if G in I:\n        return 0\n\n    # Forward Expansion - BFS\n    reached = I\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return unsolvable\n        reached = new_facts\n\n    # Backward Extraction - Greedy\n    plan = []\n    subgoals = G\n    while subgoals not in I:\n        new_subgoals = []\n        for g in subgoals:\n            pick a s.t. g in add(a) and pre(a) in reached\n            plan += {a}\n            new_subgoals += pre(a)\n        subgoals = new_subgoals\n\n    return len(plan)\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Same theoretical properties as \\(h^{add}\\) but better in practice</p> <ul> <li>Overcount sub-plans shared by different sub-goals</li> <li>Best Supporter is greedily chosen and sub-optimal</li> <li>In practice, \\(h_{FF}\\) typically does not over-estimate \\(h^*\\), or not by a large amount.</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/","title":"5 Exploration","text":""},{"location":"Unimelb/25S2/AIP/05-Exploration/#novelty","title":"Novelty","text":"<ul> <li>The size of the smallest subset of \\(P \\subseteq s\\), such that \\(s\\) is the first state that makes \\(P\\) <code>TRUE</code> during the search</li> <li>Width: The size of the smallest subset of \\(P\\) needed to be considered to achieve the goal</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#example","title":"EXAMPLE","text":"\\[ p \\to (p, q) \\to (q, r) \\to (p, r) \\to (p, q, r) \\] Current State Smallest New Subset Novelty \\(p\\) \\(p\\) 1 \\(p, q\\) \\(q\\) 1 \\(q, r\\) \\(r\\) 1 \\(p, r\\) \\(p, r\\) 2 \\(p, q, r\\) \\(p, q\\) 2"},{"location":"Unimelb/25S2/AIP/05-Exploration/#width-based-planning","title":"Width-Based Planning","text":""},{"location":"Unimelb/25S2/AIP/05-Exploration/#iterated-width-iw","title":"Iterated Width (IW)","text":"<ul> <li>\\(IW(k) =\\) BFS on \\((Q \\; \\backslash \\; s), \\mathrm{novelty}(s) &gt; k\\)<ul> <li>\\(IW(1) =\\) There is new \\(p\\) appearing in every step in search</li> </ul> </li> <li>\\(IW\\) Algorithm<ul> <li>A sequence of calls \\(IW(k), k=1,2,3,\\ldots\\), until the problem solved or \\(k &gt; len(\\bigcup P)\\) (return <code>unsolvable</code>). </li> <li>The \\(minimum\\) \\(k\\) is the \\(\\text{Width}\\) of the problem</li> </ul> </li> <li>Outcomes<ul> <li>Simple and Blind<ul> <li>Even don't need to know \\(G\\) </li> </ul> </li> <li>However performs pretty well in practice<ul> <li>\\(IW(k \\le 2)\\) can solve 88.3% IPC problems with single goals.</li> <li>Most classical problem (e.g. Blocks, Logistics, Gripper, n-puzzle) have a bounded width independent of problem size and initial situation</li> </ul> </li> <li>Fast \\(\\mathrm{O}(n^k)\\)</li> <li>Optimal if in uniform cost</li> <li>Complete</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#serialised-iterated-width-siw","title":"Serialised Iterated Width (SIW)","text":"<ul> <li> <p>Use \\(IW\\) for decomposing problem and solving sub-problems individually <pre><code>def SIW(s, G):\n    state = s\n    plan = []\n    for g in serialize(G):\n        subplan = IW(state, goal=g)\n        plan += subplan\n        state = Result(subplan, state)\n    return plan\n</code></pre></p> </li> <li> <p>Better performance in Joint Goals problem</p> <ul> <li>(Multi goals but similar approaches)</li> </ul> </li> <li>\u24c1 Goals should be easy to serialise and have low width</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#balancing-exploration-exploitation","title":"Balancing Exploration &amp; Exploitation","text":"<ul> <li>Exploitation: Trusting your heuristic function<ul> <li>State-based Satisfying Planning Often Rely on:<ul> <li>heuristics derived from problem</li> <li>plugged into Greedy Best-First Search (GBFS)  </li> <li>extensions (e.g. helpful actions and landmarks)  </li> </ul> </li> <li>Often gets stuck in local minima<ul> <li>poly + sub-optimal or optimal + NP-hard</li> </ul> </li> </ul> </li> <li>Exploration: Searching for Novelty<ul> <li>Novelty leads to much better performance in practice</li> <li>Can be model-free (No Rely/Assumption)</li> <li>Required for optimal behaviour (in RL and MTCS)</li> </ul> </li> <li>A good agent need to balance the Exploration and Exploitation</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#best-first-width-search-bfws","title":"Best-First Width Search (BFWS)","text":"<ul> <li>Do BFS (Priority Queue) on a sequence of measures:     $$     BFWS(f) \\text{ for } f= \\langle w,f_1,f_2,\\ldots \\rangle     $$<ul> <li>\\(w\\) - Novelty-Measure</li> <li>\\(f_i\\) - tie breaker</li> </ul> </li> <li>Much more efficient than GBFS</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#models-to-simulators","title":"Models to Simulators","text":""},{"location":"Unimelb/25S2/AIP/05-Exploration/#models","title":"Models","text":"<ul> <li>Nowadays, models become more powerful with helps of declarative programming<ul> <li>Expressive language features easily supported</li> <li>Development of external development tools</li> <li>Fully-Black-Box procedures for higher-level abstraction and decomposing problems</li> </ul> </li> <li>However, Declarative Languages also have their downsides:<ul> <li>Model \\(\\ne\\) Language<ul> <li>Many problems fit Classical Planning model, but hard to express in PDDL</li> </ul> </li> </ul> </li> <li>We need for planners that work without complete declarative representations</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#simulators","title":"Simulators","text":"<ul> <li>= Models but no \\(pre(a)\\) and \\(add(a)\\) for presenting \\(a\\)</li> <li>Outcomes<ul> <li>At the same level of efficiency as classic models</li> <li>Open up exciting possibilities for modelling beyond PDDL</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#simulated-bfws-framework","title":"Simulated BFWS Framework","text":"<ul> <li>Framework<ul> <li>Get communication across researchers and to build on each others\u2019 work</li> </ul> </li> <li>Approaches<ul> <li>Get \"states\" from the simulator</li> <li>The optimal combination is \\(BFWS(\\langle w_h(s), h(s) \\rangle)\\) <ul> <li>\\(w_h(s) =\\) \\(s'\\) which has smallest novelty and \\(h(s') = h(s)\\)</li> </ul> </li> </ul> </li> <li>BFWS is the first planners using simulators</li> <li>Challenges of Width-Based Planning over Simulators<ul> <li>Non-linear dynamics  </li> <li>Perturbation in flight controls  </li> <li>Partial observability  </li> <li>Uncertainty about opponent strategy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-Exploration/#arcade-learning-environment","title":"Arcade Learning Environment","text":"<ul> <li>A simple object-oriented framework to develop AI agents for Atari 2600 games<ul> <li>Deterministic</li> <li>Initial state fully known</li> </ul> </li> <li>Performance of \\(IW(1)\\) <ul> <li>better in 34/54 games than 2BFS</li> <li>better in 31/54 games than UCT</li> <li>better in 45/49 games than DeepMind (RL method)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/","title":"6 Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/06-RL/#reinforced-learning","title":"Reinforced Learning","text":"<ul> <li>Policy + Reward + Trial-and-error Interaction</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#planning-vs-learning","title":"Planning vs Learning","text":"Dimension Planning Learning Environment Model Model is known Model is unknown Learning Mode Offline computation Online trial-and-error Agent\u2013Environment Interaction No interaction with the real environment; uses internal simulator Must act in the real environment to gather experience Policy Improvement Through search, deliberation,  planning and introspection Through reward-driven learning Suitable Scenarios A precise model exists and simulation is cheap The model is unknown or hard to specify Advantages Safe, interpretable, no real-world risk Adaptive, risky working in complex/unknown environments Disadvantages Requires accurate model; modeling may be expensive Requires exploration; may be costly or risky Example - Atari Game Agent can query emulator for perfect model (source code) Agent can only sees pixels and scores on the screen \u2192 trial-and-error gameplay is necessary"},{"location":"Unimelb/25S2/AIP/06-RL/#rl-vs-planning-vs-other-ml","title":"RL vs. Planning vs. Other ML","text":"Dimension Reinforcement Learning Automated Planning Other ML Action Outcomes Non-deterministic \u2014 actions lead to probabilistic transitions Deterministic \u2014 outcome fully known from model Usually not modelled as sequential decisions Environment Representation Probabilistic model of states, transitions, and rewards Symbolic or logical model (e.g., STRIPS) No explicit environment Learning Signal Reward Signal - Feedback from environment Predefined goal or planner objective Labels or self-structures Data Structure Sequential / Time series (non-i.i.d.) Discrete steps in a planning domain Often i.i.d. samples (independent &amp; identically distributed) Search &amp; Optimisation Trail-and-error search + Reward-driven state-space search + Predefined policy Gradient-based or statistical fitting Credit Assignment Required \u2014 reward may be delayed over time Not relevant \u2014 goal known a priori Not Required - no delay"},{"location":"Unimelb/25S2/AIP/06-RL/#example-common-applications","title":"Example - Common Applications","text":"<ul> <li>Making a humanoid robot walk</li> <li>Fine tuning LLMs using human/AI feedback</li> <li>Optimising operating system routines</li> <li>Controlling a power station</li> <li>Managing an investment portfolio</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#reinforced-learning-process","title":"Reinforced Learning Process","text":"<pre><code>flowchart LR\n    subgraph Env[Environment]\n        S[(State)]\n        R[(Reward)]\n    end\n\n    subgraph Agent[Agent]\n        P[\"Policy \u03c0(a|s)\"]\n        V[\"(Optional) Value Function\"]\n    end\n\n    S --&gt;|\"(1) Initial State s\"| P\n    P --&gt;|\"(2) Action a\"| Env\n    Env --&gt;|\"(3) Reward r, Next State s'\"| V\n    V --&gt;|\"(4) Update Policy\"| P\n</code></pre>"},{"location":"Unimelb/25S2/AIP/06-RL/#environments","title":"Environments","text":""},{"location":"Unimelb/25S2/AIP/06-RL/#state-s-p","title":"State - \\(S, P\\)","text":"<ul> <li> <p>All RL Algorithms assume that State is Markov:     $$     P(s' \\mid s + H(s), a) = P(s' \\mid s, a)     $$</p> <ul> <li>Once \\(S\\) is known, \\(H\\) can be thrown away</li> <li>Any RL problem can be made Markov by expanding the state</li> <li>Add history into the current state</li> <li>Add belief state (POMDP \u2192 belief-MDP)</li> <li>Add latent state (Model-based RL)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#reward-r","title":"Reward - \\(R\\)","text":"<ul> <li>A scalar feedback signal</li> <li>Indicates how well agent is doing at one step</li> <li>Reward Hypothesis<ul> <li>All\u00a0goals can be described by the \\(maximisation\\) of expected cumulative reward</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#agents","title":"Agents","text":"<ul> <li>Prediction: Evaluate the future rewards of state-actions<ul> <li>\u2192 Value Function</li> </ul> </li> <li>Control: Find the optimal policy<ul> <li>\u2192 Policy Function</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#model-latent-p-r","title":"Model - Latent \\(P, R\\)","text":"<ul> <li>An internal simulator for predicting what the environment will do next<ul> <li>Transition model: \\(P_{ss'}^a\\) - how each action changes the state</li> <li>Reward model: \\(R_{s}^a\\) - immediate reward from each state</li> <li>The model can be imperfect but supports planning and prediction</li> </ul> </li> <li>Simulation is NOT necessary for RL <ul> <li>\u2192 Model-based/Model-free</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#value-function-v-q","title":"Value Function - \\(V, Q\\)","text":"<ul> <li>Define and predict values of states based on the expectation of future rewards</li> <li> <p>State-value Function</p> <ul> <li>Output the value of current state     $$     V_\\pi(s) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t = s\\big]     $$</li> </ul> </li> <li> <p>State-action-value Function</p> <ul> <li>Output the value of the current state with a deterministic action applied     $$     Q_\\pi(s,a) = \\mathbb{E}_\\pi \\big[\\, G_t \\mid S_t = s,\\, A_t = a \\,\\big]     $$</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#return-g_t","title":"Return - \\(G_t\\)","text":"<ul> <li>total discounted reward of the future     $$     G_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma (R_{t+3} + \\ldots)) = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#bellman-expectation-equation","title":"Bellman Expectation Equation","text":"<ul> <li> <p>Bellman Equation: Gives a recursive form of Return         $$ G_t = R_{t+1} + \\gamma G_{t+1} $$</p> <ul> <li>Based on \"look-ahead then back-up\" computational mechanism<ul> <li>Look-ahead: to the next step \u2192 \\(r, s'\\)</li> <li>Back-up: to the current state \u2192 \\(v(s) = f(r,s')\\)</li> </ul> </li> </ul> </li> <li> <p>State-value Function (Deriving BEE)</p> </li> </ul> \\[ \\begin{aligned} v_\\pi(s) &amp;= \\mathbb{E}\\!\\left[r + \\gamma v_\\pi(s')\\right] \\\\[6pt] &amp;= \\sum_a \\pi(a\\mid s)\\Bigl[\\sum_r p(r\\mid s,a)\\,r + \\gamma \\sum_{s'} p(s'\\mid s,a) \\, v_\\pi(s')\\Bigr] \\end{aligned} \\] <ul> <li>State-action-value Function (Deriving BEE)</li> </ul> \\[ \\begin{aligned} q_\\pi(s,a) &amp;= \\mathbb{E}\\!\\left[\\, r + \\gamma\\, \\mathbb{E}_{a'\\sim\\pi(\\cdot\\mid s')} \\bigl[q_\\pi(s',a')\\bigr] \\right] \\\\[6pt] &amp;= \\sum_{s',r} p(s',r\\mid s,a)\\left[ r + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\, q_\\pi(s',a') \\right] \\end{aligned} \\] <ul> <li> <p>Solving the BEE</p> <ul> <li> <p>Directly Compute \\(O(n^3)\\)     $$     v = R + \\gamma Pv \\to v = (I - \\gamma P)^{-1}R     $$</p> <ul> <li>only possible for small \\(P\\) matrix</li> </ul> </li> <li> <p>Dynamic Programming</p> </li> <li>Monte-Carlo Evaluation</li> <li>Temporal-Difference Learning</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#discount-factor-gamma","title":"Discount Factor\u00a0- \\(\\gamma\\)","text":"Gamma (\\(0 \\le \\gamma \\le 1\\)) Behaviour \\(\\gamma = 0\\) Greedy \\(\\gamma \\to 0\\) Myopic \\(\\gamma \\to 1\\) Far-sighted \\(\\gamma = 1\\) Guarantee only if all sequence terminate"},{"location":"Unimelb/25S2/AIP/06-RL/#why-discounting-is-used","title":"Why Discounting is Used","text":"<ul> <li> <p>Technical Reasons</p> <ul> <li>Makes modelling and computation easier (Bellman equations converge cleanly)</li> <li>Prevents infinite returns in cyclic Markov processes</li> <li>Reflects uncertainty about far-future outcomes</li> </ul> </li> <li> <p>Realistic Reasons</p> <ul> <li>In financial settings, immediate rewards can be reinvested (time value of money)</li> <li>Human and animal behaviour shows preference for immediate rewards over delayed rewards</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#policy-pi","title":"Policy - \\(\\pi\\)","text":"<ul> <li>Fully defines agent's behaviour</li> <li>Stationary (Time-independent) - Only relies on the current state</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-RL/#categories-of-rl-algorithms","title":"Categories of RL Algorithms","text":"<ul> <li>Markov Decision Processing<ul> <li>Value Iteration</li> <li>Policy Iteration</li> </ul> </li> </ul> <p>Model-based \u2192 Model-free</p> <ul> <li>Monte Carlo</li> </ul> <p>Non-incremental \u2192 Incremental</p> <ul> <li>Temporal Difference<ul> <li>SARSA</li> <li>Q-Learning</li> </ul> </li> </ul> <p>Tabular representation \u2192 Function representation</p> <ul> <li>Value Function Approximation</li> </ul> <p>Deterministic \u2192 Stochastic Policy</p> <ul> <li>Finite Difference</li> <li>Policy Gradient<ul> <li>REINFORCE</li> <li>Actor-Critic</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/","title":"7 Markov Decision Process","text":""},{"location":"Unimelb/25S2/AIP/07-MDP/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<ul> <li>MDP - General Simulator of the environment for Model-based RL</li> <li>Model-based Learning + Optimal Policy</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#markov-process","title":"Markov process","text":"<ul> <li> <p>A memoryless\u00a0random process      $$     M = \\langle S, P \\rangle     $$</p> <ul> <li>\\(S\\) - a finite set of states</li> <li>\\(P\\) - a state transition probability matrix, which maps every two states in \\(S\\)<ul> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>Each row of \\(P\\) sums to 1</li> <li>\ud83c\udd44 For showing actions validations and non-deterministic</li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#markov-reward-processes","title":"Markov Reward Processes","text":"<ul> <li> <p>A Markov chain with reward values:     $$     M = \\langle S, P, R, \\gamma \\rangle     $$</p> <ul> <li>\\(R\\) - the reward function<ul> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> <li>\\(\\gamma\\) - discount factor</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#markov-decision-process","title":"Markov Decision Process","text":"<ul> <li> <p>Introduce agency \u80fd\u52a8\u6027 in terms of actions:     $$     M = \\langle S, A, P, R, \\gamma \\rangle     $$</p> <ul> <li>\\(A\\) - a finite set of actions</li> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#greedy-optimal-policy","title":"Greedy Optimal Policy","text":"<ul> <li>Give a policy based on the action which has \\(maximum\\) state-action-value</li> <li> <p>Optimal State-value Function     $$     V^\\ast(s) = \\max_\\pi V_\\pi(s)     $$</p> </li> <li> <p>Optimal State-action-value Function     $$     Q^\\ast(s,a) = \\max_\\pi Q_\\pi(s,a)     $$</p> </li> <li> <p>Optimal Policy     $$     a=\\arg\\max_a Q^\\ast(s,a)     $$</p> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#bellman-optimality-equation","title":"Bellman Optimality Equation","text":"<ul> <li> <p>Optimal State-value Function (Deriving BEE)</p> \\[ \\begin{align} v_\\ast(s) &amp;= \\max_a \\; \\mathbb{E}\\!\\left[\\, r + \\gamma v_\\ast(s') \\,\\right] \\\\[6pt] &amp;= \\max_a \\; \\sum_{s'} P(s' \\mid s,a)\\left[\\, R(s,a,s') + \\gamma v_\\ast(s') \\,\\right] \\end{align} \\] </li> <li> <p>Optimal State-action-value Function (Deriving BEE)</p> \\[ \\begin{align} q_\\ast(s,a) &amp;= \\mathbb{E} \\left[\\, r + \\gamma \\max_{a'} q_\\ast(s',a') \\,\\right] \\\\[6pt] &amp;= \\sum_{s',r} p(s',r\\mid s,a)\\left[\\, r + \\gamma \\max_{a'} q_\\ast(s',a') \\,\\right] \\end{align} \\] </li> <li> <p>Solving the BOE</p> <ul> <li>No closed form solution</li> <li>Value Iteration</li> <li>Policy Iteration</li> <li>SARSA</li> <li>Q-learning</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MDP/#example-maze","title":"Example - MAZE","text":"<p>MDP Model of MAZE:</p> <ul> <li>\\(S\\) - Agent's possible locations</li> <li>\\(A\\) - Step directions \\(\\mathtt{N, E, S, W}\\)</li> <li>\\(P\\) - Map \\(s, a \\to s'\\)</li> <li>\\(R\\) - \\(-1\\) per time-step (encourage short-path solution)</li> </ul> <p>Agent of MAZE:</p> <ul> <li>\\(V(s)\\) - the expected return of following the policy from each \\(s\\) <ul> <li>closer to \\(g\\) - \\(V(s)\\) \u2934</li> <li>Farther away - \\(V(S)\\) \u2935</li> </ul> </li> <li>\\(\\pi(s)\\) - Optimal Policy = Best \\(V(s')\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-MC/","title":"8 Monte Carlo","text":""},{"location":"Unimelb/25S2/AIP/08-MC/#model-based-model-free","title":"Model-based \u2192 Model-free","text":"<p>Model-based</p> <ul> <li>Learn from Simulator of Environment<ul> <li>Commonly use MDP to model the Markov Environment</li> </ul> </li> </ul> <p>Model-free</p> <ul> <li>Learn Experience from Environment<ul> <li>Experience: \\(E_t = \\langle S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, ... \\rangle\\)</li> </ul> </li> <li>Optimise \\(V, Q, \\pi\\) directly</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-MC/#monte-carlo-mc-learning","title":"Monte Carlo (MC) Learning","text":"<ul> <li>Run-to-end in the environment for multiple times</li> <li> <p>Use average return to estimate value function, instead of solving \\(v(n)\\) in Model-based RL</p> \\[ V(s) \\approx \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)} \\] <ul> <li>\\(N(s)\\) - how many times \\(G_t\\) is visited</li> <li>When \\(N(s) \\to \\infty\\), \\(V(s) \\to v_\\pi(s)\\).</li> </ul> </li> <li> <p>Unbiased\u2705</p> </li> <li>Can only apply to episodic \u6709\u7ec8\u6b62\u72b6\u6001 Markov environment\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-MC/#mc-basic","title":"MC Basic","text":"<ul> <li>Use MC motivation to transfer Policy Iteration into a Model-free method</li> <li> <p>For each iteration:</p> <ol> <li>Policy Evaluation (Predication)<ul> <li>For each \\((s, a)\\):<ul> <li>run sufficiently many episodes and get average return</li> <li>When \\(N \\to \\infty\\), \\(Q(s, a) \\to q_\\pi(s, a)\\)</li> </ul> </li> </ul> </li> <li>Policy Improvement (Control)<ul> <li>Give the Greedy Optimal Policy based on estimated \\(Q\\)<ul> <li>\\(a^\\ast = \\arg\\max_a q_\\pi(s,a)\\)</li> </ul> </li> </ul> </li> </ol> </li> <li> <p>Convergence guarantee\u2705 Very low Time Efficiency; Not practical\u274c</p> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-MC/#mc-exploring-starts","title":"MC Exploring Starts","text":"<ul> <li>Use Generalised Policy Iteration (GPI) Framework to improve efficiency<ul> <li>In an iteration, instead of do \"Policy Evaluation \u2192 Policy Improvement\" once, do the sub-version of it repeatly</li> </ul> </li> <li>For each episode \\(e\\):<ul> <li>Randomly pick one \\((s, a)\\) and start learning (Exploring Start)</li> <li>For each \\((s,a)\\) visited:<ul> <li>\\(N(s,a) \\gets N(s,a) + 1\\)</li> <li>Update \\(Q(s,a) \\gets \\frac{1}{N(s,a)}(Q(s,a) + (G - Q(s,a))\\)</li> <li>If \\(Q\\) is updated, update \\(a^\\ast\\)</li> </ul> </li> </ul> </li> <li>Data is used much more efficiently</li> <li>Exploring Start can highly guarantee that every node is visited<ul> <li>However difficult in practice (limitation in environment)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-MC/#first-visit-vs-every-visit","title":"First-visit vs. Every-visit","text":"<ul> <li>How to deal with repeat visit of same \\((s,a)\\) in a episode</li> </ul> Aspect First-Visit MC Every-Visit MC When? Only the first time appeared Every time appeared Pros Avoids dependence between multiple visits Faster convergence in practice Cons Some visits ignored Higher variance Suitable Scenario Long episodes with repeated states Short episodes or rare-state situations"},{"location":"Unimelb/25S2/AIP/08-MC/#mc-varepsilon-greedy","title":"MC + \\(\\varepsilon\\)-greedy","text":"<ul> <li> <p>Use soft policy (e.g. \\(\\varepsilon\\)-greedy) to remove needs of exploring start</p> \\[ \\pi(a|s) = \\begin{cases} 1 - (1 - \\frac{1}{|\\mathcal{A}(s)|}) \\, \\varepsilon, &amp; a = a^*_k \\\\[6pt] \\frac{1}{|\\mathcal{A}(s)|} \\, \\varepsilon, &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>\\(|\\mathcal{A}(s)|\\) - number of valid actions of state</li> <li>Balance Exploration &amp; Exploitation</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/","title":"9 Temporal Difference","text":""},{"location":"Unimelb/25S2/AIP/09-TD/#non-incremental-incremental","title":"Non-incremental \u2192 Incremental","text":"<p>Non-incremental (Batch):</p> <ul> <li>Parameters updating per a batch of samples (a episode)</li> <li>Solving the optimal problem at once</li> <li>Offline Learning <ul> <li>Can handle high stochastic/batch environment\u2705</li> <li>Large space complexity\u274c </li> </ul> </li> <li>Closed-form<ul> <li>High computation cost\u274c</li> </ul> </li> </ul> <p>Incremental:</p> <ul> <li>Parameters updating per one sample (a step)</li> <li>Step by step approaching the optimum</li> <li>Online Learning \u2192 One sample use once\u2705</li> <li>Higher generalisation\u2705</li> <li>Lower convergence; may can only approach local optima\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>Key Mechanisms: sample + bootstrap</p> <p>Sampling: an \"1-step experience\" (TD(0))</p> <ul> <li>\\(e = \\{S_t \\to A_t \\to R_{t+1}, S_{t+1}\\}\\)</li> </ul> <p>Bootstrapping: use estimation of future values to update the current value</p> \\[ \\begin{align} \\text{TD-target} &amp; = \\text{sample} + \\gamma \\cdot \\text{bootstrap} \\\\ &amp; = \\text{immediate reward} + \\gamma \\cdot \\text{future value} \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-TD/#prediction-of-state-value","title":"Prediction of State Value","text":"<ul> <li>For each step, if \\(s_t\\) is visited, update its value by looking forward to the reward of next step(s).</li> <li>\u274cCan neither estimate action values nor optimal policies</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#td0","title":"TD(0)","text":"<p>Look into the next \\(1\\) step:</p> <pre><code>$$\n\\begin{align}\nV(s_t) \n&amp;\\gets V(s_t) + \\alpha \\big({{G_t^{(1)} - V(s_t))}} \\\\[2pt]\n&amp;= V(s_t) + \\alpha(R_{t+1} + \\gamma V(s_{t+1}) - V(s_t))\n\\end{align}\n$$\n</code></pre> <p>where</p> <ul> <li>\\(\\vec{v} = R_{t+1} + \\gamma V(s_{t+1})\\) - TD Target</li> <li>\\(\\delta = \\vec{v} - V(s_t)\\) - TD Error<ul> <li>Reflects the difference between \\(v_t\\) and \\(v_\\pi\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#tdn","title":"TD(n)","text":"<p>Look into \\(n-1\\) steps:</p> \\[ G^{(n)}_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + {{\\gamma^n V(S_{t+n})}} \\] \\[ \\begin{align*} \\color{red}{n} &amp; \\color{red}{= 1}\\ \\text{(TD(0))}      &amp; G_t^{(1)}      &amp; = R_{t+1} + \\gamma V(S_{t+1})\\\\[2pt] \\color{red}{n} &amp; \\color{red}{= 2}                   &amp; G_t^{(2)}      &amp; = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})\\\\[2pt] \\color{red}{\\vdots} &amp;                   &amp; \\vdots         &amp; \\\\[2pt] \\color{red}{n} &amp; \\color{red}{= \\infty}\\ \\text{(MC)} &amp; G_t^{(\\infty)} &amp; = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_T \\end{align*} \\] <p>Update in direction of error</p> \\[ V(S_t) \\;\\gets\\; V(S_t) + \\alpha \\big({{G_t^{(n)} - V(S_t)}} \\big) \\] <p>How to choose \\(n\\): - Root Mean Square (RMS) Errors \u5747\u65b9\u5dee\u6839     - change \\(\\alpha\\) \u2192 vary RMS Errors \u2192 different optimal \\(n\\) - Large \\(n\\) \u2192 More rely on exploitation, more precise but higher variance - Small \\(n\\) \u2192 More rely on prediction, faster but higher bias - If \\(n = \\infty\\) , it turns into MC basic</p>"},{"location":"Unimelb/25S2/AIP/09-TD/#tdlambda","title":"TD(\\(\\lambda\\))","text":"<p>Average n-Steps Returns - use weight \\({{(1 - \\lambda)\\lambda^{\\,n-1}}}\\) to balance short-term and long-term rewards     $$     G_t^{\\lambda} \\;=\\; {{(1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{\\,n-1}}} G_t^{(n)}     $$</p> <pre><code>$$\nV(S_t) \\;\\leftarrow\\; V(S_t) + \\alpha \\big( G_t^{\\lambda} - V(S_t) \\big)\n$$\n</code></pre> <ul> <li>\u2705Memoryless - i.e space complexity is same as TD(0)</li> <li>\u26a0\ufe0fForward View - We cannot look into the far future during learning\ud83d\udc47</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#backward-view","title":"Backward View","text":"<p>Update weight step by step by using Eligibility Trace:</p> <ul> <li>Initially set traces of all states \\(E_0(s) \\gets 0\\)</li> <li> <p>In each step:</p> <ul> <li> <p>For each state:</p> <ul> <li>reduce traces value by \\(\\gamma \\lambda\\)</li> <li>\\(E_t(s)+1\\) if \\(s\\) is currently visited</li> </ul> \\[ E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}(S_t = s) \\] </li> <li> <p>Compute TD Error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\] </li> <li> <p>Update Value Function</p> \\[ V(s) \\gets V(s) + \\alpha \\, \\delta_t \\, E_t(s) \\] </li> </ul> </li> </ul> <p>At the end of episodes: Backward View = Forward View</p> \\[ \\sum_{t=1}^T \\alpha \\, \\delta_t \\, E_t(s) = \\sum_{t=1}^T \\alpha (G_t^\\lambda  - V(S_t)) \\; \\mathbf{1}(S_t = S) \\]"},{"location":"Unimelb/25S2/AIP/09-TD/#prediction-of-action-value","title":"Prediction of Action Value","text":"<p>Sarsa - \"State-action-reward-state-action\" Algorithm -  A state-action function version of TD</p> \\[ Q(s_t,a_t) \\gets Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)) \\]"},{"location":"Unimelb/25S2/AIP/09-TD/#sarsa","title":"Sarsa","text":"<ul> <li>For each episode:<ul> <li>For each state:<ul> <li>If \\(s_t \\ne g\\):<ul> <li>Collect the experience \\(\\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\\}\\)<ul> <li>\\(a_t\\) \u2190 \\(\\pi(a \\mid s_t)\\)</li> <li>\\(r_{t+1}, s_{t+1}\\)  \u2190 Environment</li> <li>\\(a_{t+1}\\)  \u2190 \\(\\pi(a \\mid s_{t+1})\\)</li> </ul> </li> <li>Update \\(q(s_t, a_t)\\)</li> <li>Update \\(\\pi\\) based on policy iteration</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#n-step-sarsa","title":"\\(n\\)-step Sarsa","text":"<ul> <li>Sarsa version of TD(n): $$ q_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q(S_{t+n}) $$</li> </ul> \\[ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t)   + \\alpha \\Big( q_t^{(n)} - Q(S_t, A_t) \\Big) \\]"},{"location":"Unimelb/25S2/AIP/09-TD/#sarsalambda","title":"Sarsa(\\(\\lambda\\))","text":"<ul> <li> <p>Sarsa version of TD(\\(\\lambda\\)) $$ q_t^\\lambda \\;=\\; (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} q_t^{(n)} $$</p> </li> <li> <p>Forward View: $$ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t) + \\alpha \\Big( q_t^\\lambda - Q(S_t, A_t) \\Big) $$</p> </li> <li> <p>Backward View: $$ \\begin{align} E_0(s,a) &amp; = 0 \\[0pt] E_t(s,a) &amp; = \\gamma \\lambda E_{t-1}(s,a) + \\mathbf{1}(S_t = s, A_t = a) \\end{align} $$</p> </li> </ul> \\[ \\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\] \\[ Q(s,a) \\;\\leftarrow\\; Q(s,a) + \\alpha \\,\\delta_t \\, E_t(s,a) \\]"},{"location":"Unimelb/25S2/AIP/09-TD/#off-policy-control","title":"Off-Policy Control","text":"<p>Off-policy: Distinguish Target Policy and Behaviour (Sampling) Policy - \u2705Take use of experience from other policies</p>"},{"location":"Unimelb/25S2/AIP/09-TD/#on-policy-off-policy","title":"On-policy \u2192 Off-policy","text":"<ul> <li>Sarsa:<ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_t(s_t)\\) and \\(a_{t+1}\\) from \\(\\pi_t(s_{t+1})\\)</li> <li>Target Policy = Behaviour Policy = \\(\\pi_t\\) </li> </ul> </li> <li>Q-Learning<ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} q_t(s_{t+1}, a)\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_b(s_t)\\)</li> <li>Target Policy = Greedy Optimal \\(q_t\\)</li> <li>Behaviour Policy = Any policies (\\(\\pi_b\\))</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#q-learning","title":"Q-Learning","text":"<ul> <li>For each episode generated by \\(\\pi_b\\):<ul> <li>For each time step:<ul> <li>Update q-value<ul> <li>\\(q_{t+1}(s_t, a_t) \\gets q_t(s_t, a_t) + \\alpha \\bigl(  \\vec v - q_t(s_t, a_t) \\bigr)\\)</li> </ul> </li> <li>Update target policy<ul> <li>\\(a = \\arg\\max_a q_{t+1} (s_t, a)\\)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-TD/#mc-vs-td","title":"MC vs. TD","text":"Aspect Monte-Carlo (MC) Temporal-Difference (TD) Learning timing Offline - Update after the episode ends Online - Update immediately after receiving a reward Environment Assumption episodic  only both episodic and continuing Target used \\(\\vec{v} = G_t\\) \\(\\vec{v} = R_{t+1} + \\gamma V(S_{t+1}\\)) Bootstrapping? \u274c \u2705Value Updating relies on the previous estimate of this value (require initial guesses) Variance \u26a0\ufe0fHigh - Samples of lots of variables \u2705Low Bias Unbiased (return is true target) Biased (bootstraps using V(s\u2032)) Exploits Markov property? \u274c \u2705 Efficiency in Markov environment \u26a0\ufe0fLess efficient \u2705More efficient Efficiency in non-Markov or PMDP env \u2705More efficient with backup learning \u26a0\ufe0fLess efficient Optimisation in limited experience (Batch) Average Return = minimise MSE Fit the most likelihood Markov model that best explains the data"},{"location":"Unimelb/25S2/AIP/09-TD/#example-driving-home","title":"Example - Driving Home","text":"State Elapsed Time (min) Predicted Time to Go Predicted Total Time MC Update (After Back Home) MC New Predicated Time TD(0) Update - \\(\\gamma = 1, \\alpha = 0.5\\) TD New Predicated Time leaving office 0 30 30 43 43-0=43 30+0.5(5+35-30)=35 35-0=35 reach car, raining 5 35 40 43 43-5=38 40+0.5(20+15-40)=37.5 37.5-5=32.5 exit highway 20 15 35 43 43-20=23 35+0.5(30+10-35)=37.5 37.5-20=17.5 behind truck 30 10 40 43 43-30=13 40+0.5(40+3-40)=41.5 41.5-30=11.5 home street 40 3 43 43 43-40=3 43+0.5(43+0-43)=43 43-40=3 arrive home 43 0 43 43 43-43=0 43 43-43=0"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/","title":"10 Value Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#tabular-representation-function-representation","title":"Tabular Representation \u2192 Function Representation","text":"<p>Tabular Presentation:</p> <ul> <li>All values/policies are discrete and stored in a table</li> <li>We update values by change it directly</li> </ul> <p>Function Presentation:</p> <ul> <li>We construct a function for fitting the value distribution</li> <li>We update value by tuning parameters of the function</li> </ul> Aspect Tabular Function Approx. Interpretability \u2705Intuitive \u274cHard to interpret Scalability \u274cDifficult for large/continuous spaces \u2705Much fewer memories required Generalisability \u274cPoor \u2705Generalises from seen situations to unseen situations Bias \u2705Accurate when state space is small \u26a0\ufe0fBiased - State values cannot be represented accurately"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#function-approximation","title":"Function Approximation","text":"<p>Approximate value function by constructing a new function and tuning parameters \\(w\\).</p> <ul> <li>instead of a table (\\(s\\)-\\(a\\)) of discrete values (tabular)</li> </ul> \\[ \\begin{align} \\hat{v}(s, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; v_{\\pi}(s)\\\\[0pt] \\;\\;\\;\\hat{q}(s, a, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; q_{\\pi}(s, a) \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#object-function","title":"Object Function","text":"<p>Goal: \\(minimise\\) MSE between approx function and true value function.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_\\pi \\left[ (v_\\pi(S) - \\hat{v}(S, \\mathbf{w}))^2 \\right] \\] <p>where \\(S \\in \\mathcal{S}\\) - the state is a Random Variable which follows the probability distribution, e.g.:</p> <ul> <li>Uniform Distribution<ul> <li>All with the same probability</li> <li>\\(S = \\frac{1}{|\\mathcal{S}|}\\)</li> <li>\u274cToo simple; not practical</li> </ul> </li> <li>Stationary Distribution<ul> <li>Describe the long-term behaviour of a Markov process<ul> <li>\\(d_\\pi(s) \\approx \\frac{N(s)}{\\sum_{s \\in \\mathcal{S}} N(s)}\\)</li> </ul> </li> <li>Which can also be predicted by using \\(P\\) from a model (MDP)<ul> <li>\\(d_\\pi = d_\\pi P\\)</li> </ul> </li> <li>\u2705Stationary; can be learned</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> \\[ w_{k+1} = w_k - \\alpha \\nabla J(w_k) \\] <p>\\(\\nabla J(w_k)\\) can be computed by:</p> <ul> <li>Ture Gradient Descent<ul> <li>need to calculate the expectation (collect all \\(N\\) samples), not practical\u274c</li> </ul> </li> </ul> \\[ \\begin{align} \\nabla J(w) &amp; = \\nabla \\mathbb{E} \\left[ (v_\\pi(S) - \\hat{v}(S, w))^2 \\right] \\\\[0pt] &amp; \\propto \\mathbb{E}[(v_\\pi(S)-\\hat{v}(S,w))\\,\\nabla\\hat{v}(S,w)] \\end{align} \\] <ul> <li>Stochastic Gradient Descent<ul> <li>Use only 1 sample \\(s_t\\) </li> <li>Online update, better generalisation\u2705</li> </ul> </li> </ul> \\[ \\nabla J(w) \\approx \\bigl(v_\\pi(s_t) - \\hat{v}(s_t,w_t)\\bigr) \\, \\nabla \\hat{v}(s_t,w_t) \\] <p>For remaining two terms:</p> <ul> <li>\\(\\nabla\\hat{v}(s_t,w_t)\\) \u2190 Feature Vectors (Linear)</li> <li>\\(v_\\pi(s_t)\\) \u2190 Value Function Approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#approximate-the-value-function","title":"Approximate the Value Function","text":""},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#feature-vectors","title":"Feature Vectors","text":"<p>Represent the state by a feature of vectors \\(\\mathbf{x}(s)\\), so that in linear case:</p> <ul> <li>State-value Gradient:</li> </ul> \\[ \\nabla\\hat{v}(s,w) = \\frac{\\partial \\hat v(s)}{\\partial w} = \\mathbf{x}(s) \\] <ul> <li>State-action-value Gradient:</li> </ul> \\[ \\nabla \\hat q(s,a,w) = \\mathbf{x}(s,a) \\] <ul> <li>Intuitive; easy to implement; high interpretability\u2705</li> <li>Difficult to select appropriate feature vectors\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#state-value-approximator","title":"State-value Approximator","text":""},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-monte-carlo","title":"Derive from Monte Carlo","text":"<p>Use \\(g_t\\) (discounted return starting from \\(s_t\\)) to approximate:</p> \\[ v_\\pi(s_t) = g_t \\] <ul> <li>converges to a local optimum, even for\u00a0non-linear approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-temporal-difference","title":"Derive from Temporal Difference","text":"<p>Use \\(r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) - \\hat{v}(s_t, w_t)\\) (TD Error) to update approximator at each step:</p> \\[ v_\\pi(s_t) = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) \\] <ul> <li>Linear TD(0) converges close to the global optimum</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-td","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ v_\\pi(s_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t)-\\hat{v}(s_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{v}(s_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#action-state-value-approximator","title":"Action-state-value Approximator","text":"<p>Generally change \\(v\\) to \\(q\\) :</p> \\[ w_{k+1} = w_k  - \\alpha\\, \\bigl(q_\\pi(s_t,a_t)-\\hat{q}(s_t,a_t,w_t)\\bigr)\\, \\nabla \\hat{q}(s_t,a_t,w_t) \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-monte-carlo_1","title":"Derive from Monte Carlo","text":"\\[ q_\\pi(s_t,a_t) = g_t \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-sarsa","title":"Derive from Sarsa","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\hat{q} (s_{t+1}, a_{t+1}, w_t) \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-td_1","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ q_\\pi(s_t, a_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat q(s_{t+1}, a_{t+1}, w_t)-\\hat{q}(s_t, a_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{q}(s_t,a_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#derive-from-q-learning","title":"Derive from Q-learning","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}(s_{t+1})}\\hat{q} (s_{t+1}, a, w_t) \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#batch","title":"Batch","text":"<p>Restricted Environment: Only limited and static samples can algorithms learn from</p> \\[ \\mathcal{D} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_n, v^\\pi_n \\rangle\\} \\] <p>Distribution Shift: These samples may generate by a sub-optimal policy, so that: - Data may be out of distribution - Q-value may be over-estimated - Overfitting</p> <p>Potential Suitable Methods: - Function Approximation \u2192 generalisability - Off-policy (e.g. Q-Learning) \u2192 static sample set - Supervised Learning Methods (e.g. Neural Network) \u2192 static sample set</p>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#example-mc-td-in-a-batch","title":"Example - MC &amp; TD In a Batch","text":"<p>Sampled Episodes: 8 Samples (Limited)</p> \\[ \\begin{align} A \\to 0, B \\to 0 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 0 \\end{align} \\] <p>MC Update:</p> <ul> <li>\\(v(B) = 6/8 = 0.75\\)</li> <li>\\(v(A) = 0 / 1 = 0\\)</li> </ul> <p>TD Update:</p> <ul> <li>\\(v(B) \\to 0.75\\)</li> <li>\\(v(A) = 0 + v(B) \\to 0.75\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#batch-methods","title":"Batch Methods","text":""},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#least-squares-solution","title":"Least Squares Solution","text":"<p>By \\(minimising\\) SSE between approximate and true values:</p> \\[ LS(w_i) = \\mathbb{E} \\left[ (v^\\pi(S) - \\hat{v}(S, w))^2 \\right] \\] <ul> <li>True values can be approximated by Return (MC) or Temporal Difference (TD)</li> <li>Using all samples at once</li> <li>\\(O(n^3)\\) or \\(O(n^2)\\) by using \"Sherman-Morrison\"</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>By randomly sample a mini-batch of samples from \\(\\mathcal{D}\\):</p> \\[ \\mathcal{B} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_k, v^\\pi_k \\rangle\\} \\] <p>Then use gradient descent for updating:</p> \\[ w_{k+1} = w_k - \\alpha\\, \\frac{1}{m} \\sum_{\\langle s_i, v^\\pi_i \\rangle \\in \\mathcal{B}} \\bigl( v^\\pi_i(s_i) - \\hat{v}(s_i, w_i) \\bigr) \\nabla \\hat{v}(s_i, w_i) \\]"},{"location":"Unimelb/25S2/AIP/10-Value-Approx/#deep-q-learning","title":"Deep Q-Learning","text":"<p>By using Q-Learning + Neural Network (\\(minimise\\) MSE \\(\\mathbf{Loss}(\\cdot)\\)): </p> \\[ \\mathbf{Loss}(w_i) =\\underbrace{\\mathbb{E}_{(s,a,r,s')\\sim \\mathcal{D}_i}}_{\\text{sampled from}\\;D_i} \\Big[ \\big( \\underbrace{r+\\gamma \\max_{a'} Q(s',a';w_i^-)}_{\\text{TD target }} - \\underbrace{Q(s,a;w_i)}_{\\text{Q estimate}} \\big)^2 \\Big] \\] <ul> <li>Experience replay: <ul> <li>Randomly sample from non-i.i.d data \u2192 single sample/mini-batch</li> <li>\u2705For decorrelation (near-i.i.d)</li> </ul> </li> <li>Fixed Q-targets: <ul> <li>\\(w_i^-\\) will only be updated by \\(w_i\\) (lively updated) after running a while (like 1000 steps)</li> <li>\u2705Reduce variance</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/","title":"11 Policy Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#value-based-policy-based","title":"Value-based \u2192 Policy-based","text":"<p>In Value-based Algorithms:</p> <ul> <li>A policy is optimal if it can \\(maximise\\) every state value</li> </ul> \\[ \\pi^\\ast = \\arg\\max_\\pi v_\\pi(S) \\] <ul> <li>We store all \\(v_\\pi(S)\\) in a tabular based on different \\(\\pi\\) so that we can choose best policy from them.</li> </ul> <p>In Policy-based Algorithms:</p> <ul> <li>We construct a model of \\(\\pi\\) and optimise by tuning its parameters \\(\\theta\\).</li> </ul> \\[ \\pi^\\ast = \\pi(a\\mid S,\\theta) \\] <ul> <li>Higher Level approximation\u2705<ul> <li>Higher space efficiency\u2705</li> <li>Higher generalisability\u2705</li> <li>Can learn stochastic policies\u2705</li> </ul> </li> <li>May converges to a local optimum\u26a0\ufe0f</li> <li>Policy evaluation is inefficient and high-variance\u274c<ul> <li>Sampling looks for the whole trace - inefficient\u274c</li> <li>Off-policy unavailable (\\(\\theta\\) changed \u2192 data distribution changed \u2192 need new sampling)\u274c</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#policy-gradient","title":"Policy Gradient","text":"<ol> <li>Use a object function to define optimal policies: \\(J(\\theta\\))</li> <li>Use gradient-based optimisation to search for optimal policies</li> </ol> \\[ \\theta_{t+1} = \\theta + \\alpha \\nabla J(\\theta_t) \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#objective-function","title":"Objective Function","text":"<p>Goal: \\(maximise\\) the expected long-term return under the policy:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\, G(\\tau) \\,\\right], \\] <p>where</p> <ul> <li>\\(\\tau\\) represents the trajectory generated by following such policy</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> <p>$$ \\theta_{k+1} = \\theta_k - \\alpha \\nabla J(\\theta_k) $$ \\(\\nabla J(\\theta_k)\\) can be computed by:</p> <ul> <li>Finite Difference<ul> <li>Run one episode on \\(\\theta_k\\) and get \\(J(\\theta_k)\\)</li> <li>Slightly change \\(\\theta_k\\) to \\(\\theta_k + \\epsilon \\vec{e}_k\\)<ul> <li>where \\(\\epsilon \\vec{e}_k\\) is a small noise (in vector form)</li> </ul> </li> <li>Run one episode again on \\(\\theta_k + \\epsilon \\vec{e}_k\\) and get \\(J(\\theta_k + \\epsilon \\vec{e}_k)\\)</li> <li>Estimate gradient by using finite difference:</li> </ul> </li> </ul> \\[ \\nabla J(\\theta) \\approx  \\frac{J(\\theta + \\epsilon e_i) - J(\\theta)}{\\epsilon} \\] <ul> <li>True Gradient Descent (The Gradient-ascent Algorithm)</li> </ul> \\[ \\begin{aligned} \\nabla J(\\theta) &amp;= \\sum_{\\tau} \\nabla P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(definition of expected return)} \\\\[4pt] &amp;= \\sum_{\\tau} P(\\tau \\mid \\theta) \\, \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(apply log-derivative trick)} \\\\[4pt] &amp;= \\mathbb{E}_{\\tau \\sim P(\\tau \\mid \\theta)} \\big[ \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) \\big] &amp;&amp; \\text{(convert sum to expectation)} \\\\[4pt] &amp;= \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla \\log \\pi_\\theta(A \\mid S) \\, G(\\tau) \\big] &amp;&amp; \\text{(expand trajectory likelihood)} \\end{aligned} \\] <ul> <li>Stochastic Gradient Descent</li> </ul> \\[ \\nabla J(\\theta) \\approx \\nabla \\log \\pi_\\theta (a_t \\mid s_t) \\, G(\\tau) \\] <p>where</p> <ul> <li>\\(\\nabla \\log \\pi_\\theta(a \\mid s)\\) is also called Score Function</li> </ul> <p>[!NOTE] To compute Score Function, we can no longer use deterministic/non-linear/non-differentiable \\(\\{0,1\\}\\) to describe policy. </p> <p>So we introduce some soft strategies to make policy probabilistic.</p>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#deterministic-policy-stochasticsoft-policy","title":"Deterministic Policy\u00a0\u2192 Stochastic/Soft Policy","text":""},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#deterministicnear-deterministic-policy","title":"Deterministic/Near-deterministic Policy","text":"<p>Give the optimal action directly:</p> \\[ \\pi(a \\mid s) = \\mathbf{1}\\{\\, a = a^*(s) \\,\\} \\] <p>Or use soft policy for small explorations:</p>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#-greedy","title":"\u03b5-greedy","text":"<p>most time exploitation (\\(P = 1 - \\varepsilon\\)), little time exploration (\\(\\varepsilon\\)):</p> \\[ \\pi(a \\mid s) = \\begin{cases} 1 - \\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>All previous RL methods' outputs are deterministic (The Optimum).</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#upper-confidence-bound-ucb","title":"Upper Confidence Bound (UCB)","text":"<p>exploitation + exploration bonus:</p> \\[ a = \\arg\\max_a \\left( Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right) \\] <p>where</p> <ul> <li>\\(c\\) - importance of exploration (parameter)</li> <li>\\(\\ln{t}\\) - explore more when time goes</li> <li>\\(N(a)\\) - not try too many times on one same action</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#stochastic-policy","title":"Stochastic Policy","text":"<p>Give the probability of each action: $$ \\pi(a \\mid s) = P(a \\mid s) $$ - Ability of exploration; Robustness\u2705</p> <p>#### softmax </p> \\[ \\pi(a \\mid s) = \\frac{\\exp\\!\\left(h_\\theta(s,a)\\right)}        {\\sum_{a' \\in \\mathcal{A}} \\exp\\!\\left(h_\\theta(s,a')\\right)} \\] <p>where \\(h(\\cdot)\\) is another feature function</p>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#gaussian","title":"Gaussian","text":"<p>Action space needs to be continuous:</p> \\[ \\pi(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\theta^2(s)}}   \\exp\\!\\left(     -\\frac{\\left(a-\\mu_\\theta(s)\\right)^2}{2\\sigma_\\theta^2(s)}   \\right). \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#solve-the-score-function","title":"Solve the Score Function","text":"<p>By applying Softmax:</p> \\[ \\begin{align} \\nabla \\log \\pi_\\theta(a \\mid s) &amp;= \\nabla h(a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\nabla h(a')] \\end{align} \\] <p>By applying Gaussian (assume \\(\\sigma\\) is fixed):</p> \\[ \\nabla \\log \\pi_\\theta(a \\mid s) = \\frac{a - \\mu_\\theta(s)}{\\sigma^2}\\nabla \\mu_\\theta(s) \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#approximate-the-expected-return","title":"Approximate the Expected Return","text":""},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#average-state-value","title":"Average State Value","text":"<p>By \\(maximise\\) weighted average of all state values:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)v^{\\pi_\\theta}(s) \\] <p>where</p> <ul> <li>\\(d^{\\pi_\\theta}(s)\\) is the weight (importance) of each state</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#start-state-value","title":"Start State Value","text":"<ul> <li>In episodic environments, we may only care about value of start states:</li> </ul> \\[ J(\\theta)  = v^{\\pi_\\theta}(s_0) \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#average-one-step-reward","title":"Average One-step Reward","text":"<p>By \\(maximise\\) weighted average of all immediate rewards:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)  \\sum_{a \\in A} \\pi_\\theta(a \\mid s) \\, R^a_s \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Average State Value and Average One-step Reward are Equivalent:</p> \\[ \\bar{R}_\\pi = (1 - \\gamma) \\, \\bar{v}_\\pi \\] <p>In Gradient Form:</p> \\[ \\nabla \\bar{R}_\\pi \\simeq { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s,a) \\bigr]} \\] \\[ \\nabla \\bar{v}_\\pi = \\frac{1}{1 - \\gamma} \\nabla \\bar{R}_\\pi \\] \\[ \\nabla v_\\pi(s_0) = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s_0) \\; q_{\\pi_{\\theta}}(s_0,a) \\bigr]} \\] <p>In summary, gradient version of above methods can be simplified to one formula:</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\] <p>Then, we can use our familiar MC or value approximators to solve it.</p>"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#reinforce","title":"REINFORCE","text":"<p>Derive from MC, we can use return \\(v_t\\) as an unbiased sample of \\(q_{\\pi_{\\theta}}(s,a)\\):</p> \\[ q_{\\pi_{\\theta}}(s,a) = v_t \\]"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#actor-critic","title":"Actor-Critic","text":""},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#qac","title":"QAC","text":"<p>Derive from TD, we can use a\u00a0value approximator to estimate action-value function:</p> \\[ q_{\\pi_{\\theta}}(s,a) = q(s,a, w) \\] Actor Critic Role Policy Policy Evaluation (Value Function) Input Score State, Reward, Action Task Update \\(\\theta\\) by Policy Gradient Update \\(w\\) by Value Learning Output Action chosen by the policy Score (Value / Advantage)"},{"location":"Unimelb/25S2/AIP/11-Policy-Approx/#a2c-advantage-actor-critic","title":"A2C - Advantage actor-critic","text":"<p>Introduce a baseline for reducing variance without changing the expectation<sup>1</sup>:</p> \\[ \\begin{align} \\nabla_{\\theta} J(\\theta)  &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\\\[4pt] &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\;  \\left[ q_{\\pi_{\\theta}}(s, a) - B(s) \\right] \\bigr]} \\end{align} \\] <ul> <li>A good (sub-optimal) baseline is the state value function \\(v_{\\pi_\\theta}(s)\\)</li> </ul> <p>By applying it, we can get the Advantage Function:</p> \\[ A_{\\pi_{\\theta}}(s,a) =q_{\\pi_{\\theta}}(s, a) - v_{\\pi_\\theta}(s) \\] <p>Then,</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; A_{\\pi_{\\theta}}(s,a) \\bigr]} \\] <p>Derive from TD, we can use TD error to unbiased estimate such function</p> \\[ \\delta_{\\pi_{\\theta}} = r + \\gamma v_{\\pi_{\\theta}}(s') - v_{\\pi_{\\theta}}(s) \\] \\[ \\begin{align} \\mathbb{E}_{\\pi_{\\theta}}[\\delta_{\\pi_{\\theta}} \\mid s,a] &amp;= \\mathbb{E}_{\\pi_{\\theta}}[r + \\gamma v_{\\pi_{\\theta}}(s') \\mid s,a] - v_{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= q^{\\pi_{\\theta}}(s,a) - v^{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= A^{\\pi_{\\theta}}(s,a) \\end{align} \\] <p>Finally, we in fact use the TD error to compute the policy gradient:</p> \\[ \\nabla_{\\theta} J(\\theta) = {\\mathbb{E}_{\\pi_{\\theta}} \\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\\; \\delta_{\\pi_{\\theta}} \\big]} \\] <ol> <li> <p>P51 -\u3010\u3010\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b66\u539f\u7406\u3011\u8bfe\u7a0b\uff1a\u4ece\u96f6\u5f00\u59cb\u5230\u900f\u5f7b\u7406\u89e3\uff08\u5b8c\u7ed3\uff09-\u54d4\u54e9\u54d4\u54e9\u3011 https://b23.tv/pVXJKm8\u00a0\u21a9</p> </li> </ol>"},{"location":"Unimelb/25S2/AIP/12-Integrating/","title":"12 Integrating Learning and Planning","text":""},{"location":"Unimelb/25S2/AIP/12-Integrating/#model-free-integrated-architectures","title":"Model-free \u2192 Integrated Architectures","text":"<p>Model-free</p> <ul> <li>No model</li> <li>Learn Value/Policy from experience</li> <li>Cannot look ahead\u274c</li> <li>Sample may be expensive\u26a0\ufe0f</li> <li>May fall into sub-optimal</li> </ul> <p>Integrated Architectures</p> <ul> <li>Learn Model from experience</li> <li>Learn and plan Value/Policy from real and simulated experience</li> <li>Sample is efficient\u2705 (directly from simulators)</li> <li>Avoid model error\u2705 (Beyond Model-based)</li> <li>Supervised Learning methods can be used in model training\u2705</li> <li>Uncertainty can be learned in model training\u2705</li> <li>Modelling is hard\u274c<ul> <li>Need environment to be model-learnable\u26a0\ufe0f</li> <li>Compounding model bias\u274c (experience is from model)</li> </ul> </li> <li>High computation cost\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/12-Integrating/#simulation-based-search","title":"Simulation-based Search","text":"<p>Model-based RL + Model-free RL backup</p> <ol> <li>Collect the current state \\(s_t\\) from real world</li> <li>For each \\(a \\in A\\), simulate \\(K\\) episodes from \\(s_t\\):<ul> <li>Use a model of MDP to look ahead</li> <li>Use Forward Search methods (e.g. BFS, DFS)\u00a0to select action</li> <li>Terminate after running \\(T-t\\) steps</li> </ul> </li> </ol> \\[ \\{ s_t^k, A_t^k, R_{t+1}^k, \\ldots, S_T^k \\}_{k=1}^K \\;\\sim\\; \\mathcal{M}_\\nu \\] <ol> <li>Take each episode as a experience for Model-free update</li> <li>Select the action in real world based on optimal policy from Model-free RL</li> </ol>"},{"location":"Unimelb/25S2/AIP/12-Integrating/#simple-monte-carlo-search","title":"Simple Monte-Carlo Search","text":"<p>Derive from MC, update \\(q\\) value by mean return of all episodes, select the optimal action with \\(maximum\\) \\(q\\).</p>"},{"location":"Unimelb/25S2/AIP/12-Integrating/#monte-carlo-tree-search","title":"Monte-Carlo Tree Search","text":"<ul> <li>Build a\u00a0search tree\u00a0containing visited states and actions</li> <li>Update \\(q\\) by mean return of episodes starting from each \\((s,a)\\)</li> </ul> <p>$$ q(s,a) = \\frac{1}{N(s,a)} \\sum_{k=1}^K \\sum_{u=t}^T \\mathbf{1}(S_u, A_u = s,a) \\, G_u $$ Off-policy - Behaviour Policy: Forward Search methods - Optimal Policy: \\(\\arg\\max q(s,a)\\)</p>"}]}