{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Drava's Study Notes","text":""},{"location":"index.html#master-of-information-technology-in-the-university-of-melbourne","title":"\ud83c\udf93 Master of Information Technology in the University of Melbourne","text":""},{"location":"index.html#2025-semester-2","title":"\ud83d\uddd3\ufe0f 2025 Semester 2","text":"<ul> <li>\ud83d\udcd8 AI Planning for Autonomy - COMP90054</li> <li>\ud83d\uddd3\ufe0f Introduce to the Machine Learning - COMP90049</li> <li>\ud83d\uddd3\ufe0f Model of Computation - COMP30026</li> </ul>"},{"location":"index.html#other","title":"\ud83d\udcbb Other","text":"<ul> <li>\ud83d\udcad CNN for Visual Recognition - Stanford CS231n</li> <li>\ud83d\udcad Natural Language Processing - Stanford CS224n</li> </ul>"},{"location":"index.html#reference","title":"\ud83d\udcda Reference","text":""},{"location":"Other/DLCV/01-CNN.html","title":"1 Convolutional Neural Networks","text":""},{"location":"Other/DLCV/01-CNN.html#convolution","title":"Convolution","text":"<p>Convolution = Local weighted sum</p> \\[ O[x, y] = \\sum_{i,j} I[x+i,\\; y+j]\\;K[i,j] \\] <p>Where</p> <ul> <li>\\(I\\) - Select a local patch of the image</li> <li>\\(K\\) - Multiply each value by the local weights defined in the kernel (filter/pattern)</li> <li>\\((x,y)\\) - Sum them to obtain one output value at position</li> </ul> <p>What does a filter define?</p> <p>1. Local Area</p> <p>Range/Size of the filter = definition of locality.</p> <ul> <li>3\u00d73 kernel \u2192 observes a 3\u00d73 neighbourhood  </li> <li>5\u00d75 kernel \u2192 observes a 5\u00d75 neighbourhood  </li> </ul> <p>2. Local Weights</p> <p>The kernel values (\\(K[i,j]\\)) themselves define the weights for combining the local pixels.</p> <ul> <li>Edge filter \u2192 positive + negative values  </li> <li>Blur filter \u2192 uniform weights  </li> </ul> <p>To make the computation more machine-friendly, we can:</p> <ul> <li>Build a filter</li> <li>Slide - Scan (apply such filter to) the input with a local rule</li> <li>Form the results into a new map</li> </ul> Sobel Horizontal Filter <p>Input</p> \\[ I = \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 5 &amp; 4 &amp; 2 \\\\ 3 &amp; 5 &amp; 8 &amp; 5 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 3 &amp; 2 &amp; 1 \\end{bmatrix} \\] <p>Kernel</p> <p>Detects horizontal edges:</p> \\[ K = \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -2 &amp; -1 \\end{bmatrix} \\] <ul> <li>Top row = positive weights  </li> <li>Bottom row = negative weights  </li> <li>Measures intensity change in vertical direction \u2192 highlights horizontal boundaries</li> </ul> <p>Convolution</p> <p>Compute the first output pixel \\(O[0,0]\\)</p> <ul> <li>Patch covered by kernel:</li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 8 \\end{bmatrix} \\] <ul> <li>Multiply element-wise:</li> </ul> \\[ \\begin{aligned} O[0,0] &amp;=  (1\\cdot1 + 2\\cdot2 + 3\\cdot1) \\\\ &amp;\\quad + (2\\cdot0 + 4\\cdot0 + 5\\cdot0) \\\\ &amp;\\quad + (3\\cdot(-1) + 5\\cdot(-2) + 8\\cdot(-1)) \\end{aligned} \\] <ul> <li>Compute:</li> </ul> \\[ O[0,0] = (1 + 4 + 3) + 0 + (-3 -10 -8) = -13 \\] <p>After sliding the kernel across all positions:</p> \\[ O = \\begin{bmatrix} -13 &amp; -16 &amp; -13 \\\\ 0 &amp; 0 &amp; 0 \\\\ 13 &amp; 16 &amp; 13 \\end{bmatrix} \\] <p>Outcomes tell us:</p> <ul> <li>Where the pattern occurs  </li> <li>How strongly it appears  </li> <li>How it varies across space  </li> </ul>"},{"location":"Other/DLCV/01-CNN.html#perceptron-linear-classifier","title":"Perceptron \u2192 Linear Classifier","text":"<p>Linear Classifier is a modern use of Perceptron for mapping image features to multiple categories.</p>"},{"location":"Other/DLCV/01-CNN.html#classifier-score-function","title":"Classifier (Score Function)","text":"<p>Linear Boundary:</p> \\[ s_{x_i} = f(x_i, W) = Wx_i \\]"},{"location":"Other/DLCV/01-CNN.html#loss-function","title":"Loss Function","text":"<p>Cross-Entropy + softmax</p> \\[ L_i = - \\log \\frac{\\exp(s_{y_i})}{\\sum_j \\exp(s_j)} \\] <ul> <li>Regularisation Loss (L2):</li> </ul> \\[ R(W) = \\lambda ||W||^2 \\] <ul> <li>Full Loss:</li> </ul> \\[ L = \\frac{1}{N} \\sum_{i=1}^{N} L_i + R(W) \\]"},{"location":"Other/DLCV/01-CNN.html#optimisation","title":"Optimisation","text":"<p>RMSProp: Dynamically adjust learning rates of different gradients.</p> <p>Maintain weighted average square of gradients:</p> \\[ E[g^2] \\gets \\beta E[g^2] + (1 - \\beta)g^2 \\] <p>Update parameters:</p> \\[ W \\gets W - \\frac{\\alpha}{\\sqrt{E[g^2]}+\\epsilon}g \\]"},{"location":"Other/DLCV/01-CNN.html#linear-classifier-fully-connected-networks","title":"Linear Classifier \u2192 Fully-Connected Networks","text":""},{"location":"Other/DLCV/01-CNN.html#activate-function","title":"Activate Function","text":"<p>Goal - Introduce non-linearities to the model.</p> \\[ \\underbrace{\\text{Linear Operators} \\to \\textcolor{red}{\\text{Activate Function}}}_{\\text{Feedforward/Linear Layer, Convolutional Layer, etc.}} \\to \\ldots \\] <p>Sigmoid</p> \\[ \\sigma(x) = \\frac{1}{1+e^{-x}} $$ $$ L_i = \\sum_{j \\ne y_i}\\big[1-\\sigma(s_{y_i}-s_j)\\big] \\] <p>Outcome of Sigmoid</p> <ul> <li>Squash numbers to range \\([0, 1]\\)</li> <li>Nice interpretability as a saturating \"firing rate\" of a biological neurone (historically popular) \u2705</li> <li>Large positive/negative values can easily \"kill\" the gradients \u274c</li> </ul> <p>ReLU (Rectified Linear Unit)</p> \\[ L_i = \\sum_{j \\ne y_i} \\max (0, s_j - s_{y_i} + 1) \\] <p>Outcome of ReLU</p> <ul> <li>Does not saturate \u2705</li> <li>Very computationally efficient \u2705</li> <li>Converges much faster than sigmoid in practice \u2705</li> <li>\\(\\mu \\ne 0\\) \u26a0\ufe0f</li> <li>Dead when all \\(x &lt; 0\\) \u274c</li> </ul> <p>GELU (Gaussian Error Linear Unit)</p> \\[ \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x \\,e^{-t^2/2}\\,dt \\] \\[ \\text{GELU}(x) = x\\,\\Phi(x) \\] \\[ L_i = \\sum_{j \\ne y_i} \\text{GELU}(1 - (s_{y_i}-s_j)) \\] <p>Outcome of GELU</p> <ul> <li>Very nice behaviour around 0 \u2705</li> <li>Smoothness facilitates training in practice \u2705</li> <li>Higher computational cost than ReLU \u2639\ufe0f</li> <li>partial saturate \u2192 Large negative values can still easily \"kill\" gradients \u274c</li> </ul> <p>Other Activation Function</p> \\[ \\text{Leaky ReLU}(x)=\\max(0.1x,\\,x) \\] \\[ \\text{ELU}(x) = \\begin{cases} x, &amp; x \\ge 0 \\\\ \\alpha(e^x-1), &amp; x \\lt 0 \\end{cases} \\] \\[ \\text{SiLU}(x) = x \\cdot \\sigma(x) \\]"},{"location":"Other/DLCV/01-CNN.html#backpropagation","title":"Backpropagation","text":"<pre><code>flowchart LR\n    o[\"\\*\"]\n\n    x --&gt; o\n    y --&gt; o\n    o --&gt; z\n    z --&gt; ...\n</code></pre> <p>Every Node knows its Local Gradients:</p> \\[ z = x \\cdot y, \\; \\textcolor{green}{\\frac{\\partial z}{\\partial x}} = y \\] <p>It gets Upstream Gradient from the next layer:</p> \\[ \\textcolor{red}{\\frac{\\partial L}{\\partial z}} \\] <p>Then, it can get Downstream Gradients by simply multiplying its local gradients (Chain Rule) and send to the previous layer:</p> \\[ \\frac{\\partial L}{\\partial x} = \\textcolor{green}{\\frac{\\partial z}{\\partial x}} \\cdot \\textcolor{red}{\\frac{\\partial L}{\\partial z}} \\] <pre><code>flowchart LR\n    ... --&gt;|&amp;part;L#47;&amp;part;z| z\n    z --&gt;|&amp;part;L#47;&amp;part;x| x\n    z --&gt;|&amp;part;L#47;&amp;part;y| y</code></pre>"},{"location":"Other/DLCV/01-CNN.html#weight-initialisation","title":"Weight Initialisation","text":"<p>Goal - Balance the output distribution, which includes:</p> <ul> <li>Activations in forward propagation</li> <li>Gradients in backward propagation</li> <li>Convergence in training</li> </ul> <p>Problem - If initial weights are:</p> <ul> <li>too small \u2192 Vanishing Gradients</li> <li>too large \u2192 Explode Gradients</li> </ul> <p>Solution - Kaiming/MSRA Initialisation</p> \\[ \\text{Input Variance} = \\text{Output Variance} \\] <p>For ReLU:</p> \\[ \\text{std} = \\sqrt\\frac{2}{D_\\text{in}} \\] <p>Set into Gaussian Distribution:</p> \\[ W \\sim \\mathcal{N}\\,(0, \\frac{2}{D_\\text{in}}) \\] <p>which means \\(\\bar{w} = 0\\), \\(s^2 = \\frac{2}{D_\\text{in}}\\).</p>"},{"location":"Other/DLCV/01-CNN.html#fcns-convolutional-networks-cnns","title":"FCNs \u2192 Convolutional Networks (CNNs)","text":"<p>Key Limitation of FCNs on Computer Vision</p> <p>The spatial structure of images is destroyed.</p> <ul> <li>The neurone of FCNs can only process data in 1D vector structure.</li> <li>An image must be flattened before being passed into the network.</li> </ul> <p>Outcome:</p> <ul> <li>Pixel Adjacency is lost</li> <li>Geometry and topology in images disappear</li> </ul> <p>This problem is almost impossible to be solved in FCNs framework.</p> <ul> <li>Re-learn spatial structure is inefficient and requires huge datasets</li> <li>May need too much parameters, causing overfitting and poor generalisation</li> </ul> <p>Solution of CNNs: Convolution + Downsample</p> <ul> <li>Convolution layers - Extract features with a limited view/filter for respecting 2D image structure.</li> <li>Pooling layers (Downsample) - Expand receptive field (view) and dropdown resolution for faster computation.</li> <li>Fully-Connected layers - Form an MLP at the end to predict scores and output categories.</li> </ul> <p>A CNN is a neural network with Convolution layers.</p>"},{"location":"Other/DLCV/01-CNN.html#convolution-layer","title":"Convolution Layer","text":"<p>Assume a colour (RGB) image with 32x32 pixels</p> <p>FCNs: Use stretch to transfer an image into a 1D vector.</p> <pre><code>flowchart LR\n    A[\"3x32x32 image\"] --&gt;|\"stretch\"| B[\"1x3072 vector\"]\n    B --&gt;|\"10x3072\" weights| C[\"score\"]</code></pre> <p>Convolution:</p> <ul> <li>Perform filters with a small view (e.g., only care about a local range of pixels, only care about contrast, only care about margin, ...).</li> <li>Convolve (slide) over all spatial locations on the image.</li> <li>Perform activation function (ReLU).</li> <li>Store results in an activation map.</li> </ul> <p>One Filter:</p> <pre><code>flowchart LR\n    A[\"3x32x32 image\"]\n    subgraph c[\"Convolution Layer\"]\n        B[\"1 number\"]\n        C[\"1 number\"]\n        D[\"...\"]\n        E[\"1 number\"]\n    end\n    F[\"1x28x28 activation map\"]\n\n    z[\"bias vector\"] --&gt; c\n    A --&gt;|\"3x5x5 filter at (0,0)\"| B\n    A --&gt;|\"slide filter to (1,0)\"| C\n    A --&gt;|\"slide filter to ...\"| D\n    A --&gt;|\"slide filter to (27,27)\"| E\n    B --&gt;|\"ReLU\"| F\n    C --&gt;|\"ReLU\"| F\n    D --&gt;|\"ReLU\"| F\n    E --&gt;|\"ReLU\"| F</code></pre> <p>Multiple Filters:</p> <pre><code>flowchart LR\n    A[\"3x32x32 image\"]\n    subgraph c[\"Convolution Layer\"]\n        B[\"Filter 1\"]\n        C[\"Filter 2\"]\n        D[\"...\"]\n        E[\"Filter 6\"]\n    end\n    F[\"6 28x28 activation maps\"]\n\n    A --&gt;|\"(0,0) -&gt; (27,27)\"| B\n    A --&gt;|\"(0,0) -&gt; (27,27)\"| C\n    A --&gt;|\"(0,0) -&gt; (27,27)\"| D\n    A --&gt;|\"(0,0) -&gt; (27,27)\"| E\n    B --&gt;|\"ReLU\"| F\n    C --&gt;|\"ReLU\"| F\n    D --&gt;|\"ReLU\"| F\n    E --&gt;|\"ReLU\"| F\n    x[\"6D bias vector\"] --&gt; c</code></pre> <p>One Convolution Layer:</p> <ul> <li>Input: \\(C_\\text{in} \\times H \\times W\\)</li> <li>Filters: \\(C_\\text{out} \\times C_\\text{in} \\times K_h \\times K_w\\) </li> <li>Output: \\(C_\\text{out} \\times (H - K_h + 1) \\times (W - K_w + 1)\\)</li> </ul> <p>Multiple Convolution Layers:</p> <pre><code>flowchart LR\n    A[\"3x32x32 image\"]\n    B[\"6 28x28 activation maps\"]\n    C[\"10 24x24 activation maps\"]\n\n    A --&gt;|6 5x5x3 filters + ReLU| B\n    B --&gt;|10 5x5x6 filters + ReLU| C\n    C --&gt;|CONV + ReLU| ...</code></pre>"},{"location":"Other/DLCV/01-CNN.html#padding","title":"Padding","text":"<p>Problem - Feature maps shrink with each layer.</p> \\[ W' = W - K + 1 \\lt W \\] <p>Solution - Add padding around the input before sliding the filter.</p> <ul> <li>Raw Input: \\(W \\times W\\)</li> <li>Padding: \\(P\\)</li> <li>Padded Input: \\((W+2P) \\times (W+2P)\\)</li> <li>Filter: \\(K \\times K\\)</li> <li>Output: \\((W - K + 1 + 2P) \\times (W - K + 1 + 2P)\\)</li> </ul> <p>Same Padding: In practice, we use padding to keep the output size the same as the input size.</p> \\[ P = \\frac{K - 1}{2} \\]"},{"location":"Other/DLCV/01-CNN.html#downsampling","title":"Downsampling","text":""},{"location":"Other/DLCV/01-CNN.html#pooling","title":"Pooling","text":"<p>Problem - Translation Variance</p> <p>The visual features should not depend on absolute coordinates</p> \\[ \\text{Conv}(\\text{Translate}(X)) =  \\text{Translate}(\\text{Conv}(X)) \\] <p>Translation Variance: A slight spatial shift in the input image may produce significant variations in the resulting feature map, leading to instability in the classifier\u2019s predictions.</p> <p>Solution - Downsample by applying a pool filter with a fixed sampling rule.</p> <p>Max Pool</p> \\[ O[i,j]=\\max_{u,v \\in R} I[u,v] \\] <p>Average Pool</p> \\[ O[i,j]=\\frac{1}{|R|}\\sum_{u,v \\in R}I[u,v] \\] <p>where</p> <ul> <li>\\(R\\) - View of the pool filter</li> </ul>"},{"location":"Other/DLCV/01-CNN.html#stride","title":"Stride","text":"<p>Problem - Convolution operator cost a lot.</p> \\[ \\text{ConvOps}_{S=1} = (W - K + 2P +1)^2 \\] <p>Solution - Downsample by increasing the Stride.</p> <ul> <li>Not continuously slide the filter (\\(S = 1\\)) anymore, but step with a distance (\\(S \\ge 2\\))).</li> </ul> \\[ \\text{ConvOps}_{S} = (\\frac{W - K + 2P}{S} +1)^2 \\] <p>Modern CV designs (ResNet, MobileNet) use strided convolution instead of pooling.</p> Why strided convolution is replacing pooling? <ul> <li>Learnable downsampling \u2014 Stride reduces spatial size while learning how to combine features, unlike fixed max/avg rules.</li> <li>Better information preservation \u2014 Pooling discards details abruptly; stride can preserve meaningful structure.</li> <li>End-to-end optimisation \u2014 Stride integrates into convolution layers, making the whole pipeline differentiable and more stable.</li> <li>Cleaner architecture \u2014 Fewer separate layers</li> </ul> <p>Summary</p> <p>Input: \\(C_{in} \\times H\\times W\\)</p> <p>Hyperparameters: - Kernel size: \\(K_H\\times K_W\\) - Number filters: \\(C_{out}\\) - Padding: \\(P\\) - Stride: \\(S\\)</p> <p>Weight matrix: \\(C_{out} \\times C_{in} \\times K_H\\times K_W\\)</p> <p>Bias vector:  \\(C_{out}\\)</p> <p>Output size: \\(C_{out} \\times H'\\times W'\\) </p> <p>where:</p> <ul> <li>\\(H'=\\frac{H-K+2P}{S}+1\\)</li> <li>\\(W'=\\frac{W-K+2P}{S}+1\\)</li> </ul> Common Settings <ul> <li>\\(K_H=K_W\\) (Small square filters)</li> <li>\\(P=\\frac{K-1}{2}\\) (Same padding)</li> <li>\\(C_{in}, C_{out} =32,64,128,256\\) (powers of 2)</li> <li>\\(K=3,P=1,S=1\\) (3\u00d73 Filter)</li> <li>\\(K=5,P=2,S=1\\) (5\u00d75 Filter)</li> <li>\\(K=1,P=0,S=1\\) (1\u00d71 Filter)</li> <li>\\(K=3,P=1,S=2\\) (Downsample by 2)</li> </ul>"},{"location":"Other/DLCV/01-CNN.html#normalisation-layer","title":"Normalisation Layer","text":""},{"location":"Other/DLCV/01-CNN.html#normalise","title":"Normalise","text":"<p>Motivation - Stabilise the Internal Covariate Shift by applying activate functions between layers, which causes</p> <ul> <li>unstable gradients</li> <li>slower convergence</li> <li>more difficulty in optimising deep models</li> </ul> <p>For each input vector:</p> \\[ \\hat{x}_i = \\frac{x_i-\\mu}{\\sigma+\\varepsilon} \\] <ul> <li>Outcome: \\(\\bar{x} = 0\\), \\(s^2_x = 1\\).</li> </ul> <p>What is the statistical dimension of \\(i\\)</p> <ul> <li>Batch Norm: \\(H, W, N\\) - normalise between samples</li> <li>Layer Norm: \\(H, W, C\\) - normalise between channels</li> <li>Instance Norm: \\(H, W\\) - normalise per sample per channel</li> <li>Group Norm:  \\(H, W, c\\) - Divide channels into groups</li> </ul> <p>Channel = Depth/Features per pixel</p>"},{"location":"Other/DLCV/01-CNN.html#affine-transform","title":"Affine Transform","text":"<p>Problem -  Directly use normalising will restricts the representational capacity of the network, which makes the model CANNOT:</p> <ul> <li>change the scale of the feature</li> <li>shift the activation distribution</li> <li>preserve important magnitude information</li> </ul> <p>Solution - Introduce learnable parameters specifically for optimising distribution of original data.</p> \\[ y_i = \\gamma \\hat{x}_i + \\beta \\] <ul> <li>Outcome: \\(\\bar{x} = \\beta^\\ast\\), \\(s^2 = \\gamma^\\ast\\).</li> <li>If the original distribution is optimal, then:</li> </ul> \\[ \\gamma = \\sigma,\\; \\beta = \\mu \\]"},{"location":"Other/DLCV/01-CNN.html#dropout","title":"Dropout","text":"<p>Problem - Overfitting from co-adaption of features:</p> <ul> <li>A group of Neurones work at same time</li> <li>Limited generalisation even scaling up the network</li> <li>Zero Training Loss but high validation loss</li> </ul> <p>Solution - For each step, randomly (by a hyperparameter) dropout (block) some neurones (set to zero probability).</p> <p>Drop at training time (Forward Propagation):</p> \\[ \\text{Conv} \\to \\text{GELU} \\to \\textcolor{red}{\\text{Dropout}} \\to \\text{Next Layer} \\] <ul> <li>Sample a binary mask \\(m_i \\in \\{0, 1\\}\\) with \\(p\\) (often 0.5)</li> </ul> \\[ m_i \\sim \\text{Bernoulli}(p) \\] <ul> <li>Original Dropout (rarely used)</li> </ul> \\[ \\tilde{x}_i = m_i \\cdot x_i \\] <ul> <li>Inverted Dropout</li> </ul> \\[ \\tilde{x}_i = \\frac{m_i}{p} x_i \\] <p>Scale at test time:</p> \\[ \\text{Output at test time} = \\text{Expected output at training time} \\] <ul> <li>Original Dropout - Output of neurone \\(\\ast \\,p\\)</li> <li>Inverted Dropout - No additional steps required</li> </ul> <p>Not necessary in CNN architectures (even not commonly used).</p>"},{"location":"Other/DLCV/01-CNN.html#residual-learning","title":"Residual Learning","text":"<p>Problem - Vanishing/Explode Gradients Problem in Deep Learning</p> <p>Solution - Let each layer learn only the change (residual) from its input, not the full mapping.</p> <p>Forward Propagation:</p> <p>Add \\(x\\) to the original output of each layer.</p> \\[ H(x) = F(x) + x \\] <pre><code>graph TD\n\n    x([x]) --&gt; fblock[\"F(x) block\"]\n    x --&gt;|shortcut| add((\"(+)\"))\n    fblock --&gt; add\n\n    add --&gt; output([output])</code></pre> <p>Backward Propagation:</p> <p>Apply \\(1\\)  for ensuring that the gradient can propagate to earlier layers without degradation.</p> \\[ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H}\\left(1+\\frac{\\partial F}{\\partial x}\\right) \\] <p>Outcome of Residual Learning</p> <ul> <li>The network is at least not worse than simple \\(H(x) = x\\). \u2705<ul> <li>If \\(F(x)\\) cannot learn something new, it at least keep the same output quality as doing nothing.</li> <li>Keep the variance/distribution at least good as the previous layer.</li> </ul> </li> <li>Backward propagation is still easy \u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html","title":"1 Search","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#search-space","title":"Search Space","text":"<p>Search Space - The set of all possible states and transitions that a search or planning algorithm must consider</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#example","title":"EXAMPLE","text":"<p>In a robot delivery scene:</p> <pre><code>world_state_of_a_robot = {\n\u00a0 \u00a0 \"position\": (3, 5),\n\u00a0 \u00a0 \"direction\": \"North\",\n\u00a0 \u00a0 \"battery\": 80,\n\u00a0 \u00a0 \"carrying_package\": True\n}\n\nsubgoals = {\"position\": (5, 8), \"carrying_package\": True}\n</code></pre> Common Functions <p>Search states:</p> <ul> <li>\\(\\mathrm{is}\\_\\mathrm{start}(s)\\) return if the state is the start state of the search space</li> <li>\\(\\mathrm{is}\\_\\mathrm{target}(s)\\) mark if the state is the goal state of the search space</li> <li>\\(\\mathrm{succ}(s)\\) return a list of successors/next states of \\(s\\)</li> </ul> <p>Search nodes:\u00a0</p> <ul> <li>\\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(\\mathrm{parent}(\\sigma)\\) where \\(\\sigma\\) was reached</li> <li>\\(\\mathrm{action}(\\sigma)\\) leads from \\(\\mathrm{state}(\\mathrm{parent}(\\sigma))\\) to \\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(g(\\sigma)\\) denotes cost of path from the root to \\(\\sigma\\)</li> <li>The root\u2019s \\(\\mathrm{parent}(\\cdot)\\) and \\(\\mathrm{action}(\\cdot)\\) are undefined</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#search-methods","title":"Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#forward-search-vs-backward-search","title":"Forward Search vs. Backward Search","text":"<ul> <li>Forward Search (Progression)<ul> <li>search space = world space (represents the current real world)</li> <li>BFS, DFS, A*, MDP</li> </ul> </li> <li>Backward Search (Regression)<ul> <li>search space = a sets of world spaces (represents all predications of sub-goals)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#blind-search-vs-informed-search","title":"Blind Search vs. Informed Search","text":"<ul> <li>Blind search not require any input beyond the problem</li> <li>No additional work but rarely effective</li> <li>Informed search requires function \\(h(x)\\) mapping states to estimates their goal distance</li> <li>Effective but lots of work to construct \\(h(x)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#blind-systematic-search","title":"Blind Systematic Search","text":"<ul> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>Iterative Deepening Search<ul> <li>Do DLS(DFS with depth limited) with continuously increasing depth limited by 1.</li> </ul> </li> </ul> Properties <ul> <li>Completeness 100%\u2705 </li> <li>Really poor efficient when scale up\u2639\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#informed-systematic-search","title":"Informed Systematic Search","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#heuristic-functions","title":"Heuristic Functions","text":"<ul> <li>\\(h(n)\\) - Estimated remaining cost (from the current state to the goal state)</li> <li>\\(h^*(n)\\) - Actual remaining cost</li> </ul> <p>proficiency of \\(h(n)\\):</p> <ul> <li>\\(h = h^{\\ast}\\) perfectly informed, \\(h(n) = h^{\\ast}(n) - \\textbf{optimal } A^{\\ast}\\)</li> <li>\\(h = 0\\) no information at all - uniform cost search</li> </ul> Properties Description Safe \\(h(n) = \\infty\\) iff \\(h^{\\ast}(n) = \\infty\\) Goal-Aware \\(h(\\text{goal}) = 0\\) Admissible \\(h(n) \\le h^*(n)\\) Consistent \\(h(n) \\le c(n,n\u2019) + h(n\u2019)\\) for all possible \\(c(n, n\u2019)\\) <p>Relations</p> <ul> <li>Consistent &amp; Goal-aware \u2192 Admissible</li> <li>Admissible \u2192 Safe &amp; Goal-aware</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#greedy-best-first-search-gbfs","title":"Greedy Best-First Search (GBFS)","text":"<ul> <li>Use priority queue to sort \\(h(n)\\) of each node in ascending order<ul> <li>If \\(h(n) = 0\\), it becomes what fully depends\u00a0on how\u00a0we\u00a0break\u00a0ties</li> </ul> </li> </ul> <pre><code>def greedy_BFS:\n\u00a0 \u00a0 frontier = priority queue ordered by h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <p>Completeness\u2705for safe heuristics; Optimal\u274c</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#a","title":"A*","text":"<ul> <li>Only difference from GBFS:<ul> <li>\\(h(n) \\rightarrow f(n) = g(n) + h(n)\\)</li> </ul> </li> </ul> <p><pre><code>function A*(start, goal):\n    open = priority queue\n    open.push(start, f(start))\n\n    while open is not empty:\n        n = open.pop_min_f()\n\n        if n == goal:\n            return reconstruct_path(n)\n\n        for each successor s of n:\n            compute g(s) and f(s)\n            open.push_or_update(s)\n</code></pre> <pre><code>#### Re-opening\n\nA node $n$ could be re-opened if we find a cheaper $g(n)$.\n</code></pre> def a_star_with_re_opening:     frontier = priority queue ordered by f(n) = g(n) + h(n)     explored = set     path = list \u00a0 \u00a0 frontier.add(h(start), start, path)</p> <p>\u270f\ufe0f  g = dict \u270f\ufe0f  g[start] = 0</p> <pre><code>while frontier:\n    _, current, path = frontier.pop()\n    if current == goal:\n        return path\n    explored.add(current)\n    for successor in succ(current):\n        new_path = path + action(current, successor)\n        new_g = cost(new_path)\n        if successor not in g or new_g &lt; g[successor]:\n            [successor] = new_g\n            f = g[successor] + h(successor)\n            frontier.add(f, successor, f)\n</code></pre> <p>\u270f\ufe0f              if successor in explored:     # Re-opening \u270f\ufe0f                  explored.remove(successor)     return unsolvable</p> <p><pre><code>Not needed if consistent\u2705  $g$ is already cheapest\n\n---\n### Weighted A*\n$$\nf_W(n) = g(n) + W \\cdot h(n)\n$$\n\n| Weight         |       Algorithm        |\n|:-------------- |:----------------------:|\n| $W \\to 0$      |   Digkstra Algorithm   |\n| $W \\to 1$      |           A*           |\n| $W &gt; 1$        | Bounded sub-optimal A* |\n| $W \\to \\infty$ |          GBFS          |\n\n&gt; If $h$ is admissible, $f_W(n) \\le W \\cdot h(n)$\n\n---\n### Hill-Climbing\n\nBrute Force to find greedy best direction for heuristic descent\n</code></pre> def hill_climbing: \u00a0 \u00a0 path = list \u00a0 \u00a0 current = start \u00a0 \u00a0 while h(n) &gt; 0: \u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current)) \u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best) \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break \u00a0 \u00a0 \u00a0 \u00a0 return path <pre><code>&gt; - Can only find local maxima\n&gt; - Available only if $h(n) &gt; 0$ for all non-goal states\n\n---\n### Enforced Hill-Climbing\n\nDo small range BFS when find local optimal\n</code></pre> def enforced_hill_climbing: \u00a0 \u00a0 path = list \u00a0 \u00a0 explored = set \u00a0 \u00a0 current = start \u00a0 \u00a0 while h(n) &gt; 0: \u00a0 \u00a0 \u00a0 \u00a0 explored.add(current) \u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current)) \u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best) \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subs = n for n in neighbours_of(current) and not in explored \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 best_sub = argmin_h(subs, goal=current) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if best_sub and h(best_sub) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path -= action(parent, current) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(parent, best_sub) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = best_sub \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable \u00a0 \u00a0 return path <pre><code>&gt; Can across small gap between local best and global best\n\n---\n### Iterative Deepening A*\n\nIDS + $A^*$: Use f(n) instead of depth to limit IDS\n- In First Search:\u00a0 f(n) = f(start) = 0 + h(start)\n- Following Searches: f(n) = min_out_of_bound_excess\n</code></pre> def ida_star: \u00a0 \u00a0 bound = f(start) \u00a0 \u00a0 while True: \u00a0 \u00a0 \u00a0 \u00a0 t = ids(start, bound) \u00a0 \u00a0 \u00a0 \u00a0 if t == goal: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return solution \u00a0 \u00a0 \u00a0 \u00a0 if t == infinity: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable \u00a0 \u00a0 \u00a0 \u00a0 bound = t ```</p> <p>Solve one of \\(A^*\\)\u2019s problem: large queue/closed_set</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#evaluation-of-search-methods","title":"Evaluation of Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#guarantees","title":"Guarantees","text":"<ul> <li>Completeness sure to find a solution if there is one</li> <li>Optimality solutions sure be optimal</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#complexity","title":"Complexity","text":"<ul> <li>Time/Space (Measured in generated states/states cost)</li> <li>Typical state space features governing complexity<ul> <li>Branching factor \\(b\\) how many successors</li> <li>Goal depth \\(d\\) number of actions to reach shallowest goal state</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#summary","title":"Summary","text":"DFS BrFS IDS A* HC IDA* Complete \u274c \u2705 \u2705 \u2705 \u274c \u2705 Optimal \u274c \u2705* \u2705 \u2705* \u274c \u2705* Time \\(\\infty\\) \\(b^d\\) \\(b^d\\) \\(b^d\\) \\(\\infty\\) \\(b^d\\) Space \\(b \\cdot d\\) \\(b^d\\) \\(b \\cdot d\\) \\(b^d\\) \\(b\\) \\(b \\cdot d\\) <p>where</p> <ul> <li>\\(b\\) - Branching Factor: sum of number of child nodes of each node</li> <li>\\(d\\) - Solution Depth: \\(minimum\\) depth among all goal nodes</li> <li>DFS cannot handle cyclic graph</li> <li>BFS is optimal only when uniform costs are applied </li> <li>\\(A^*\\) / Iterative Deepening \\(A^*\\) is optimal only when \\(h\\) is admissible (\\(h \\le h^*\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html","title":"2 Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#ai-solver","title":"AI Solver","text":"3-level Solver Ambitions <p>Ambition: Write one program to solve all classical search problems</p> <ul> <li>Ambition 1.0 (Problem Solving) Write one program to solve a problem</li> <li>Ambition 2.0 (Problem Generation) Write one program to solve a large class of problems</li> <li>Ambition 3.0 (Meta Problem Solving) Write one program to solve a large class of problems effectively</li> </ul> <p>Programming-Based:</p> <ul> <li>Use static programs designed by the programmer</li> <li>e.g. Pac-man</li> </ul> <p>Learning-Based:</p> <ul> <li>Unsupervised/Reinforced Learning: Use reward &amp; penalise</li> <li>Supervised: Use labelled data</li> <li>Evolutionary: Use original controllers to mutate and recombine to build better controller</li> <li>e.g. OpenAI-Five</li> </ul> <p>Model-Based (Planning):</p> <ul> <li>Construct a model for one specific class of problems</li> <li>Different models yield different types of controllers</li> </ul> <p></p> <p>Pros &amp; Cons</p> <p>Programming-Based</p> <ul> <li>Domain-knowledge easy to express\u2705</li> <li>Less Flexible\u2639\ufe0f </li> <li>Can\u2019t deal with no anticipation of programmer\u274c</li> </ul> <p>Learning-Based</p> <ul> <li>Dose not require much knowledge in principle\u2705</li> <li>Slower; Hard to know which features to learn\u2639\ufe0f</li> </ul> <p>Model-Based</p> <ul> <li>Powerful; Quick; Flexible; Clear; Intelligent; Domain-independent\u2705</li> <li>Need a model (sometimes very hard); Efficiency loss\u2639\ufe0f</li> </ul> <p>Tip</p> <p>To design a good solver, always balance between \u201cAutomatic and General\u201d vs. \u201cManual but Effective\u201d</p>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#models-for-planning","title":"Models for Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#classical-planning-model","title":"Classical Planning Model","text":"<p>The most basic model for describing the environment, the goal output is a sequence of actions map \\(I\\) to \\(G\\).</p> \\[ M = \\langle S, A, T, I, G \\rangle \\] <p>where</p> <ul> <li>\\(S\\) - State spaces </li> <li>\\(A(s)\\) - Actions applicable for \\(s \\in S\\)</li> <li>\\(T\\) - Deterministic Transition Function: \\(s\u2019 = T(A(s), s)\\) shows one successor \\(s\u2019\\) of \\(s\\)</li> <li>\\(I\\) - Initial state</li> <li>\\(G\\) - Goal states</li> <li>Uniform action costs \\(c(A(s), s) = 1\\)</li> </ul> <p>Properties of Classical Planning Model</p> <ul> <li>Simple; Intuitive\u2705</li> <li>Very strict assumptions\u2757\ufe0f\u2757\ufe0f<ul> <li>Deterministic</li> <li>Fully Observable</li> <li>Static World</li> <li>Discrete Time &amp; Finite Actions</li> <li>Uniform Cost</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#conformant-planning-model","title":"Conformant Planning Model","text":"<p>Derived from Brute Force, implement the ability to handle stochasticity\uff1a</p> <ul> <li>Initial State \u2192 A set of possible initial states</li> <li>Deterministic Transition Function \u2192 Non-deterministic</li> </ul> <p>Outcomes: Conformant</p> <ul> <li>Goal Guarantee \u2705</li> <li>No Observation - No new info; must pre-planning\u26a0\ufe0f</li> <li>Rarely optimal \u26a0\ufe0f<ul> <li>Must map any possible \\(s_0\\) to \\(g\\), too much unnecessary work</li> <li>Sensitive to Worst/Special case </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#languages-for-planning","title":"Languages for Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#strips","title":"STRIPS","text":"<p>STRIPS = \"STanford Research Institute Problem Solver\", which designed a language for:</p> <ul> <li>Specification - concise model description</li> <li>Computation - reveal useful information/structure for heuristics</li> </ul> <p>All problems in following format can be solved by STRIPS:</p> \\[ P = \\langle P,A,I,G \\rangle \\] <p>where</p> <ul> <li>\\(P\\) - All Possible Predicates</li> <li>\\(A\\) - Actions</li> <li>\\(I\\) - Initial States: Predicates initially to be <code>TRUE</code></li> <li>\\(G\\) - Goal Conditions: Predicates Finally need to be <code>TRUE</code></li> </ul> <p>Actions</p> \\[ a \\in A, a = \\langle Pre(a), Add(a), Del(a) \\rangle \\] <p>where</p> <ul> <li>\\(Pre(a)\\) - Preconditions need to be <code>TRUE</code> before executing the action</li> <li>\\(Add(a)\\) - Add Effects: Preconditions set to be<code>TRUE</code> after execution</li> <li>\\(Del(a)\\) - Delete Effects: Preconditions set to be <code>FALSE</code> after execution</li> </ul> <p>Result Function (Application/Transition Function)</p> \\[ Result(s, a) = (s - Del(a)) \\cup Add(a) \\] <p>Goal of STRIPS</p> <p>Find a list of actions \\([a_1,a_2,a_3,\\ldots]\\) which satisfies:</p> \\[ Result(Result(\\ldots Result(I,a_1\u200b) \\ldots ,a_n\u22121\u200b),a_n\u200b) \\models G \\] <p>Outcomes: STRIPS</p> <p>It turns \"Solving a problem\" into \"Search for solutions\".</p> <p>Still PSPACE-complete, but problem is now more concise and intuitive:     - Explicit Search         - e.g. Blind/Heuristic search         - Optimal Solution Guarantee\u2705; Not effective\u274c</p> <p>And \"approximation\" is now able to be applied as well:     - Near Decomposition         - e.g. Relaxation         - Effective\u2705; Maybe fail\u274c</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html","title":"3 Relaxation","text":""},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#complexity","title":"Complexity","text":"<ul> <li>P: Problem can be solved using polytime</li> <li>NP: Problem can be solved using non-deterministic polytime</li> <li>PSPACE: Problem can be solved using polynomial space</li> <li>EXP: Problem can be solved using exponential time</li> </ul> \\[ P \\subseteq NP \\subseteq PSPACE \\subseteq EXP \\] <p>Savitch\u2019s Theorem </p> <p>NPSPACE = PSPACE - Non-determinism won't make problems to be harder or solvers to be more powerful.</p> <p>A decision problem \\(L\\) is PSPACE-complete iff:</p> <ol> <li>\\(L \\in\\) PSPACE<ul> <li>\\(L\\) can be solved using polynomial space</li> </ul> </li> <li>\\(L\\) is PSPACE-hard<ul> <li>every problem in PSPACE can be polynomial-time reduced to \\(L\\)</li> </ul> </li> </ol>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#goals-definition","title":"Goals Definition","text":"<p>\\(\\mathrm{PlanEx}(P)\\)</p> <ul> <li>for making a Satisficing planning</li> <li>\\(\\mathrm{PlanEx}(P) =\\) <code>TRUE</code> - Existence of a plan for problem \\(P\\).</li> </ul> <p>\\(\\mathrm{PlanLen}(P)\\) </p> <ul> <li>for making a Optimal planning</li> <li>\\(mathrm{PlanLen}(P) \\le B\\) - Existence of a plan which length is at most \\(B\\).</li> </ul> <p>Note</p> <p>Both \\(\\mathrm{PlanEx}\\) and \\(\\mathrm{PlanLen}\\) are PSPACE-complete. - In practice, optimal planning is almost never easy</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxation","title":"Relaxation","text":"<p>A transformation for simplifying the heuristic value computation.</p> \\[ \\mathcal{R} = (\\mathcal{P'}, r, h'^*) \\] <p>where</p> <ul> <li>\\(\\mathcal{P}'\\) - the class of the simpler problem </li> <li>\\(r\\) - transformer turning the original problem into a simplified one</li> <li>\\(h'^*(n)\\) - Perfect heuristic function of the simplified problem </li> </ul> <p>Properties of a Relaxation</p> <p>The relaxation is:</p> <ul> <li>Native - if \\(\\mathcal{P'} \\subseteq \\mathcal{P}\\) and  \\(h'^*(n) = h(n)\\)</li> <li>Efficiently Constructible - if a polynomial \\(r\\) exists</li> <li>Efficiently Computable - if a polynomial \\(h\u2019(n)\\) exists</li> </ul> Applications of Relaxation <ol> <li> <p>Route-Finding</p> <ul> <li>Relaxation<ul> <li>Route-find as a bird (ignoring the road)</li> </ul> </li> <li>Outcome<ul> <li>Road Distance \u2192 Manhattan distance, Euclidean distance, etc.</li> </ul> </li> <li>Native\u274c Efficiently constructible\u2705 Efficiently computable\u2705</li> </ul> </li> <li> <p>Goal-Counting</p> <ul> <li>Relaxation<ul> <li>Assume we can achieve each goal directly</li> </ul> </li> <li>Outcome<ul> <li>Tasks \u2190 No precondition and delete</li> </ul> </li> <li>Admissible but still NP-hard</li> <li>Native\u2705 Efficiently constructible\u2705 Efficiently computable\u274c</li> </ul> </li> </ol> <p>Still inefficiency?</p> <ul> <li>Approximate \\(h\u2019^*\\)</li> <li>Re-design \\(h\u2019^*\\) in a way so that it will typically be feasible<ul> <li>Critical path heuristics</li> <li>Delete relaxation \u2190 wide-spread for satisficing planning</li> <li>Abstractions</li> <li>Landmarks</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#delete-relaxation","title":"Delete Relaxation","text":"<p>Apply \\(Del(a) = \\emptyset, \\forall a \\in A\\).</p> <ul> <li>Facts once be <code>TRUE</code> will remain <code>TRUE</code> \"forever\"</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#dominance","title":"Dominance","text":"<p>\\(s' \\supseteq s\\) - \\(s'\\) dominate \\(s\\)</p> <p>Properties of a Dominance</p> <ul> <li>If \\(s\\) is a goal state, then \\(s'\\) must be a goal state.</li> <li>If \\(a^+\\) is applicable in \\(s\\), then \\(a^+\\) must be applicable in \\(s'\\).</li> <li>\\(Result(s, a^+)\\) dominates both \\(s\\) and \\(Result(s, a)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#optimal-delete-relaxation-heuristic-h","title":"Optimal Delete Relaxation Heuristic (\\(h^+\\))","text":"<p>Apply delete relaxation on heuristic computation.</p> <pre><code>def h_plus(s, G, A):\n    if G in s:\n        return 0\n\n    open = [s]\n    cost[s] = 0\n    cost[others] = inf\n    best = inf\n\n    while open:\n        cur_state = pop(open)\n        if G in cur_state:\n            best = min(best, cost[cur_state])\n            continue\n\n        for a in A:\n            if pre(a) in current:\n                next_state = current + add(a)\n                if cost[next_state] &gt; cost[cur_state] + 1:\n                    cost[next_state] = cost[current] + 1\n                    push(open, next_state)\n\n    if best == inf:\n        return unsolvable\n    else:\n        return best\n</code></pre> <p>Outcomes: \\(h^+\\)</p> <ul> <li>Native relaxation\u2705</li> <li>Safe\u2705, goal-aware\u2705, admissible guarantee\u2705</li> <li>Efficiently constructible\u2705 Polytime</li> <li>Efficiently computable\u274c<ul> <li>\\(\\mathrm{PlanOpt^+} = \\sum h^+\\) still NP-hard</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<p>A extension planning algorithm based on \\(h^+\\).</p> <ol> <li>Initially set \\(d[s_0] = 0\\); other \\(d[s] = \\infty\\).</li> <li>Relax every edge:<ul> <li>For each edge \\(\\langle u, r, v \\rangle\\):<ul> <li>If \\(d[u] + w &lt; d[v]\\), update \\(d[v] \\gets d[u] + w\\) </li> </ul> </li> <li>Repeat \\(|V| - 1\\) times to ensure all shortest paths are propagated</li> </ul> </li> <li>Check all edges again, if any relaxable edges still exist, there are negative cycles in the graph</li> </ol> <p>Outcomes: Bellman-Ford</p> <ul> <li>Find all shortest path to every node at most \\(|V| - 1\\) steps \u2705</li> <li>Able to detect negative circles</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#additive-heuristic-hadd","title":"Additive Heuristic (\\(h^{add}\\))","text":"<p>Take the sum of heuristic values to all goals from the current state.</p> <pre><code>def h_sum(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n\u270f\ufe0f              new_cost = 1 + sum(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n\u270f\ufe0f  return sum(cost[g] for g in G)\n</code></pre> <p>Outcomes of \\(h^{add}\\)</p> <ul> <li>Efficiently Computable\u2705 Polytime</li> <li>Pessimistic Estimation: admissible\u274c informative\u2705 </li> <li>Overcounts by ignoring positive interactions, i.e. shared sub-plans<ul> <li>May result in \\(dramatic\\) over-estimates of \\(h^*\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#max-heuristic-hmax","title":"Max Heuristic (\\(h^{max}\\))","text":"<p>Only take the \\(highest\\) heuristic value to such goal from the current state.</p> <pre><code>def h_max(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n                # a is reachable\n                new_cost = 1 + max(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n    return max(cost[g] for g in G)\n</code></pre> <p>Outcomes of \\(h^{max}\\)</p> <ul> <li>Efficient Computable\u2705 Polytime<ul> <li>However, sometimes maybe too optimistic\u26a0\ufe0f</li> </ul> </li> <li>Optimistic Estimation: admissible\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxed-plan-existence-mathrmplanex","title":"Relaxed Plan Existence  (\\(\\mathrm{PlanEx}^+\\))","text":"<p>Check if there exists a plan by Fast Forward Expansion.</p> <ul> <li>Fast Forward Expansion: Continuously expand facts set of the initial state by BrFS until it dominates all goal states.</li> </ul> <pre><code>def plan_exists(s, G, A):\n    if G in s:\n        return TRUE\n\n    reached = s\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return FALSE\n        reached = new_facts\n\n    return TRUE\n</code></pre> <p>Outcomes of \\(\\mathrm{PlanEx}^+\\)</p> <ul> <li>Sound, complete, Terminates in polytime\u2705<ul> <li>\\(\\mathrm{PlanEx}^+\\) now becomes a polytime problem</li> </ul> </li> <li>Safe\u2705, goal-aware\u2705, admissible\u274c</li> <li>Can ONLY be used to check existence of a Relaxed plan \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#helpful-actions","title":"Helpful Actions","text":"<p>An action is helpful iff:</p> <ul> <li>it is applicable in the current state (\\(pre(a) \\subseteq s\\))</li> <li>it is contained in the relaxed plan given by Fast Forward Expansion</li> </ul> <p>Expanding only helpful actions does not guarantee completeness.</p> <p>Helpful actions is a heuristic-based pruning, some necessary but non-intuitive might unachievable</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxed-plan-heuristic-h_ff","title":"Relaxed Plan Heuristic (\\(h_{FF}\\))","text":"<p>Derived from Fast Forward Expansion, we can further perform Greedy Backward Extraction for finding the optimal relaxed plan.</p> <ul> <li>Greedy Backward Extraction: Find the cheapest helpful action based on Best Supporter Function.</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#best-supporter-function-bsp","title":"Best Supporter Function \\(bs(p)\\)","text":"<p>Output the \\(cheapest\\) action \\(a\\) where \\(add(a) \\supseteq p\\).</p> <p>Prerequisites of a valid \\(bs(\\cdot)\\)</p> <p>\\(bs(\\cdot)\\) is closed</p> <ul> <li>Every predicate that can appear during regression must have a defined regression (no missing cases).</li> <li>Guarantees that backward search never gets stuck due to undefined subgoals.\u2705</li> </ul> <p>\\(bs(\\cdot)\\) is well-bounded</p> <ul> <li>The dependency / support graph of subgoals is acyclic.</li> <li>Ensures regression does not loop back to previous subgoals, preventing infinite recursion.\u2705</li> </ul> <p>By performing Fast Forward Expansion, if a relaxed plan exists, a closed well-founded \\(bs(\\cdot)\\) definitely exists, which means:</p> <ul> <li>There is a relaxed path from \\(I\\) to \\(G\\)</li> <li>Every \\(g\\) has at least one supporter, so as its subgoals</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#algorithm-h_ff","title":"Algorithm: \\(h_{FF}\\)","text":"<ul> <li>Perform Fast Forward Expansion</li> <li>For each goal states \\(g \\in G\\):<ul> <li>Find the cheapest \\(a = bs(g)\\)</li> <li>Add this action to the plan</li> <li>repeat on \\(bs(pre(a))\\)</li> </ul> </li> </ul> <pre><code>def h_FF(I, G, A):\n    if G in I:\n        return 0\n\n    # Forward Expansion - BrFS\n    reached = I\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return unsolvable\n        reached = new_facts\n\n    # Backward Extraction - Greedy\n    plan = []\n    subgoals = G\n    while subgoals not in I:\n        new_subgoals = []\n        for g in subgoals:\n            pick a s.t. g in add(a) and pre(a) in reached\n            plan += {a}\n            new_subgoals += pre(a)\n        subgoals = new_subgoals\n\n    return len(plan)\n</code></pre> <p>Outcomes of \\(h_{FF}\\)</p> <ul> <li>Same theoretical properties as \\(h^{add}\\):<ul> <li>May overcount sub-plans shared by different sub-goals \u26a0\ufe0f</li> </ul> </li> <li>Best Supporter is greedily selected and sub-optimal \u26a0\ufe0f</li> </ul> <p>However in practice, \\(h_{FF}\\) typically does not over-estimate \\(h^*\\), or not by a large amount.\ud83d\udcad</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html","title":"4 Exploration","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#balancing-the-exploitation-exploration","title":"Balancing the Exploitation &amp; Exploration","text":"<p>Exploitation: Trusting your heuristic function</p> <p>Exploration: Searching for Unseen Facts (Novelty)</p> <p>Trade-off Exploitation &amp; Exploration</p> <p>Exploitation</p> <ul> <li>State-based Satisfying Planning has multiple reliances \u26a0\ufe0f<ul> <li>heuristics derived from problem</li> <li>plugged into Greedy Best-First Search (GBFS)  </li> <li>extensions (e.g. helpful actions and landmarks)  </li> </ul> </li> <li>Often gets stuck in local minima \u274c<ul> <li>poly + sub-optimal or optimal + NP-hard</li> </ul> </li> </ul> <p>Exploration</p> <ul> <li>Novelty leads to much better performance in practice \u2705</li> <li>Can be model-free (No Reliance/Assumption) \u2705</li> <li>Required for optimal behaviour (in RL and MTCS) \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#novelty","title":"Novelty","text":"<p>Novelty: The size of the smallest subset of facts \\(P \\subseteq s\\), where such \\(s\\) is the first state that makes \\(P\\) <code>TRUE</code> during the search</p> <p>Width: The size of the smallest subset of \\(P\\) needed to be considered to achieve the goal</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#example","title":"EXAMPLE","text":"\\[ p \\to (p, q) \\to (q, r) \\to (p, r) \\to (p, q, r) \\] Current State Smallest New Subset Novelty \\(p\\) \\(p\\) 1 \\(p, q\\) \\(q\\) 1 \\(q, r\\) \\(r\\) 1 \\(p, r\\) \\(p, r\\) 2 \\(p, q, r\\) \\(p, q\\) 2"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#width-based-planning","title":"Width-Based Planning","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#iterated-width-iw","title":"Iterated Width (IW)","text":"<p>\\(IW(k)\\) </p> <ul> <li>BFS on \\((Q \\; \\backslash \\; s), \\mathrm{novelty}(s) &gt; k\\)</li> <li>\\(IW(1) =\\) There is new \\(p\\) appearing in every step in search</li> </ul> <p>\\(IW\\) Algorithm</p> <ul> <li>A sequence of calls \\(IW(k), k=1,2,3,\\ldots\\), until the problem solved or \\(k &gt; len(\\bigcup P)\\) (return <code>unsolvable</code>). </li> <li>The \\(minimum\\) \\(k\\) is the \\(\\text{Width}\\) of the problem</li> </ul> <p>Outcomes: IW</p> <ul> <li>Simple and Blind; No need for heuristic \u2b50\ufe0f</li> <li>Fast if small width \\(\\mathrm{O}(n^k)\\)\u2705<ul> <li>In practical, only \\(IW(k \\le 2)\\) can solve most of classical planning problems. </li> </ul> </li> <li>Optimal if in uniform cost \u2705</li> <li>Complete \u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#serialised-iterated-width-siw","title":"Serialised Iterated Width (SIW)","text":"<p>Use \\(IW\\) for decomposing multi-goals problem and solving sub-problems (with single goal) individually.</p> <pre><code>def SIW(s, G):\n    state = s\n    plan = []\n    for g in serialize(G):\n        subplan = IW(state, goal=g)\n        plan += subplan\n        state = Result(subplan, state)\n    return plan\n</code></pre> <p>Outcomes: SIW</p> <ul> <li>Better performance in Joint Goals problem (Multi goals but similar approaches). \u2705</li> <li>Goals need to be easy to serialise and have low width. \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#best-first-width-search-bfws","title":"Best-First Width Search (BFWS)","text":"<p>BFWS is a Framework:</p> <ul> <li>Framework: Get communication across researchers and to build on each others\u2019 work</li> <li>BFWS can be used as a framework to implement different measures</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#bfwsf","title":"BFWS(\\(f\\))","text":"<p>Apply Best-first Search on a sequence of measures:</p> \\[ BFWS(f) \\text{ for } f= \\langle w,f_1,f_2,\\ldots \\rangle \\] <p>where</p> <ul> <li>\\(w\\) - Novelty-Measure</li> <li>\\(f_i\\) - tie breakers</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#classical-bfws","title":"Classical BFWS","text":"\\[ f = \\langle w, h \\rangle \\] <p>where</p> <ul> <li>\\(h = h_{add} \\text{ or } h_{FF}\\) </li> <li>\\(w = w_h(s) =\\) \\(s'\\) which has smallest novelty and \\(h(s') = h(s)\\)</li> </ul> <p>Outcomes: Classical BFWS</p> <ul> <li>Still simple but practical \u2705</li> <li>Able to be implemented as a simulator \u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#model-simulator","title":"Model \u2192 Simulator","text":"<p>Model: Describe problems in a compact form. (e.g. STRIPS, PDDL)</p> <p>Simulator: Black-box function that returns only the next state and reward. </p> <ul> <li>No explicit preconditions or effects (no \\(pre(a), add(a), del(a)\\)).  </li> <li>Used by RL / MCTS for forward simulation, not symbolic reasoning.</li> </ul> <p>Model vs Simulator</p> <p>Model</p> <ul> <li>Powerful with development of declarative programming:<ul> <li>Expressive language features easily supported</li> <li>Development of external development tools</li> <li>Fully-Black-Box detailed procedures for higher-level abstraction and reasoning and decomposing problems</li> </ul> </li> <li>Limitations:<ul> <li>Model \\(\\ne\\) Language<ul> <li>Many problems fit Classical Planning model, but hard to express in STRIPS or PDDL</li> </ul> </li> <li>Declarative \\(\\ne\\) Practical<ul> <li>Simulation platforms may be black-boxes as well, they need for planners that work without complete declarative representations</li> </ul> </li> </ul> </li> </ul> <p>Simulator</p> <ul> <li>At the same level of efficiency as classic models</li> <li>Open up exciting possibilities for modelling beyond PDDL (Model-free RL)</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#simulated-bfws","title":"Simulated BFWS","text":"<p>Implement BFWS(\\(f\\)) algorithm to plan for simulators:</p> <ul> <li>Instead of using Transition Function in PDDL or STRIPS, run the simulator and record such transitions:</li> </ul> \\[ s' = \\text{Simulator.step}(s,a) \\] <ul> <li>Choose the best action based on \\(BFWS(\\langle w_h(s), h(s) \\rangle)\\)</li> </ul> Challenges of Width-Based Planning over Simulators <ul> <li>Non-linear dynamics  </li> <li>Perturbation in flight controls  </li> <li>Partial observability  </li> <li>Uncertainty about opponent strategy</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#plan-goal-recognition","title":"Plan &amp; Goal Recognition","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#motivation","title":"Motivation","text":"<p>Folk Psychology</p> <p>Folk Psychology</p> <p>Humans can explain and predict behaviour and mental state of others.</p> <p>To solve hard problems, we may need our planners recognise the \"intention\" and \"motivation\" behind the problem and environment.</p> <p>Common Sense</p> <p>Common sense makes human's initiative possible.</p> <p>A Theory of Common Sense</p> <ul> <li>We always assume that others are rational.</li> <li>We speculate beliefs, goals, reasons of others' actions.</li> <li>We use practical reasoning to assess what others' will do next.</li> </ul> <p>Planners can also use such formalised reasoning system as well.</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#planning-plan-goal-recognition","title":"Planning \u2192 Plan &amp; Goal Recognition","text":"<p>Planning</p> <p>Given a set of goals, find a possible plan to achieve all goals from the current state.</p> <p>Plan &amp; Goal Recognition (PR/GR)</p> <p>Given a list of actions, find most possible goals accounting for such partially observed plan.</p> <p>PR/GR = Planning in reverse</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#formalising-gr","title":"Formalising GR","text":"<ol> <li>Input: Partially Observed Actions + Domain Model</li> <li>For every potential goals \\(g_i\\)<ul> <li>Use classical planner to generate a plan from \\(s_0\\) to \\(g_i\\)</li> <li>Check if such plan accounting for input actions</li> </ul> </li> <li>Output: Select the most reasonable goals and/or in probabilistic distribution form</li> </ol> Transfer to Probabilistic Goals <p>Derive from Bayes' Rule</p> \\[ P(G \\mid O) = \\alpha \\, P(O \\mid G)\\, P(G), \\] <p>where</p> \\[ P(O \\mid G) = \\mathrm{exp}\\!\\left( - \\big(c^{*}(P'[\\neg O \\mid G]) - c^{*}(P'[O \\mid G])\\big) \\right). \\] <ul> <li>\\(c^{*}(P'[\\neg O \\mid G])\\) gives the optimal cost to \\(G\\) NOT accounting for the partial observations \\(O\\)</li> <li>\\(c^{*}(P'[O \\mid G])\\) gives the optimal cost to \\(G\\) accounting for \\(O\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html","title":"5 Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning","title":"Reinforced Learning","text":"<p>Key: Policy + Reward + Trial-and-error Interaction</p>"},{"location":"Unimelb/25S2/AIP/05-RL.html#planning-vs-learning","title":"Planning vs Learning","text":"Dimension Planning Learning Environment Model Model is known Model is unknown Learning Mode Offline computation Online trial-and-error Agent\u2013Environment Interaction No interaction with the real environment; uses internal simulator Must act in the real environment to gather experience Policy Improvement Through search, deliberation,  planning and introspection Through reward-driven learning Suitable Scenarios A precise model exists and simulation is cheap The model is unknown or hard to specify Advantages Safe, interpretable, no real-world risk Adaptive, risky working in complex/unknown environments Disadvantages Requires accurate model; modeling may be expensive Requires exploration; may be costly or risky Example - Atari Game Agent can query emulator for perfect model (source code) Agent can only sees pixels and scores on the screen \u2192 trial-and-error gameplay is necessary"},{"location":"Unimelb/25S2/AIP/05-RL.html#rl-vs-planning-vs-other-ml","title":"RL vs. Planning vs. Other ML","text":"Dimension Reinforcement Learning Automated Planning Other ML Action Outcomes Non-deterministic \u2014 actions lead to probabilistic transitions Deterministic \u2014 outcome fully known from model Usually not modelled as sequential decisions Environment Representation Probabilistic model of states, transitions, and rewards Symbolic or logical model (e.g., STRIPS) No explicit environment Learning Signal Reward Signal - Feedback from environment Predefined goal or planner objective Labels or self-structures Data Structure Sequential / Time series (non-i.i.d.) Discrete steps in a planning domain Often i.i.d. samples (independent &amp; identically distributed) Search &amp; Optimisation Trail-and-error search + Reward-driven state-space search + Predefined policy Gradient-based or statistical fitting Credit Assignment Required \u2014 reward may be delayed over time Not relevant \u2014 goal known a priori Not Required - no delay"},{"location":"Unimelb/25S2/AIP/05-RL.html#example-common-applications","title":"Example - Common Applications","text":"<ul> <li>Making a humanoid robot walk</li> <li>Fine tuning LLMs using human/AI feedback</li> <li>Optimising operating system routines</li> <li>Controlling a power station</li> <li>Managing an investment portfolio</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning-process","title":"Reinforced Learning Process","text":"<pre><code>flowchart LR\n    subgraph Env[Environment]\n        S[(State)]\n        R[(Reward)]\n    end\n\n    subgraph Agent[Agent]\n        P[\"Policy \u03c0(a|s)\"]\n        V[\"(Optional) Value Function\"]\n    end\n\n    S --&gt;|\"(1) Initial State s\"| P\n    P --&gt;|\"(2) Action a\"| Env\n    Env --&gt;|\"(3) Reward r, Next State s'\"| V\n    V --&gt;|\"(4) Update Policy\"| P\n</code></pre>"},{"location":"Unimelb/25S2/AIP/05-RL.html#environments","title":"Environments","text":""},{"location":"Unimelb/25S2/AIP/05-RL.html#state-s-p","title":"State - \\(S, P\\)","text":"<p>All RL Algorithms assume that State is Markov:</p> \\[ P(s' \\mid s + H(s), a) = P(s' \\mid s, a) \\] <ul> <li>Once \\(S\\) is known, \\(H\\) can be thrown away</li> </ul> <p>Any RL problem can be made Markov by expanding the state:</p> <ul> <li>Add history into the current state</li> <li>Add belief state (POMDP \u2192 belief-MDP)</li> <li>Add latent state (Model-based RL)</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reward-r","title":"Reward - \\(R\\)","text":"<p>A scalar feedback signal, indicating how well agent is doing at one step.</p> <p>Reward Hypothesis</p> <p>All\u00a0goals can be described by the \\(maximisation\\) of expected cumulative reward</p>"},{"location":"Unimelb/25S2/AIP/05-RL.html#agents","title":"Agents","text":"<ul> <li>Prediction: Evaluate the future rewards of state-actions<ul> <li>\u2192 Value Function</li> </ul> </li> <li>Control: Find the optimal policy<ul> <li>\u2192 Policy Function</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#model-latent-p-r","title":"Model - Latent \\(P, R\\)","text":"<ul> <li>An internal simulator for predicting what the environment will do next<ul> <li>Transition model: \\(P_{ss'}^a\\) - how each action changes the state</li> <li>Reward model: \\(R_{s}^a\\) - immediate reward from each state</li> <li>The model can be imperfect but supports planning and prediction</li> </ul> </li> <li>Simulation is NOT necessary for RL <ul> <li>\u2192 Model-based/Model-free</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#value-function-v-q","title":"Value Function - \\(V, Q\\)","text":"<ul> <li>Define and predict values of states based on the expectation of future rewards</li> <li> <p>State-value Function</p> <ul> <li>Output the value of current state     $$     V_\\pi(s) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t = s\\big]     $$</li> </ul> </li> <li> <p>State-action-value Function</p> <ul> <li>Output the value of the current state with a deterministic action applied     $$     Q_\\pi(s,a) = \\mathbb{E}_\\pi \\big[\\, G_t \\mid S_t = s,\\, A_t = a \\,\\big]     $$</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#return-g_t","title":"Return - \\(G_t\\)","text":"<ul> <li>total discounted reward of the future     $$     G_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma (R_{t+3} + \\ldots)) = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#bellman-expectation-equation","title":"Bellman Expectation Equation","text":"<ul> <li> <p>Bellman Equation: Gives a recursive form of Return         $$ G_t = R_{t+1} + \\gamma G_{t+1} $$</p> <ul> <li>Based on \"look-ahead then back-up\" computational mechanism<ul> <li>Look-ahead: to the next step \u2192 \\(r, s'\\)</li> <li>Back-up: to the current state \u2192 \\(v(s) = f(r,s')\\)</li> </ul> </li> </ul> </li> <li> <p>State-value Function (Deriving BEE)</p> </li> </ul> \\[ \\begin{aligned} v_\\pi(s) &amp;= \\mathbb{E}\\!\\left[r + \\gamma v_\\pi(s')\\right] \\\\[6pt] &amp;= \\sum_a \\pi(a\\mid s)\\Bigl[\\sum_r p(r\\mid s,a)\\,r + \\gamma \\sum_{s'} p(s'\\mid s,a) \\, v_\\pi(s')\\Bigr] \\end{aligned} \\] <ul> <li>State-action-value Function (Deriving BEE)</li> </ul> \\[ \\begin{aligned} q_\\pi(s,a) &amp;= \\mathbb{E}\\!\\left[\\, r + \\gamma\\, \\mathbb{E}_{a'\\sim\\pi(\\cdot\\mid s')} \\bigl[q_\\pi(s',a')\\bigr] \\right] \\\\[6pt] &amp;= \\sum_{s',r} p(s',r\\mid s,a)\\left[ r + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\, q_\\pi(s',a') \\right] \\end{aligned} \\] <ul> <li> <p>Solving the BEE</p> <ul> <li> <p>Directly Compute \\(O(n^3)\\)     $$     v = R + \\gamma Pv \\to v = (I - \\gamma P)^{-1}R     $$</p> <ul> <li>only possible for small \\(P\\) matrix</li> </ul> </li> <li> <p>Dynamic Programming</p> </li> <li>Monte-Carlo Evaluation</li> <li>Temporal-Difference Learning</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#discount-factor-gamma","title":"Discount Factor\u00a0- \\(\\gamma\\)","text":"Gamma (\\(0 \\le \\gamma \\le 1\\)) Behaviour \\(\\gamma = 0\\) Greedy \\(\\gamma \\to 0\\) Myopic \\(\\gamma \\to 1\\) Far-sighted \\(\\gamma = 1\\) Guarantee only if all sequence terminate Why Discounting is Used? <p>Technical Reasons</p> <ul> <li>Makes modelling and computation easier (Bellman equations converge cleanly)</li> <li>Prevents infinite returns in cyclic Markov processes</li> <li>Reflects uncertainty about far-future outcomes</li> </ul> <p>Realistic Reasons</p> <ul> <li>In financial settings, immediate rewards can be reinvested (time value of money)</li> <li>Human and animal behaviour shows preference for immediate rewards over delayed rewards</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#policy-pi","title":"Policy - \\(\\pi\\)","text":"<p>Fully defines agent's behaviour</p> <ul> <li>Stationary(Time-independent): Only relies on the current state</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning-algorithms","title":"Reinforced Learning Algorithms","text":"<ul> <li>Markov Decision Processing<ul> <li>Value Iteration</li> <li>Policy Iteration</li> </ul> </li> </ul> <p>Model-based \u2192 Model-free</p> <ul> <li>Monte Carlo</li> </ul> <p>Non-incremental \u2192 Incremental</p> <ul> <li>Temporal Difference<ul> <li>SARSA</li> <li>Q-Learning</li> </ul> </li> </ul> <p>Tabular representation \u2192 Function representation</p> <ul> <li>Value Function Approximation</li> </ul> <p>Deterministic \u2192 Stochastic Policy</p> <ul> <li>Finite Difference</li> <li>Policy Gradient<ul> <li>REINFORCE</li> <li>Actor-Critic</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html","title":"6 Markov Decision Process","text":""},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-decision-process","title":"Markov Decision Process","text":"<p>General Simulator of the environment for Model-based RL</p>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-process","title":"Markov process","text":"<p>A memoryless\u00a0random process </p> <ul> <li>Memoryless: the state is Markov</li> <li>Introduce non-deterministic in terms of probabilistic transition \\(P\\)</li> </ul> \\[ M = \\langle S, P \\rangle \\] <p>where</p> <ul> <li>\\(S\\) - a finite set of states</li> <li>\\(P\\) - a matrix showing state transition probability<ul> <li>\\(P(s \\to s') = P(s' \\mid s), \\forall s, s' \\in S\\)</li> <li>Each row/column of \\(P\\) sums to 1</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-reward-processes-mrp","title":"Markov Reward Processes (MRP)","text":"<p>A Markov chain with reward values:</p> \\[ M = \\langle S, P, R, \\gamma \\rangle \\] <p>where</p> <ul> <li>\\(R\\) - the reward function<ul> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> <li>\\(\\gamma\\) - discount factor</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>Introduce agency in terms of actions:</p> \\[ M = \\langle S, A, P, R, \\gamma \\rangle \\] <ul> <li>\\(A\\) - a finite set of actions</li> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#partially-observable-mdp-pomdp","title":"Partially Observable MDP (POMDP)","text":"<p>Assumption</p> <ul> <li>Markov Property</li> <li>Limited Observable</li> </ul> <p>Components (Diff with MDP)</p> <ul> <li>Introduce Sensor Model (based on Probability Distribution)</li> </ul> \\[ \\mathrm{b}(s_t) = P(s_t \\mid \\text{historical actions and observations}) \\] <ul> <li>Initial/Goal States \u2192 Belief States \\(\\mathrm{b}(s_0)\\) and \\(\\mathrm{b}(g)\\)</li> </ul> <p>Outcomes</p> <ul> <li>Map belief states into actions</li> <li>Optimal if total expected cost from \\(\\mathrm{b}(s_0)\\) to \\(\\mathrm{b}(g)\\) is minimum</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#greedy-optimal-policy","title":"Greedy Optimal Policy","text":"<p>Give a policy with \\(maximum\\) state-action-value</p> <p>Optimal State-value Function</p> \\[ V^\\ast(s) = \\max_\\pi V_\\pi(s) \\] <p>Optimal State-action-value Function</p> \\[ Q^\\ast(s,a) = \\max_\\pi Q_\\pi(s,a) \\] <p>Optimal Policy</p> \\[ a=\\arg\\max_a Q^\\ast(s,a) \\]"},{"location":"Unimelb/25S2/AIP/06-MDP.html#bellman-optimality-equation","title":"Bellman Optimality Equation","text":"<p>Derived from the Bellman Equation, we can use \"immediate reward + discounted return of next state\" to express the expected return:</p> <p>Optimal State-value Function</p> \\[ \\begin{align} V^\\ast(s) &amp;= \\max_a \\; \\mathbb{E}\\!\\left[\\, r + \\gamma V^\\ast(s') \\,\\right] \\\\[6pt] &amp;= \\max_a \\; \\sum_{s'} P(s' \\mid s,a)\\left[\\, r + \\gamma V^\\ast(s') \\,\\right] \\end{align} \\] <p>Optimal State-action-value Function</p> <p>Select the action leading to \\(maximum\\) state-action-value of the next state:</p> \\[ \\begin{align} Q^\\ast(s,a) &amp;= \\mathbb{E} \\left[\\, r + \\gamma \\max_{a'} Q^\\ast(s',a') \\,\\right] \\\\[6pt] &amp;= \\sum_{s',r} P(s',r\\mid s,a)\\left[\\, r + \\gamma \\max_{a'} Q^\\ast(s',a') \\,\\right] \\end{align} \\] How to solve the Bellman Optimality Equation  <ul> <li>No closed form solution</li> <li>Value Iteration</li> <li>Policy Iteration</li> <li>SARSA</li> <li>Q-learning</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#example-maze","title":"Example - MAZE","text":"<p>Environment:</p> <ul> <li>\\(S\\) - Agent's possible locations</li> <li>\\(A\\) - Step directions \\(\\mathtt{N, E, S, W}\\)</li> <li>\\(P\\) - Map \\(s, a \\to s'\\)</li> <li>\\(R\\) - \\(-1\\) per time-step (encourage short-path solution)</li> </ul> <p>Agent:</p> <ul> <li>\\(V(s)\\) - the expected return of following the policy from each \\(s\\) <ul> <li>closer to \\(g\\) - \\(V(s)\\) \u2934</li> <li>Farther away - \\(V(S)\\) \u2935</li> </ul> </li> <li>\\(\\pi(s)\\) - Optimal Policy = Best \\(V(s')\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html","title":"7 Monte Carlo","text":""},{"location":"Unimelb/25S2/AIP/07-MC.html#model-based-model-free","title":"Model-based \u2192 Model-free","text":"<p>Model-based</p> <ul> <li>Learn from Simulator of Environment<ul> <li>Commonly use MDP to model the Markov Environment</li> </ul> </li> </ul> <p>Model-free</p> <ul> <li>Learn Experience from Environment<ul> <li>Experience: \\(E_t = \\langle S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, ... \\rangle\\)</li> </ul> </li> <li>Optimise \\(V, Q, \\pi\\) directly</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#monte-carlo-mc-learning","title":"Monte Carlo (MC) Learning","text":"<ol> <li>Run-to-end in the environment for multiple times</li> <li>Use Average Actual Return to estimate value function</li> </ol> \\[ V(s) \\approx \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)} \\] <p>where</p> <ul> <li>\\(N(s)\\) - how many times \\(G_t\\) is visited</li> <li>When \\(N(s) \\to \\infty\\), \\(V(s) \\to v_\\pi(s)\\).</li> </ul> <p>Outcomes: Monte Carlo</p> <ul> <li>Unbiased\u2705</li> <li>Can only apply to episodic (terminating) Markov environment\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-policy-iteration","title":"MC Policy Iteration","text":"<p>Directly transfer Policy Iteration into a Monte-Carlo version.</p> <p>Algorithm</p> <ul> <li>For each iteration:<ol> <li>Policy Evaluation (Predication)<ul> <li>For each \\((s, a)\\):<ul> <li>run sufficiently many episodes and get average return</li> <li>When \\(N \\to \\infty\\), \\(Q(s, a) \\to q_\\pi(s, a)\\)</li> </ul> </li> </ul> </li> <li>Policy Improvement (Control)<ul> <li>Give the Greedy Optimal Policy based on estimated \\(Q\\)<ul> <li>\\(a^\\ast = \\arg\\max_a q_\\pi(s,a)\\)</li> </ul> </li> </ul> </li> </ol> </li> </ul> <p>Outcomes: MCPI</p> <ul> <li>Convergence guarantee\u2705 </li> <li>Very low Time Efficiency; Not practical\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-exploring-starts","title":"MC Exploring Starts","text":"<p>Generalised Policy Iteration (GPI): Repeatedly interleave evaluation and improvement, instead of doing them once.</p> <p>Exploring Starts: Randomly select start points to avoid missing the optimal episode.</p> <p>Algorithm</p> <ul> <li>Repeat:<ul> <li>Start from a random \\((s_0, a_0)\\)</li> <li>For each \\((s,a)\\) visited:<ul> <li>\\(N(s,a) \\gets N(s,a) + 1\\)</li> <li>Update \\(Q(s,a) \\gets \\frac{1}{N(s,a)}(Q(s,a) + (G - Q(s,a))\\)</li> <li>If \\(Q\\) is updated, update \\(a^\\ast\\)</li> </ul> </li> </ul> </li> </ul> <p>Outcomes: ES-MC</p> <ul> <li>Data is used much more efficiently</li> <li>Exploring Start can highly guarantee that every node is visited</li> <li>However difficult in practice (limitation in environment)</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#first-visit-vs-every-visit","title":"First-visit vs. Every-visit","text":"<p>Different ways to handle repeated visits of the same \\((s,a)\\) pair during an episode:</p> Aspect First-Visit MC Every-Visit MC When? Only the first time visit Every time visit Pros Avoids dependence between multiple visits Faster convergence in practice Cons Some visits ignored Higher variance Suitable Scenario Long episodes with repeated states Short episodes or rare-state situations"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-greedy","title":"MC + \u03b5-greedy","text":"<p>Use a soft policy (e.g., \u03b5-greedy) to avoid the need for exploring starts.</p> \\[ \\pi(a|s) = \\begin{cases} 1 - (1 - \\frac{1}{|\\mathcal{A}(s)|}) \\, \\varepsilon, &amp; a = a^*_k \\\\[6pt] \\frac{1}{|\\mathcal{A}(s)|} \\, \\varepsilon, &amp; \\text{otherwise} \\end{cases} \\] <p>where</p> <ul> <li>\\(|\\mathcal{A}(s)|\\) - number of valid actions of state</li> <li>Balance Exploration &amp; Exploitation</li> </ul> <p>Outcomes: \u03b5-greedy MC</p> <ul> <li>Ensures continual exploration by assigning non-zero probability to all actions. \u2705</li> <li>More stable than pure greedy MC because it prevents being trapped in suboptimal actions early. \u2b50\ufe0f</li> <li>Slight performance trade-off: exploration introduces variance, but improves long-term value estimation and policy quality. \ud83d\udcad</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html","title":"8 Temporal Difference","text":""},{"location":"Unimelb/25S2/AIP/08-TD.html#non-incremental-incremental","title":"Non-incremental \u2192 Incremental","text":"<p>Non-incremental Methods(Batch)</p> <ul> <li>Parameters updating per a batch of samples (a episode)</li> <li>Solving the optimal problem at once</li> <li>Offline Learning <ul> <li>Can handle high stochastic/batch environment\u2705</li> <li>Large space complexity\u274c </li> </ul> </li> <li>Closed-form<ul> <li>High computation cost\u274c</li> </ul> </li> </ul> <p>Incremental Methods</p> <ul> <li>Parameters updating per one sample (a step)</li> <li>Step by step approaching the optimum</li> <li>Online Learning \u2192 One sample use once\u2705</li> <li>Higher generalisation\u2705</li> <li>Lower convergence; may can only approach local optima\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>TD is the most basic incremental method based on Bootstrapping.</p> <p>Bootstrapping: Use estimation of future values to update the current value.</p> \\[ \\begin{align} \\text{TD-target} &amp; = \\text{sample} + \\gamma \\cdot \\text{bootstrap} \\\\ &amp; = \\text{immediate reward} + \\gamma \\cdot \\text{future value} \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#prediction-of-state-value","title":"Prediction of State Value","text":"<p>For each step, if \\(s_t\\) is visited, update its value by looking forward to the reward of next step(s).</p> <ul> <li>CANNOT estimate action values or optimal policies \u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#td0","title":"TD(0)","text":"<p>Look into the next \\(1\\) step:</p> \\[ \\begin{align} V(s_t)  &amp;\\gets V(s_t) + \\alpha \\big({{G_t^{(1)} - V(s_t))}} \\\\[2pt] &amp;= V(s_t) + \\alpha(R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\end{align} \\] <p>where</p> <ul> <li>\\(\\vec{v} = R_{t+1} + \\gamma V(s_{t+1})\\) - TD Target</li> <li>\\(\\delta = \\vec{v} - V(s_t)\\) - TD Error<ul> <li>Reflects the difference between \\(v_t\\) and \\(v_\\pi\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#tdn","title":"TD(n)","text":"<p>Look into \\(n-1\\) steps:</p> \\[ G^{(n)}_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + {{\\gamma^n V(S_{t+n})}} \\] \\[ \\begin{align*} \\color{red}{n} &amp; \\color{red}{= 1}\\ \\text{(TD(0))}      &amp; G_t^{(1)}      &amp; = R_{t+1} + \\gamma V(S_{t+1})\\\\[2pt] \\color{red}{n} &amp; \\color{red}{= 2}                   &amp; G_t^{(2)}      &amp; = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})\\\\[2pt] \\color{red}{\\vdots} &amp;                   &amp; \\vdots         &amp; \\\\[2pt] \\color{red}{n} &amp; \\color{red}{= \\infty}\\ \\text{(MC)} &amp; G_t^{(\\infty)} &amp; = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_T \\end{align*} \\] <p>Update in direction of error:</p> \\[ V(S_t) \\;\\gets\\; V(S_t) + \\alpha \\big({{G_t^{(n)} - V(S_t)}} \\big) \\] How to choose a good \\(n\\) <ul> <li>Root Mean Square (RMS) Errors<ul> <li>change \\(\\alpha\\) \u2192 vary RMS Errors \u2192 different optimal \\(n\\)</li> </ul> </li> <li>Large \\(n\\) \u2192 More rely on exploitation, more precise but higher variance</li> <li>Small \\(n\\) \u2192 More rely on prediction, faster but higher bias</li> <li>If \\(n = \\infty\\) , it turns into MC basic</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#td","title":"TD(\u03bb)","text":"<p>Average n-Steps Returns - use weight \\({{(1 - \\lambda)\\lambda^{\\,n-1}}}\\) to balance short-term and long-term rewards</p> \\[ G_t^{\\lambda} \\;=\\; {{(1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{\\,n-1}}} G_t^{(n)} \\] \\[ V(S_t) \\;\\leftarrow\\; V(S_t) + \\alpha \\big( G_t^{\\lambda} - V(S_t) \\big) \\] <p>Outcomes: TD(\u03bb)</p> <ul> <li>\u2705Memoryless - i.e space complexity is same as TD(0)</li> <li>\u26a0\ufe0fForward View - We cannot look into the far future during learning\ud83d\udc47</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#backward-view","title":"Backward View","text":"<p>Update weight step by step by using Eligibility Trace:</p> <ul> <li>Initially set traces of all states \\(E_0(s) \\gets 0\\)</li> <li> <p>In each step:</p> <ul> <li> <p>For each state:</p> <ul> <li>reduce traces value by \\(\\gamma \\lambda\\)</li> <li>\\(E_t(s)+1\\) if \\(s\\) is currently visited</li> </ul> \\[ E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}(S_t = s) \\] </li> <li> <p>Compute TD Error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\] </li> <li> <p>Update Value Function</p> \\[ V(s) \\gets V(s) + \\alpha \\, \\delta_t \\, E_t(s) \\] </li> </ul> </li> </ul> <p>At the end of episodes: Backward View = Forward View</p> \\[ \\sum_{t=1}^T \\alpha \\, \\delta_t \\, E_t(s) = \\sum_{t=1}^T \\alpha (G_t^\\lambda  - V(S_t)) \\; \\mathbf{1}(S_t = S) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#prediction-of-action-value","title":"Prediction of Action Value","text":"<p>Sarsa : \"State-action-reward-state-action\" Algorithm</p> <p>A state-action function version of TD</p> \\[ Q(s_t,a_t) \\gets Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#sarsa","title":"Sarsa","text":"<ul> <li>For each episode:<ul> <li>For each state:<ul> <li>If \\(s_t \\ne g\\):<ul> <li>Collect the experience \\(\\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\\}\\)<ul> <li>\\(a_t\\) \u2190 \\(\\pi(a \\mid s_t)\\)</li> <li>\\(r_{t+1}, s_{t+1}\\)  \u2190 Environment</li> <li>\\(a_{t+1}\\)  \u2190 \\(\\pi(a \\mid s_{t+1})\\)</li> </ul> </li> <li>Update \\(q(s_t, a_t)\\)</li> <li>Update \\(\\pi\\) based on policy iteration</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#n-step-sarsa","title":"\\(n\\)-step Sarsa","text":"<p>Sarsa version of TD(n)</p> \\[ q_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q(S_{t+n}) \\] \\[ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t)   + \\alpha \\Big( q_t^{(n)} - Q(S_t, A_t) \\Big) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#sarsalambda","title":"Sarsa(\\(\\lambda\\))","text":"<p>Sarsa version of TD(\\(\\lambda\\))</p> \\[ q_t^\\lambda \\;=\\; (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} q_t^{(n)} \\] <ul> <li> <p>Forward View: $$ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t) + \\alpha \\Big( q_t^\\lambda - Q(S_t, A_t) \\Big) $$</p> </li> <li> <p>Backward View:</p> </li> </ul> \\[ \\begin{align*} E_0(s,a) &amp; = 0 \\\\[0pt] E_t(s,a) &amp; = \\gamma \\lambda E_{t-1}(s,a) + \\mathbf{1}(S_t = s, A_t = a) \\end{align*} \\] \\[ \\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\] \\[ Q(s,a) \\;\\leftarrow\\; Q(s,a) + \\alpha \\,\\delta_t \\, E_t(s,a) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#off-policy-control","title":"Off-Policy Control","text":"<p>Off-policy: Distinguish Target Policy and Behaviour (Sampling) Policy</p> <ul> <li>\u2705Take use of experience from other policies</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#on-policy-off-policy","title":"On-policy \u2192 Off-policy","text":"<p>Sarsa (On-policy)</p> <ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_t(s_t)\\) and \\(a_{t+1}\\) from \\(\\pi_t(s_{t+1})\\)</li> <li>Target Policy = Behaviour Policy = \\(\\pi_t\\) </li> </ul> <p>Q-Learning (Off-policy)</p> <ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} q_t(s_{t+1}, a)\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_b(s_t)\\)</li> <li>Target Policy = Greedy Optimal \\(q_t\\)</li> <li>Behaviour Policy = Any policies (\\(\\pi_b\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#q-learning","title":"Q-Learning","text":"<ul> <li>For each episode generated by \\(\\pi_b\\):<ul> <li>For each time step:<ul> <li>Update q-value<ul> <li>\\(q_{t+1}(s_t, a_t) \\gets q_t(s_t, a_t) + \\alpha \\bigl(  \\vec v - q_t(s_t, a_t) \\bigr)\\)</li> </ul> </li> <li>Update target policy<ul> <li>\\(a = \\arg\\max_a q_{t+1} (s_t, a)\\)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#mc-vs-td","title":"MC vs. TD","text":"Aspect Monte-Carlo (MC) Temporal-Difference (TD) Learning timing Offline - Update after the episode ends Online - Update immediately after receiving a reward Environment Assumption episodic  only both episodic and continuing Target used \\(\\vec{v} = G_t\\) \\(\\vec{v} = R_{t+1} + \\gamma V(S_{t+1}\\)) Bootstrapping? \u274c \u2705Value Updating relies on the previous estimate of this value (require initial guesses) Variance \u26a0\ufe0fHigh - Samples of lots of variables \u2705Low Bias Unbiased (return is true target) Biased (bootstraps using V(s\u2032)) Exploits Markov property? \u274c \u2705 Efficiency in Markov environment \u26a0\ufe0fLess efficient \u2705More efficient Efficiency in non-Markov or PMDP env \u2705More efficient with backup learning \u26a0\ufe0fLess efficient Optimisation in limited experience (Batch) Average Return = minimise MSE Fit the most likelihood Markov model that best explains the data"},{"location":"Unimelb/25S2/AIP/08-TD.html#example-driving-home","title":"Example - Driving Home","text":"State Elapsed Time (min) Predicted Time to Go Predicted Total Time MC Update (After Back Home) MC New Predicated Time TD(0) Update - \\(\\gamma = 1, \\alpha = 0.5\\) TD New Predicated Time leaving office 0 30 30 43 43-0=43 30+0.5(5+35-30)=35 35-0=35 reach car, raining 5 35 40 43 43-5=38 40+0.5(20+15-40)=37.5 37.5-5=32.5 exit highway 20 15 35 43 43-20=23 35+0.5(30+10-35)=37.5 37.5-20=17.5 behind truck 30 10 40 43 43-30=13 40+0.5(40+3-40)=41.5 41.5-30=11.5 home street 40 3 43 43 43-40=3 43+0.5(43+0-43)=43 43-40=3 arrive home 43 0 43 43 43-43=0 43 43-43=0"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html","title":"9 Value Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#tabular-representation-function-representation","title":"Tabular Representation \u2192 Function Representation","text":"<p>Tabular Presentation:</p> <ul> <li>All values/policies are discrete and stored in a table</li> <li>We update values by change it directly</li> </ul> <p>Function Presentation:</p> <ul> <li>We construct a function for fitting the value distribution</li> <li>We update value by tuning parameters of the function</li> </ul> Aspect Tabular Function Approx. Interpretability \u2705Intuitive \u274cHard to interpret Scalability \u274cDifficult for large/continuous spaces \u2705Much fewer memories required Generalisability \u274cPoor \u2705Generalises from seen situations to unseen situations Bias \u2705Accurate when state space is small \u26a0\ufe0fBiased - State values cannot be represented accurately"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#function-approximation","title":"Function Approximation","text":"<p>Approximate value function by constructing a new function and tuning parameters \\(w\\).</p> <ul> <li>instead of a table (\\(s\\)-\\(a\\)) of discrete values (tabular)</li> </ul> \\[ \\begin{align} \\hat{v}(s, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; v_{\\pi}(s)\\\\[0pt] \\;\\;\\;\\hat{q}(s, a, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; q_{\\pi}(s, a) \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#object-function","title":"Object Function","text":"<p>Goal: \\(minimise\\) MSE between approx function and true value function.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_\\pi \\left[ (v_\\pi(S) - \\hat{v}(S, \\mathbf{w}))^2 \\right] \\] <p>where \\(S \\in \\mathcal{S}\\) - the state is a Random Variable which follows the probability distribution, e.g.:</p> <ul> <li>Uniform Distribution<ul> <li>All with the same probability</li> <li>\\(S = \\frac{1}{|\\mathcal{S}|}\\)</li> <li>\u274cToo simple; not practical</li> </ul> </li> <li>Stationary Distribution<ul> <li>Describe the long-term behaviour of a Markov process<ul> <li>\\(d_\\pi(s) \\approx \\frac{N(s)}{\\sum_{s \\in \\mathcal{S}} N(s)}\\)</li> </ul> </li> <li>Which can also be predicted by using \\(P\\) from a model (MDP)<ul> <li>\\(d_\\pi = d_\\pi P\\)</li> </ul> </li> <li>\u2705Stationary; can be learned</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> \\[ w_{k+1} = w_k - \\alpha \\nabla J(w_k) \\] <p>\\(\\nabla J(w_k)\\) can be computed by:</p> <ul> <li>Ture Gradient Descent<ul> <li>need to calculate the expectation (collect all \\(N\\) samples), not practical\u274c</li> </ul> </li> </ul> \\[ \\begin{align} \\nabla J(w) &amp; = \\nabla \\mathbb{E} \\left[ (v_\\pi(S) - \\hat{v}(S, w))^2 \\right] \\\\[0pt] &amp; \\propto \\mathbb{E}[(v_\\pi(S)-\\hat{v}(S,w))\\,\\nabla\\hat{v}(S,w)] \\end{align} \\] <ul> <li>Stochastic Gradient Descent<ul> <li>Use only 1 sample \\(s_t\\) </li> <li>Online update, better generalisation\u2705</li> </ul> </li> </ul> \\[ \\nabla J(w) \\approx \\bigl(v_\\pi(s_t) - \\hat{v}(s_t,w_t)\\bigr) \\, \\nabla \\hat{v}(s_t,w_t) \\] <p>For remaining two terms:</p> <ul> <li>\\(\\nabla\\hat{v}(s_t,w_t)\\) \u2190 Feature Vectors (Linear)</li> <li>\\(v_\\pi(s_t)\\) \u2190 Value Function Approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#approximate-the-value-function","title":"Approximate the Value Function","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#feature-vectors","title":"Feature Vectors","text":"<p>Represent the state by a feature of vectors \\(\\mathbf{x}(s)\\), so that in linear case:</p> <ul> <li>State-value Gradient:</li> </ul> \\[ \\nabla\\hat{v}(s,w) = \\frac{\\partial \\hat v(s)}{\\partial w} = \\mathbf{x}(s) \\] <ul> <li>State-action-value Gradient:</li> </ul> \\[ \\nabla \\hat q(s,a,w) = \\mathbf{x}(s,a) \\] <ul> <li>Intuitive; easy to implement; high interpretability\u2705</li> <li>Difficult to select appropriate feature vectors\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#state-value-approximator","title":"State-value Approximator","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-monte-carlo","title":"Derive from Monte Carlo","text":"<p>Use \\(g_t\\) (discounted return starting from \\(s_t\\)) to approximate:</p> \\[ v_\\pi(s_t) = g_t \\] <ul> <li>converges to a local optimum, even for\u00a0non-linear approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-temporal-difference","title":"Derive from Temporal Difference","text":"<p>Use \\(r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) - \\hat{v}(s_t, w_t)\\) (TD Error) to update approximator at each step:</p> \\[ v_\\pi(s_t) = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) \\] <ul> <li>Linear TD(0) converges close to the global optimum</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-td","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ v_\\pi(s_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t)-\\hat{v}(s_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{v}(s_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#action-state-value-approximator","title":"Action-state-value Approximator","text":"<p>Generally change \\(v\\) to \\(q\\) :</p> \\[ w_{k+1} = w_k  - \\alpha\\, \\bigl(q_\\pi(s_t,a_t)-\\hat{q}(s_t,a_t,w_t)\\bigr)\\, \\nabla \\hat{q}(s_t,a_t,w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-monte-carlo_1","title":"Derive from Monte Carlo","text":"\\[ q_\\pi(s_t,a_t) = g_t \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-sarsa","title":"Derive from Sarsa","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\hat{q} (s_{t+1}, a_{t+1}, w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-td_1","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ q_\\pi(s_t, a_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat q(s_{t+1}, a_{t+1}, w_t)-\\hat{q}(s_t, a_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{q}(s_t,a_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-q-learning","title":"Derive from Q-learning","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}(s_{t+1})}\\hat{q} (s_{t+1}, a, w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#batch","title":"Batch","text":"<p>Restricted Environment: Only limited and static samples can algorithms learn from</p> \\[ \\mathcal{D} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_n, v^\\pi_n \\rangle\\} \\] <p>Distribution Shift: These samples may generate by a sub-optimal policy, so that: - Data may be out of distribution - Q-value may be over-estimated - Overfitting</p> <p>Potential Suitable Methods: - Function Approximation \u2192 generalisability - Off-policy (e.g. Q-Learning) \u2192 static sample set - Supervised Learning Methods (e.g. Neural Network) \u2192 static sample set</p>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#example-mc-td-in-a-batch","title":"Example - MC &amp; TD In a Batch","text":"<p>Sampled Episodes: 8 Samples (Limited)</p> \\[ \\begin{align} A \\to 0, B \\to 0 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 0 \\end{align} \\] <p>MC Update:</p> <ul> <li>\\(v(B) = 6/8 = 0.75\\)</li> <li>\\(v(A) = 0 / 1 = 0\\)</li> </ul> <p>TD Update:</p> <ul> <li>\\(v(B) \\to 0.75\\)</li> <li>\\(v(A) = 0 + v(B) \\to 0.75\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#batch-methods","title":"Batch Methods","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#least-squares-solution","title":"Least Squares Solution","text":"<p>By \\(minimising\\) SSE between approximate and true values:</p> \\[ LS(w_i) = \\mathbb{E} \\left[ (v^\\pi(S) - \\hat{v}(S, w))^2 \\right] \\] <ul> <li>True values can be approximated by Return (MC) or Temporal Difference (TD)</li> <li>Using all samples at once</li> <li>\\(O(n^3)\\) or \\(O(n^2)\\) by using \"Sherman-Morrison\"</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>By randomly sample a mini-batch of samples from \\(\\mathcal{D}\\):</p> \\[ \\mathcal{B} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_k, v^\\pi_k \\rangle\\} \\] <p>Then use gradient descent for updating:</p> \\[ w_{k+1} = w_k - \\alpha\\, \\frac{1}{m} \\sum_{\\langle s_i, v^\\pi_i \\rangle \\in \\mathcal{B}} \\bigl( v^\\pi_i(s_i) - \\hat{v}(s_i, w_i) \\bigr) \\nabla \\hat{v}(s_i, w_i) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#deep-q-learning","title":"Deep Q-Learning","text":"<p>By using Q-Learning + Neural Network (\\(minimise\\) MSE \\(\\mathbf{Loss}(\\cdot)\\)): </p> \\[ \\mathbf{Loss}(w_i) =\\underbrace{\\mathbb{E}_{(s,a,r,s')\\sim \\mathcal{D}_i}}_{\\text{sampled from}\\;D_i} \\Big[ \\big( \\underbrace{r+\\gamma \\max_{a'} Q(s',a';w_i^-)}_{\\text{TD target }} - \\underbrace{Q(s,a;w_i)}_{\\text{Q estimate}} \\big)^2 \\Big] \\] <ul> <li>Experience replay: <ul> <li>Randomly sample from non-i.i.d data \u2192 single sample/mini-batch</li> <li>\u2705For decorrelation (near-i.i.d)</li> </ul> </li> <li>Fixed Q-targets: <ul> <li>\\(w_i^-\\) will only be updated by \\(w_i\\) (lively updated) after running a while (like 1000 steps)</li> <li>\u2705Reduce variance</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html","title":"10 Policy Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#value-based-policy-based","title":"Value-based \u2192 Policy-based","text":"<p>In Value-based Algorithms:</p> <ul> <li>A policy is optimal if it can \\(maximise\\) every state value</li> </ul> \\[ \\pi^\\ast = \\arg\\max_\\pi v_\\pi(S) \\] <ul> <li>We store all \\(v_\\pi(S)\\) in a tabular based on different \\(\\pi\\) so that we can choose best policy from them.</li> </ul> <p>In Policy-based Algorithms:</p> <ul> <li>We construct a model of \\(\\pi\\) and optimise by tuning its parameters \\(\\theta\\).</li> </ul> \\[ \\pi^\\ast = \\pi(a\\mid S,\\theta) \\] <p>Outcomes of Policy-Based Methods</p> <p>Advantages</p> <ul> <li>High-level function approximation  </li> <li>Much more space-efficient (no huge tabular, just a function with features/variables)  \u2b50\ufe0f</li> <li>Better generalisation across large/continuous state spaces \u2705 </li> <li>Naturally supports stochastic policies \u2705</li> </ul> <p>Disadvantages</p> <ul> <li>May converge only to a local optimum  \u26a0\ufe0f</li> <li>Policy evaluation suffers from high variance \u26a0\ufe0f</li> <li>Monte-Carlo returns require full trajectory sampling \u2192 inefficient </li> <li>Difficult to use off-policy \u274c</li> <li>Updating \\(\\theta\\) changes the policy \u2192 changes the data distribution  </li> <li>Requires resampling under the new policy (no reuse of old data)</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#policy-gradient","title":"Policy Gradient","text":"<ol> <li>Use a object function to define optimal policies: \\(J(\\theta\\))</li> <li>Use gradient-based optimisation to search for optimal policies</li> </ol> \\[ \\theta_{t+1} = \\theta + \\alpha \\nabla J(\\theta_t) \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#objective-function","title":"Objective Function","text":"<p>Goal: \\(maximise\\) the expected long-term return under the policy:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\, G(\\tau) \\,\\right], \\] <p>where</p> <ul> <li>\\(\\tau\\) represents the trajectory generated by following such policy</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> <p>$$ \\theta_{k+1} = \\theta_k - \\alpha \\nabla J(\\theta_k) $$ \\(\\nabla J(\\theta_k)\\) can be computed by:</p> <ul> <li>Finite Difference<ul> <li>Run one episode on \\(\\theta_k\\) and get \\(J(\\theta_k)\\)</li> <li>Slightly change \\(\\theta_k\\) to \\(\\theta_k + \\epsilon \\vec{e}_k\\)<ul> <li>where \\(\\epsilon \\vec{e}_k\\) is a small noise (in vector form)</li> </ul> </li> <li>Run one episode again on \\(\\theta_k + \\epsilon \\vec{e}_k\\) and get \\(J(\\theta_k + \\epsilon \\vec{e}_k)\\)</li> <li>Estimate gradient by using finite difference:</li> </ul> </li> </ul> \\[ \\nabla J(\\theta) \\approx  \\frac{J(\\theta + \\epsilon e_i) - J(\\theta)}{\\epsilon} \\] <ul> <li>True Gradient Descent (The Gradient-ascent Algorithm)</li> </ul> \\[ \\begin{aligned} \\nabla J(\\theta) &amp;= \\sum_{\\tau} \\nabla P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(definition of expected return)} \\\\[4pt] &amp;= \\sum_{\\tau} P(\\tau \\mid \\theta) \\, \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(apply log-derivative trick)} \\\\[4pt] &amp;= \\mathbb{E}_{\\tau \\sim P(\\tau \\mid \\theta)} \\big[ \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) \\big] &amp;&amp; \\text{(convert sum to expectation)} \\\\[4pt] &amp;= \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla \\log \\pi_\\theta(A \\mid S) \\, G(\\tau) \\big] &amp;&amp; \\text{(expand trajectory likelihood)} \\end{aligned} \\] <ul> <li>Stochastic Gradient Descent</li> </ul> \\[ \\nabla J(\\theta) \\approx \\nabla \\log \\pi_\\theta (a_t \\mid s_t) \\, G(\\tau) \\] <p>where</p> <ul> <li>\\(\\nabla \\log \\pi_\\theta(a \\mid s)\\) is also called Score Function</li> </ul> <p>To compute Score Function, we can no longer use deterministic/non-linear/non-differentiable \\(\\{0,1\\}\\) to describe policy. </p> <p>So we introduce some soft policy strategies for making policy outputs are probabilistic.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#deterministic-policy-stochastic-policy","title":"Deterministic Policy\u00a0\u2192 Stochastic Policy","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#deterministicnear-deterministic-policy","title":"Deterministic/Near-deterministic Policy","text":"<p>Give the optimal action directly:</p> \\[ \\pi(a \\mid s) = \\mathbf{1}\\{\\, a = a^*(s) \\,\\} \\] <p>Or use soft policy for small explorations:</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#-greedy","title":"\u03b5-greedy","text":"<p>most time exploitation (\\(P = 1 - \\varepsilon\\)), little time exploration (\\(\\varepsilon\\)):</p> \\[ \\pi(a \\mid s) = \\begin{cases} 1 - \\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#upper-confidence-bound-ucb","title":"Upper Confidence Bound (UCB)","text":"<p>exploitation + exploration bonus:</p> \\[ a = \\arg\\max_a \\left( Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right) \\] <p>where</p> <ul> <li>\\(c\\) - importance of exploration (parameter)</li> <li>\\(\\ln{t}\\) - explore more when time goes</li> <li>\\(N(a)\\) - not try too many times on one same action</li> </ul> <p>All previous RL methods' outputs can be deterministic (The Optimum) which do not need differentiable probabilities.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#stochastic-policy","title":"Stochastic Policy","text":"<p>Give the probability of each action: $$ \\pi(a \\mid s) = P(a \\mid s) $$ - Ability of exploration; Robustness\u2705</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#softmax","title":"softmax","text":"\\[ \\pi(a \\mid s) = \\frac{\\exp\\!\\left(h_\\theta(s,a)\\right)}        {\\sum_{a' \\in \\mathcal{A}} \\exp\\!\\left(h_\\theta(s,a')\\right)} \\] <p>where \\(h(\\cdot)\\) is another feature function</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#gaussian","title":"Gaussian","text":"<p>Action space needs to be continuous:</p> \\[ \\pi(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\theta^2(s)}}   \\exp\\!\\left(     -\\frac{\\left(a-\\mu_\\theta(s)\\right)^2}{2\\sigma_\\theta^2(s)}   \\right). \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#solve-the-score-function","title":"Solve the Score Function","text":"<p>By applying Softmax:</p> \\[ \\begin{align} \\nabla \\log \\pi_\\theta(a \\mid s) &amp;= \\nabla h(a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\nabla h(a')] \\end{align} \\] <p>By applying Gaussian (assume \\(\\sigma\\) is fixed):</p> \\[ \\nabla \\log \\pi_\\theta(a \\mid s) = \\frac{a - \\mu_\\theta(s)}{\\sigma^2}\\nabla \\mu_\\theta(s) \\] <p>\u03b5-greedy CANNOT Be Used to solve the Score Function</p> <p>The score-function estimator requires a differentiable stochastic policy so that the log-probability gradient is well-defined.</p> <p>However, the \\(\\arg\\max Q(s,a)\\) step used in \u03b5-greedy is discrete and therefore\u00a0non-differentiable, violating this requirement.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#approximate-the-expected-return","title":"Approximate the Expected Return","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#average-state-value","title":"Average State Value","text":"<p>By \\(maximise\\) weighted average of all state values:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)v^{\\pi_\\theta}(s) \\] <p>where</p> <ul> <li>\\(d^{\\pi_\\theta}(s)\\) is the weight (importance) of each state</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#start-state-value","title":"Start State Value","text":"<p>In episodic environments, we may only care about value of start states:</p> \\[ J(\\theta)  = v^{\\pi_\\theta}(s_0) \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#average-one-step-reward","title":"Average One-step Reward","text":"<p>By \\(maximise\\) weighted average of all immediate rewards:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)  \\sum_{a \\in A} \\pi_\\theta(a \\mid s) \\, R^a_s \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Average State Value and Average One-step Reward are Equivalent:</p> \\[ \\bar{R}_\\pi = (1 - \\gamma) \\, \\bar{v}_\\pi \\] <p>In Gradient Form:</p> \\[ \\nabla \\bar{R}_\\pi \\simeq { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s,a) \\bigr]} \\] \\[ \\nabla \\bar{v}_\\pi = \\frac{1}{1 - \\gamma} \\nabla \\bar{R}_\\pi \\] \\[ \\nabla v_\\pi(s_0) = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s_0) \\; q_{\\pi_{\\theta}}(s_0,a) \\bigr]} \\] <p>In summary, gradient version of above methods can be simplified to just one formula:</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\] <p>Then, we can use our familiar value approximators (MC/TD) to solve \\(q_{\\pi_{\\theta}}(s, a)\\).</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#reinforce","title":"REINFORCE","text":"<p>Derive from MC, we can use return \\(v_t\\) as an unbiased sample of \\(q_{\\pi_{\\theta}}(s,a)\\):</p> \\[ q_{\\pi_{\\theta}}(s,a) = v_t \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#actor-critic","title":"Actor-Critic","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#qac","title":"QAC","text":"<p>Derive from TD, we can use a\u00a0value function approximator to estimate action-value function:</p> \\[ q_{\\pi_{\\theta}}(s,a) = q(s,a, w) \\] Actor Critic Role Policy Policy Evaluation (Value Function) Input Score State, Reward, Action Task Update \\(\\theta\\) by Policy Gradient Update \\(w\\) by Value Learning Output Action chosen by the policy Score (Value / Advantage)"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#a2c-advantage-actor-critic","title":"A2C - Advantage actor-critic","text":"<p>Introduce a baseline for reducing variance without changing the expectation<sup>1</sup>:</p> \\[ \\begin{align} \\nabla_{\\theta} J(\\theta)  &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\\\[4pt] &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\;  \\left[ q_{\\pi_{\\theta}}(s, a) - B(s) \\right] \\bigr]} \\end{align} \\] <ul> <li>A good (sub-optimal) baseline is the state value function \\(v_{\\pi_\\theta}(s)\\)</li> </ul> <p>By applying it, we can get the Advantage Function:</p> \\[ A_{\\pi_{\\theta}}(s,a) =q_{\\pi_{\\theta}}(s, a) - v_{\\pi_\\theta}(s) \\] <p>Then,</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; A_{\\pi_{\\theta}}(s,a) \\bigr]} \\] <p>Derive from TD, we can use TD error to unbiased estimate such function</p> \\[ \\delta_{\\pi_{\\theta}} = r + \\gamma v_{\\pi_{\\theta}}(s') - v_{\\pi_{\\theta}}(s) \\] \\[ \\begin{align} \\mathbb{E}_{\\pi_{\\theta}}[\\delta_{\\pi_{\\theta}} \\mid s,a] &amp;= \\mathbb{E}_{\\pi_{\\theta}}[r + \\gamma v_{\\pi_{\\theta}}(s') \\mid s,a] - v_{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= q^{\\pi_{\\theta}}(s,a) - v^{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= A^{\\pi_{\\theta}}(s,a) \\end{align} \\] <p>Finally, we actually just use the TD error to compute the policy gradient:</p> \\[ \\nabla_{\\theta} J(\\theta) = {\\mathbb{E}_{\\pi_{\\theta}} \\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\\; \\delta_{\\pi_{\\theta}} \\big]} \\] <ol> <li> <p>\u3010\u7b2c10\u8bfe-Actor-Critic\u65b9\u6cd5\uff08Part2-Advantage Actor-Critic (A2C)\uff09\u3010\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b66\u539f\u7406\u3011-\u54d4\u54e9\u54d4\u54e9\u3011 https://b23.tv/SlGApOj\u00a0\u21a9</p> </li> </ol>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html","title":"11 Integrating Learning and Planning","text":""},{"location":"Unimelb/25S2/AIP/11-Integrating.html#model-free-integrated-architectures","title":"Model-free \u2192 Integrated Architectures","text":"<p>Model-free</p> <ul> <li>No model</li> <li>Learn Value/Policy from experience</li> </ul> <p>Outcome</p> <ul> <li>Cannot look ahead\u274c</li> <li>Sample may be expensive\u26a0\ufe0f</li> <li>May fall into sub-optimal</li> </ul> <p>Integrated Architectures</p> <ul> <li>Learn Model from experience</li> <li>Learn and plan Value/Policy from real and simulated experience</li> </ul> <p>Outcome</p> <ul> <li>Sample is efficient\u2705 (directly from simulators)</li> <li>Avoid model error\u2705 (Beyond Model-based)</li> <li>Supervised Learning methods can be used in model training\u2705</li> <li>Uncertainty can be learned in model training\u2705</li> <li>Modelling is hard\u274c<ul> <li>Need environment to be model-learnable\u26a0\ufe0f</li> <li>Compounding model bias\u274c (experience is from model)</li> </ul> </li> <li>High computation cost\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#simulation-based-search","title":"Simulation-based Search","text":"<p>Model-based RL + Model-free RL backup</p> <ol> <li>Collect the current state \\(s_t\\) from real world</li> <li>For each \\(a \\in A\\), simulate \\(K\\) episodes from \\(s_t\\):<ul> <li>Use a model of MDP to look ahead</li> <li>Use Forward Search methods (e.g. BFS, DFS)\u00a0to select action</li> <li>Terminate after running \\(T-t\\) steps</li> </ul> </li> </ol> \\[ \\{ s_t^k, A_t^k, R_{t+1}^k, \\ldots, S_T^k \\}_{k=1}^K \\;\\sim\\; \\mathcal{M}_\\nu \\] <ol> <li>Take each episode as a experience for Model-free update</li> <li>Select the action in real world based on optimal policy from Model-free RL</li> </ol>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#simple-monte-carlo-search","title":"Simple Monte-Carlo Search","text":"<p>Derive from MC, update \\(q\\) value by mean return of all episodes, select the optimal action with \\(maximum\\) \\(q\\).</p>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#monte-carlo-tree-search","title":"Monte-Carlo Tree Search","text":"<ul> <li>Build a\u00a0search tree\u00a0containing visited states and actions</li> <li>Update \\(q\\) by mean return of episodes starting from each \\((s,a)\\)</li> </ul> \\[ q(s,a) = \\frac{1}{N(s,a)} \\sum_{k=1}^K \\sum_{u=t}^T \\mathbf{1}(S_u, A_u = s,a) \\, G_u \\] <p>MCTS is Off-policy</p> <ul> <li>Behaviour policy: rollout/simulation policy</li> <li>Target policy: greedy w.r.t search Q(s,a)</li> </ul>"},{"location":"Unimelb/25S2/IML/01-probability.html","title":"1 Probability","text":""},{"location":"Unimelb/25S2/IML/01-probability.html#probability-theory","title":"Probability Theory","text":"<p>Probability Theory is the theoretical and mathematical foundation of Machine Learning.</p> <p>What is Probability?</p> <ul> <li>A language used for describing and quantifying Uncertainty</li> <li>A framework of designing Machine Learning Models<ul> <li>Distribution</li> <li>Prediction</li> <li>Optimisation</li> <li>Generalisation</li> <li>Evaluation</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/IML/01-probability.html#sample-space","title":"Sample Space","text":"<p>Defines a set of events with mutually exclusive and exhaustive possibilities.</p> \\[ \\Omega = \\{ \\omega_1, \\omega_2, \\ldots \\} \\] <p>Exclusive: Different events won't happen at the same time.</p> \\[ P(\\omega_i \\cap \\omega_j) = 0, \\forall (\\omega_i, \\omega_j )\\in A \\] <p>Exhaustive: The events listed is exactly all of possible events.</p> \\[ P(\\Omega)=1 \\] Sample spaces of Coin &amp; Dice \\[ \\Omega_\\text{Coin} = \\{\\text{heads}, \\text{tails}\\} \\] \\[ \\Omega_\\text{D6} = \\{1,2,3,4,5,6\\} \\]"},{"location":"Unimelb/25S2/IML/01-probability.html#random-variable","title":"Random Variable","text":"<p>A function maps the outcome of a sample space to a value.</p> \\[ X: \\Omega \\to E \\] the Random Variable of Coin <p>Use \\(Y\\) to model a coin payoff for a successful bet on heads:</p> \\[ Y(\\omega) =  \\begin{cases} 1, &amp;\\text{if}\\; \\omega = \\text{heads} \\\\ 0, &amp;\\text{if}\\; \\omega = \\text{tails} \\end{cases} \\]"},{"location":"Unimelb/25S2/IML/01-probability.html#bayes-rule","title":"Bayes Rule","text":"\\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\] <p>where</p> <ul> <li>\\(P(A \\mid B)\\) - The belief of \\(A\\) while given knowledge of \\(B\\)</li> <li>\\(P(A)\\) - The belief of \\(A\\) without any knowledge</li> <li>\\(P(B \\mid A)\\) - The support \\(B\\) provides for \\(A\\)</li> <li>\\(P(B)\\) - Constant for normalising</li> </ul>"},{"location":"Unimelb/25S2/IML/01-probability.html#entropy","title":"Entropy","text":""},{"location":"Unimelb/25S2/IML/01-probability.html#information-ig","title":"Information (IG)","text":"<p>The information content of an outcome \\(x\\):</p> \\[ I(x) = -\\log_2 P(x) \\] <ul> <li>Certain Events \u2192 Zero Information carried</li> <li>Common Events \u2192 Poor Information carried</li> <li>Rare Events \u2192 High Information carried</li> <li>Impossible Events \u2192 Infinity Information carried</li> </ul> <p>Why is Information so denoted?</p> <p>Map a probability (range \\([0, 1]\\)) to a quantified score of Information (range \\([0, +\\infty)\\) ).</p>"},{"location":"Unimelb/25S2/IML/01-probability.html#entropy_1","title":"Entropy","text":"<p>The expected Information obtained from a random variable.</p> \\[ H(X) = \\mathbb{E}[I(X)] = -\\sum_x P(x)\\, \\log_2 P(x) \\] <p>where</p> <ul> <li>\\(X\\) - a random variable with possible values \\(x\\)</li> <li>\\(P(x)\\) - the probability (weight) of outcome \\(x\\)</li> <li> <p>\\(-\\log_2 P(x)\\) - the information carried by outcome \\(x\\)</p> </li> <li> <p>The value of \\(X\\) is 100% \\(a\\) \u2192 \\(H(X) = 0\\)</p> </li> <li>The value of \\(X\\) is almost certain \u2192 Low Entropy</li> <li>The value of \\(X\\) is highly uncertain \u2192 High Entropy</li> <li>The value of \\(X\\) is 50% \\(a\\) and 50% \\(b\\) \u2192 \\(H(X) = 1 \\text{bit}\\)</li> </ul> Low Entropy vs. High Entropy <p>\\(X = \\{a, b\\}\\)</p> <ul> <li>Low Entropy (\\(P(X = a) = 0.99; P(X = b) = 0.01\\)):</li> </ul> \\[ H(X) = - ((0.99 * \\log_2 0.99) + (0.01 * \\log_2 0.01) \\approx 0.0807 \\text{bits} \\] <ul> <li>High Entropy (\\(P(X = a) = 0.51; P(X = b) = 0.49\\)):</li> </ul> \\[ H(X) = - ((0.51 * \\log_2 0.51) + (0.49 * \\log_2 0.49) \\approx 0.9989 \\text{bits} \\]"},{"location":"Unimelb/25S2/IML/01-probability.html#joint-entropy","title":"Joint Entropy","text":"<p>The expected Information obtained from joint variables.</p> \\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 P(x, y) \\]"},{"location":"Unimelb/25S2/IML/01-probability.html#conditional-entropy","title":"Conditional Entropy","text":"<p>The expected Information obtained from one variable given knowledge of the other.</p> \\[ H(X \\mid Y) = H(X, Y) - H(Y) \\] Derivation via chain rule <p>Given  </p> \\[ P(x, y) = P(y)\\, P(x \\mid y), \\] <p>Substitute into joint entropy:</p> \\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 \\big[ P(y)\\, P(x \\mid y) \\big] \\] <p>Split the log:</p> \\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 P(y)   - \\sum_{x,y} P(x, y)\\, \\log_2 P(x \\mid y) \\] <p>Since \\(\\sum_x P(x,y) = P(y)\\):</p> \\[ H(X, Y) = -\\sum_y P(y)\\, \\log_2 P(y)   - \\sum_{y} P(y) \\sum_x P(x \\mid y)\\, \\log_2 P(x \\mid y) \\] <p>Thus:</p> \\[ H(X, Y) = H(Y) + H(X \\mid Y) \\]"},{"location":"Unimelb/25S2/IML/01-probability.html#mutual-information","title":"Mutual Information","text":"<p>If we get the knowledge of \\(Y\\) , how much we can reduces the Uncertainty about \\(X\\):</p> \\[ \\begin{align} IG(X; Y) &amp;= H(X) - H(X \\mid Y) \\\\[4pt]          &amp;= H(Y) - H(Y \\mid X) \\end{align} \\] <p>Mutual information can be expressed using joint probabilities:</p> \\[ IG(X; Y) = \\sum_{x,y} P(x,y)\\, \\log_2 \\frac{P(x,y)}{P(x)\\, P(y)} \\] Log Ratio \\[ r = \\log_2 \\frac{P(x,y)}{P(x)P(y)} \\] <ul> <li>\\(r = 0\\) \u2192 occurs as expected under independence </li> <li>\\(r &gt; 0\\) \u2192 co-occur more than expected \u2192 positive association </li> <li>\\(r &lt; 0\\) \u2192 co-occur less than expected \u2192 negative association</li> </ul>"},{"location":"Unimelb/25S2/IML/01-probability.html#probability-theory-data-distribution","title":"Probability Theory \u2192 Data Distribution","text":"<p>In Machine Learning, we treat data as samples drawn from an unknown underlying probability distribution.</p> \\[ x_1, x_2, \\ldots, x_n \\sim P(X) \\] <p>where</p> <ul> <li>Data points are samples of a random variable \\(X\\).</li> <li>Its behaviour is described by the distribution \\(P(X)\\).</li> <li>The distribution \\(P(X)\\) is unknown.</li> <li> <ul> <li>Our dataset is a finite snapshot of the true underlying distribution.</li> </ul> </li> </ul> <p>The goal of Machine Learning is to recover information about such distribution, which is also called generalisation.</p> <ul> <li>Generalisation: learning patterns from the underlying data distribution.</li> </ul>"},{"location":"Unimelb/25S2/IML/01-probability.html#machine-learning","title":"Machine Learning","text":"<p>generalisation can be categorised into different paradigms depending on the available sources and the goal of the problem.</p> Learning Paradigm Input Mathematical Objective Practical Objective Unsupervised Learning Only data samples (\\(X\\)) without labels Directly learn the distribution (\\(P(X)\\)) Discover the structures Supervised Learning Labeled pairs (\\((X, Y)\\)) Learn the conditional distribution (\\(P(Y \\mid X)\\)) Solve the classification and regression problem Weakly-Supervised Learning Incomplete/noisy labels Learn an approximate \\(P(Y \\mid X)\\) using imperfect or indirect supervision Reduce labeling cost; learn from noisy/cheap labels Generative Models Labelled or Unlabelled Same output as the generative model Generate new samples Ensemble Learning Diverse models trained on the same data Same output as the meta model. Increase accuracy, stability, robustness"},{"location":"Unimelb/25S2/IML/01-probability.html#models","title":"Models","text":"<p>Supervised Learning</p> <ul> <li>K-Nearest Neighbours</li> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>Decision Tree</li> <li>Support Vector Machine</li> <li>Perceptron</li> <li>Multilayer Perceptron (Neural Networks)</li> </ul> <p>Unsupervised Learning</p> <ul> <li>Clustering</li> <li>K-Mean Clustering</li> <li>Hierarchical Clustering</li> <li>Agglomerative Clustering</li> <li>Divisive Clustering</li> </ul> <p>Weak-supervised Learning</p> <ul> <li>Self-training<ul> <li>Self-supervised Learning</li> <li>Data argumentation</li> </ul> </li> <li>Active Learning</li> </ul> <p>Ensemble Learning</p> <ul> <li>Stacking</li> <li>Bagging (RF)</li> <li>Boosting (AdaBoost)</li> </ul>"},{"location":"Unimelb/25S2/IML/02-KNN.html","title":"2 K-Nearest Neighbours","text":""},{"location":"Unimelb/25S2/IML/02-KNN.html#lazy-learning","title":"Lazy Learning","text":"<ul> <li> <p>Lazy: No training; Compute at query time</p> </li> <li> <p>Store the training data; No Modelling &amp; Training</p> </li> <li>Computing when only predicting</li> </ul> <p>Pros &amp; Cons</p> <ul> <li>Effective</li> <li>Long predicting time (need to traverse the data)</li> </ul>"},{"location":"Unimelb/25S2/IML/02-KNN.html#measuring","title":"Measuring","text":""},{"location":"Unimelb/25S2/IML/02-KNN.html#nominal-features","title":"Nominal Features","text":"<ul> <li>Hamming Distance \\(N(\\text{mismatching features})\\)</li> <li>Simple Matching Distance \\(1 - \\frac{N(\\text{matching features})}{N(\\text{all features})}\\)</li> <li>Jaccard Distance \\(1-\\frac{|A \\cap B|}{|A \\cup B|}\\)</li> </ul>"},{"location":"Unimelb/25S2/IML/02-KNN.html#numerical-feature-vectors","title":"Numerical Feature Vectors","text":"<ul> <li>Manhattan Distance \\(\\sum(A_i - B_i)\\)</li> <li>Euclidean Distance \\(\\sqrt{\\sum(A_i - B_i)^2}\\)</li> <li>Cosine Distance \\(1 - \\cos(a,b) = 1 - \\frac{a \\cdot b}{|a||b|}\\)</li> </ul> When is Cosine Distance commonly used? <ul> <li>Normalised a\u00b7b = comparison only directions, no lengths</li> <li>Useful when compare long vs short documents / high vs low resolution images</li> </ul>"},{"location":"Unimelb/25S2/IML/02-KNN.html#ordinal-feature-vectors","title":"Ordinal Feature Vectors","text":"<ul> <li>Normalised Ranks map ranks evenly spaced in [0,1] then use numerical comparisons</li> </ul> <p>Measure Weight </p> <ul> <li>Majority Voting choose the most common label (all\u2019s weight = 1)</li> <li>Inverse Distance Weighting weight = 1/(distance + \u03f5)</li> <li> <p>Inverse Linear Distance weight = ( max - each distance) / ( max - min distance) among neighbours\u00a0</p> </li> <li> <p>Normalised d(each) between [0, 1]</p> </li> </ul> <p>Select K (The number of neighbours)</p> <p>Small K</p> <ul> <li>Jagged decision boundary</li> <li>Capturing noise more easily</li> <li>Lower classifier performance</li> </ul> <p>Large K</p> <ul> <li>Smooth decision boundary</li> <li>Risk of grouping together unrelated classes</li> <li>Lower classifier performance as well if K is too big like even K = N)</li> </ul> <p>KNN Algorithm Pros and Cons</p> <p>Pros</p> <ul> <li>Intuitive and simple</li> <li>No need assumptions</li> <li>Supports both classification and regression</li> <li>No pre-training needed lazy</li> </ul> <p>Cons</p> <ul> <li>Expensive large data sets</li> <li>How to decide sub-methods is still a problem</li> </ul> <p>Vs. Eager Learning</p> <ul> <li>Train a model using labelled training instances</li> <li>The model will generalise from seen data to unseen data</li> <li>Using the model to predict labels for test instances</li> </ul>"}]}