{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Drava's Study Notes","text":""},{"location":"#master-of-information-technology-in-the-university-of-melbourne","title":"\ud83c\udf93 Master of Information Technology in the University of Melbourne","text":""},{"location":"#2025-semester-2","title":"\ud83d\uddd3\ufe0f 2025 Semester 2","text":""},{"location":"#ai-planning-for-autonomy-comp90054","title":"\u270f\ufe0f AI Planning for Autonomy - COMP90054","text":""},{"location":"#other","title":"\ud83d\udcbb Other","text":""},{"location":"#reference","title":"\ud83d\udcda Reference","text":""},{"location":"Unimelb/25S2/AIP/WEEK-1-General-Basics/","title":"WEEK 1 General Basics","text":"<p>AIP - Artificial Intelligence Planning for Autonomy</p> <p>Agents - Sensors/Precepts \u2192 Actuators/Actions</p> <p>Planning - How to combine a set of functions to achieve goals: From every initial state to their optimal goals - State Space - Finite and Discrete - Every function costs/is deterministic - A solution is a sequence of applicable actions that maps s\u2080 into SG - Optimal if it minimises sum of action costs (lowest cost)</p> <p>SAT - Satisfiability CSP - Constraint Satisfaction Problem</p>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/","title":"WEEK 2 Search Methods","text":""},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#search-space","title":"Search Space","text":"<ul> <li>A set of search states</li> <li>Forward Search (Progression)<ul> <li>search space = world space (represents the current real world)</li> </ul> </li> <li>Backward Search (Regression)<ul> <li>search space = a sets of world spaces (represents all predications of sub-goals)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#example","title":"EXAMPLE","text":"<p>In a robot delivery scene: <pre><code>world_state_of_a_robot = {\n\u00a0 \u00a0 \"position\": (3, 5),\n\u00a0 \u00a0 \"direction\": \"North\",\n\u00a0 \u00a0 \"battery\": 80,\n\u00a0 \u00a0 \"carrying_package\": True\n}\n</code></pre> Progression: \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5c31\u662f\u673a\u5668\u4eba\u5728\u67d0\u4e2a\u4f4d\u7f6e\u3001\u671d\u5411\u3001\u80fd\u91cf\u3001\u8d1f\u8f7d\u7684\u7ec4\u5408\u72b6\u6001\u3002 Regression:\u00a0 \u641c\u7d22\u6811\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\u5171\u540c\u7ec4\u6210\u201c\u6211\u4eec\u60f3\u8981\u6240\u6709\u6ee1\u8db3\u4e00\u4e2a\u6761\u4ef6\u7684\u4e16\u754c\u72b6\u6001\u96c6\u5408\u201d\u3002 <pre><code>subgoals = {\"position\": (5, 8), \"carrying_package\": True}\n</code></pre> Common Functions: </p> <ul> <li>\\(s\\) = search states</li> <li>\\(\\mathrm{is}\\_\\mathrm{start}(s)\\) return if the state is the start state of the search space</li> <li>\\(\\mathrm{is}\\_\\mathrm{target}(s)\\) mark if the state is the goal state of the search space</li> <li>\\(\\mathrm{succ}(s)\\) return a list of successors/next states of \\(s\\)</li> <li>Search nodes:\u00a0<ul> <li>\\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(\\mathrm{parent}(\\sigma)\\) where \\(\\sigma\\) was reached</li> <li>\\(\\mathrm{action}(\\sigma)\\) leads from \\(\\mathrm{state}(\\mathrm{parent}(\\sigma))\\) to \\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(g(\\sigma)\\) denotes cost of path from the root to \\(\\sigma\\)</li> <li>The root\u2019s \\(\\mathrm{parent}(\\cdot)\\) and \\(\\mathrm{action}(\\cdot)\\) are undefined</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#search-methods","title":"Search Methods","text":""},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#blind-search-vs-informed-search","title":"Blind Search vs. Informed Search","text":"<ul> <li>Blind search not require any input beyond the problem</li> <li>No additional work but rarely effective</li> <li>Informed search requires function \\(h(x)\\) mapping states to estimates their goal distance</li> <li>Effective but lots of work to construct \\(h(x)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#blind-systematic-search-algorithms","title":"Blind Systematic Search Algorithms","text":"<ul> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>Iterative Deepening Search<ul> <li>Do DLS(DFS with depth limited) with continuously increasing depth limited by 1.</li> </ul> </li> <li>Blind Search: Completeness 100%, but poor efficient when doing hard work</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#heuristic-functions","title":"Heuristic Functions","text":"<ul> <li>\\(h(n)\\) - Estimated remaining cost (from current state to goal state)</li> <li>\\(h^*(n)\\) - Real remaining cost</li> <li>proficiency of \\(h(n)\\) <ul> <li>\\(h = h^{\\ast}\\) perfectly informed, \\(h(n) = h^{\\ast}(n) - \\textbf{optimal } A^{\\ast}\\)</li> <li>\\(h = 0\\) no information at all - uniform cost search</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#properties","title":"Properties","text":"<ul> <li>Safe \\(h(n) = \\infty\\) iff \\(h^{\\ast}(n) = \\infty\\)</li> <li>Goal-aware \\(h(\\text{goal}) = 0\\)</li> <li>Admissible \\(h(n) \\le h^*(n)\\)</li> <li>Consistent \\(h(n) \\le c(n,n\u2019) + h(n\u2019)\\) for all possible \\(c(n, n\u2019)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#relations-of-properties","title":"Relations of properties","text":"<ul> <li>Consistent &amp; Goal-aware \\(\\to\\) Admissible</li> <li>Admissible \\(\\to\\) Safe &amp; Goal-aware</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#informed-systematic-search-algorithms","title":"Informed Systematic Search Algorithms","text":""},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#greedy-best-first-search-with-duplicate-detection","title":"Greedy Best-First Search (with duplicate detection)","text":"<pre><code>def greedy_BFS:\n\u00a0 \u00a0 frontier = priority queue ordered by h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <ul> <li>Use priority queue to sort \\(h(n)\\) of each node in ascending order<ul> <li>If \\(h(n) = 0\\), it becomes what fully depends\u00a0on how\u00a0we\u00a0break\u00a0ties</li> </ul> </li> <li>Completeness\u2705for safe heuristics; Optimal\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#a-with-duplicate-detection-and-re-opening","title":"A* (with duplicate detection and re-opening)","text":"<pre><code>def a_star:\n\u00a0 \u00a0 frontier = priority queue ordered by f(n) = g(n) + h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u270f\ufe0f\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(cost(new_path) + h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <ul> <li>Only difference between greedy and A* \\(h(n) \\rightarrow f(n) = g(n) + h(n)\\)</li> <li>Re-opening if a node is closed but we find a better cost(n), then we can re-visit and extend this node</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#weighted-a","title":"Weighted A*","text":"\\[ f_W(n) = g(n) + W \\cdot h(n) \\] Weight Algorithm \\(W \\to 0\\) Digkstra Algorithm \\(W \\to 1\\) A* \\(W \\to \\infty\\) GBFS \\(W &gt; 1\\) Bounded sub-optimal A* <ul> <li>If \\(h\\) is admissible, \\(f_W(n) \\le W \\cdot h(n)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#hill-climbing","title":"Hill-Climbing","text":"<p><pre><code>def hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break \u00a0 \u00a0\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Can only find local maxima - Make sense only if \\(h(n) &gt; 0\\) for all non-goal states</p>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#enforced-hill-climbing","title":"Enforced Hill-Climbing","text":"<p><pre><code>def enforced_hill_climbing:\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 current = start\n\u00a0 \u00a0 while h(n) &gt; 0:\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current))\n\u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best)\n\u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subs = n for n in neighbours_of(current) and not in explored\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 best_sub = argmin_h(subs, goal=current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if best_sub and h(best_sub) &lt; h(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path -= action(parent, current)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(parent, best_sub)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = best_sub\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 return path\n</code></pre> - Local Search: Do small range BFS when find local optimal - Can across small gap between local best and global best</p>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#iterative-deepening-a","title":"Iterative Deepening \\(A^*\\)","text":"<ul> <li>IDS + \\(A^*\\): Use f(n) instead of depth to limit IDS<ul> <li>In First Search:\u00a0 f(n) = f(start) = 0 + h(start)</li> <li>Following Searches: f(n) = min_out_of_bound_excess <pre><code>def ida_star:\n\u00a0 \u00a0 bound = f(start)\n\u00a0 \u00a0 while True:\n\u00a0 \u00a0 \u00a0 \u00a0 t = ids(start, bound)\n\u00a0 \u00a0 \u00a0 \u00a0 if t == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return solution\n\u00a0 \u00a0 \u00a0 \u00a0 if t == infinity:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable\n\u00a0 \u00a0 \u00a0 \u00a0 bound = t\n</code></pre></li> </ul> </li> <li>Dealing with one of \\(A^*\\)\u2019s problem: large queue/closed_set</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#evaluation-of-search-methods","title":"Evaluation of Search Methods","text":""},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#guarantees","title":"Guarantees","text":"<ul> <li>Completeness sure to find a solution if there is one</li> <li>Optimality solutions sure be optimal</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#complexity","title":"Complexity","text":"<ul> <li>Time/Space (Measured in generated states/states cost)</li> <li>Typical state space features governing complexity<ul> <li>Branching factor \\(b\\) how many successors</li> <li>Goal depth \\(d\\) number of actions to reach shallowest goal state</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-2-Search-Methods/#summary","title":"Summary","text":"DFS BrFS IDS A* HC IDA* Complete \u274c \u2705 \u2705 \u2705 \u274c \u2705 Optimal \u274c \u2705* \u2705 \u2705* \u274c \u2705* Time \\(\\infty\\) \\(b^d\\) \\(b^d\\) \\(b^d\\) \\(\\infty\\) \\(b^d\\) Space \\(b \\cdot d\\) \\(b^d\\) \\(b \\cdot d\\) \\(b^d\\) \\(b\\) \\(b \\cdot d\\) <ul> <li>\\(b\\) - Branching Factor: sum of number of child nodes of each node</li> <li>\\(d\\) - Solution Depth: \\(minimum\\) depth among all goal nodes</li> <li>DFS cannot handle cyclic graph</li> <li>BFS is optimal only when uniform costs are applied </li> <li>\\(A^*\\) / Iterative Deepening \\(A^*\\) is optimal only when \\(h\\) is admissible (\\(h \\le h^*\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/","title":"WEEK 3 Planning Methods","text":""},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#problem-solving","title":"Problem Solving","text":""},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#autonomous-behaviour-in-ai","title":"Autonomous Behaviour in AI","text":"<ul> <li>Programming-based</li> <li>Learning-Based</li> <li>Model-Based</li> <li>Approaches not orthogonal</li> <li>Different models yield different types of controllers \u6267\u884c\u52a8\u4f5c\u7684\u7b56\u7565\u6a21\u5757</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#3-level-solver-ambitions","title":"3-level Solver Ambitions","text":"<ul> <li>Ambition: Write one program to solve all classical search problems<ul> <li>Ambition 1.0 (Problem Solving) Write one program to solve a problem</li> <li>Ambition 2.0 (Problem Generation) Write one program to solve a large class of problems</li> <li>Ambition 3.0 (Meta Problem Solving) Write one program to solve a large class of problems effectively</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#solving-approaches","title":"Solving Approaches","text":""},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#programming-based-approach","title":"Programming-Based Approach","text":"<ul> <li>Pro: Domain-knowledge easy to express</li> <li>Con: Less Flexible (Can\u2019t deal with situations not anticipated by programmer)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#learning-based-approach","title":"Learning-Based Approach","text":"<ul> <li>Unsupervised/Reinforced Learning: Use reward &amp; penalise</li> <li>Supervised: Use labelled data</li> <li>Evolutionary: Use original controllers to mutate and recombine to build better controller</li> <li>Pros Dose not require much knowledge in principle\u00a0</li> <li>Cons Slower; Hard to know which features to learn\u00a0</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#model-based-approach-planning","title":"Model-Based Approach (Planning)","text":"<ul> <li>One model for one specific problem </li> <li>Pros Powerful; Quick; Flexible; Clear; Intelligent; Domain-independent</li> <li>Cons Need a model (sometimes very hard); Efficiency loss</li> <li>Always need trade-off between \u201cAutomatic and general\u201d vs. \u201cManual work but effective\u201d</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#planning-models","title":"Planning Models","text":""},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#classical-planning-model","title":"Classical Planning Model","text":"<ul> <li>Classical Planning Model = Basic State Model</li> <li> <p>Assumptions</p> <ul> <li>Deterministic</li> <li>Fully Observable</li> <li>Static World</li> <li>Discrete Time &amp; Finite Actions</li> <li>Uniform Cost</li> </ul> </li> <li> <p>Components </p> \\[ M = \\langle S, A, T, I, G \\rangle \\] <ul> <li>\\(S\\) - State spaces </li> <li>\\(A(s)\\) - Actions applicable for \\(s \\in S\\)</li> <li>\\(T\\) - Deterministic Transition Function: \\(s\u2019 = T(A(s), s)\\) shows one successor \\(s\u2019\\) of \\(s\\)</li> <li>\\(I\\) - Initial state</li> <li>\\(G\\) - Goal states</li> <li>Uniform action costs \\(c(A(s), s) = 1\\)</li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>A seq of actions map \\(s_0\\) into \\(g\\)</li> <li>Optimal if total cost to goal is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#conformant-planning-model","title":"Conformant Planning Model","text":"<ul> <li>\\(\\approx\\) Brute Force</li> <li>Components (Diff with classic)<ul> <li>Initial State \u2192 A set of possible initial states</li> <li>Deterministic Transition Function \u2192 Non-deterministic</li> </ul> </li> <li>Outcomes<ul> <li>No Observation - No new info; must pre-planning</li> <li>Goal Guarantee</li> <li>Rarely optimal<ul> <li>Must map any possible \\(s_0\\) to \\(g\\), too much unnecessary work</li> <li>Sensitive to Worst/Special case </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#markov-decision-processes-mdps","title":"Markov Decision Processes (MDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li> <p>Markov Property: the next state only depends on the current state and actions.</p> \\[ P(s_{t+1} \\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots) = P(s_{t+1} \\mid s_t,a_t)  \\] </li> <li> <p>Fully Observable</p> </li> <li>Discrete Time &amp; Finite Actions</li> <li>Fixed Transition Probabilities</li> </ul> </li> <li> <p>Components (Diff with conformant)     $$     M = \\langle S,A,T,R,\\gamma \\rangle     $$</p> <ul> <li>Introduce the Transition Probability Function:     $$     T(s, a, s') = P(s' \\mid s, a)     $$</li> <li>Introduce Reward Function \\(R\\) and Discount Factor \\(\\gamma\\):<ul> <li>For estimating the value of each state</li> </ul> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map states to actions</li> <li>Optimal if total expected cost to goal is \\(minimum\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-3-Planning-Methods/#partially-observable-mdps-pomdps","title":"Partially Observable MDPs (POMDPs)","text":"<ul> <li> <p>Assumption</p> <ul> <li>Markov Property</li> <li>Limited Observable</li> </ul> </li> <li> <p>Components (Diff with MDPs)</p> <ul> <li> <p>Introduce Sensor Model (based on Probability Distribution)</p> \\[ \\mathrm{b}(s_t) = P(s_t \\mid \\text{historical actions and observations}) \\] </li> <li> <p>Initial/Goal States \u2192 Belief States \\(\\mathrm{b}(s_0)\\) and \\(\\mathrm{b}(g)\\)</p> </li> </ul> </li> <li> <p>Outcomes</p> <ul> <li>Map belief states into actions</li> <li>Optimal if total expected cost from \\(\\mathrm{b}(s_0)\\) to \\(\\mathrm{b}(g)\\) is minimum</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/","title":"WEEK 4 Complexity","text":""},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#savitchs-theorem","title":"Savitch\u2019s Theorem","text":"<ul> <li>NPSPACE = PSPACE<ul> <li>Non-determinism won't make solvers more powerful.</li> </ul> </li> <li>PSPACE-complete<ul> <li>The problem is at least hard as any Polynomial time problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#goals-definition","title":"Goals Definition","text":"<ul> <li>To make a Satisficing planning:<ul> <li>\\(\\mathrm{PlanEx}(P) =\\) <code>TRUE</code> - Existence of a plan for problem \\(P\\).</li> </ul> </li> <li>To make a Optimal planning:<ul> <li>\\(\\mathrm{PlanLen}(P) \\le B\\)  - Existence of a plan which length is at most \\(B\\).</li> </ul> </li> <li>\\(\\mathrm{PlanEx}\\) and \\(\\mathrm{PlanLen}\\) is PSPACE-complete<ul> <li>In practice, optimal planning is almost never easy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#strips-planning-problem","title":"STRIPS planning problem","text":""},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#strips-stanford-research-institute-problem-solver","title":"STRIPS = STanford Research Institute Problem Solver","text":"<ul> <li>Design Goals:<ul> <li>Specification - concise model description</li> <li>Computation - reveal useful information/structure for heuristics</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#components","title":"Components","text":"\\[ P = \\langle P,A,I,G \\rangle \\] <ul> <li>\\(P\\) - All Possible Predicates</li> <li>\\(A\\) - Actions</li> <li>\\(I\\) - Initial States: Predicates initially to be <code>TRUE</code></li> <li>\\(G\\) - Goal Conditions: Predicates Finally need to be <code>TRUE</code></li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#action","title":"Action","text":"\\[ a \\in A, a = \\langle Pre(a), Add(a), Del(a) \\rangle \\] <ul> <li>\\(Pre(a)\\) - Preconditions need to be <code>TRUE</code> before executing the action</li> <li>\\(Add(a)\\) - Add Effects: Preconditions set to be<code>TRUE</code> after execution</li> <li>\\(Del(a)\\) - Delete Effects: Preconditions set to be <code>FALSE</code> after execution</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#result-of-applying-actions","title":"Result of applying actions","text":"\\[ Result(s, a) = (s - Del(a)) \\cup Add(a) \\] <ul> <li>which could also be seemed as a transition function</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#goal","title":"Goal","text":"<ul> <li>Find a list of actions \\([a_1,a_2,a_3,\\ldots]\\) which satisfies:     $$     Result(Result(\\ldots Result(I,a_1\u200b) \\ldots ,a_n\u22121\u200b),a_n\u200b) \\models G     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-4-Complexity/#properties","title":"Properties","text":"<ul> <li>It turns a \"solving\" problem into a \"search\" problem</li> <li>Still PSPACE-complete, but \"approximation\" is able to be applied<ul> <li>Explicit Search \u663e\u5f0f\u641c\u7d22<ul> <li>e.g. Blind/Heuristic search</li> <li>Not effective</li> </ul> </li> <li>Near Decomposition \u8fd1\u4f3c\u5206\u89e3<ul> <li>e.g. Relaxation: similar to Decrease/Divide &amp; Conquer</li> <li>Maybe fail</li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/","title":"WEEK 5 Relaxation","text":""},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#relaxation","title":"Relaxation","text":"<ul> <li>A method to compute heuristic functions \\(h(n)\\)<ul> <li>Defines a transformation for simplifying the STRIPS problem</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#components","title":"Components","text":"\\[ \\mathcal{R} = (\\mathcal{P'}, r, h'^*) \\] <ul> <li>\\(\\mathcal{P}'\\) - the class of the simpler problem </li> <li>\\(r\\) - transformer turning the original problem into a simplified one</li> <li>\\(h'^*(n)\\) - Perfect heuristic function of the simplified problem </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#properties","title":"Properties","text":"<p>The Relaxation is - Native - if \\(\\mathcal{P'} \\subseteq \\mathcal{P}\\) and  \\(h'^*(n) = h(n)\\) - Efficiently Constructible - if a polynomial \\(r\\) exists - Efficiently Computable - if a polynomial \\(h\u2019(n)\\) exists</p>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#examples","title":"Examples","text":"<ol> <li>Route-Finding<ul> <li>Relaxation<ul> <li>Route-find as a bird (ignoring the road)</li> </ul> </li> <li>Outcome<ul> <li>Road Distance \u2192 Manhattan distance, Euclidean distance, etc.</li> </ul> </li> <li>Native\u274c Efficiently constructible\u2705 Efficiently computable\u2705</li> </ul> </li> <li>Goal-Counting<ul> <li>Relaxation<ul> <li>Assume we can achieve each goal directly</li> </ul> </li> <li>Outcome<ul> <li>Tasks \u2190 No precondition and delete</li> </ul> </li> <li>Admissible but still NP-hard</li> <li>Native\u2705 Efficiently constructible\u2705 Efficiently computable\u274c</li> </ul> </li> </ol>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#still-inefficiency","title":"Still Inefficiency?","text":"<ul> <li>Approximate \\(h\u2019^*\\)</li> <li>Re-design \\(h\u2019^*\\) in a way so that it will typically be feasible<ul> <li>Critical path heuristics</li> <li>Delete relaxation \u2190 wide-spread for satisficing planning</li> <li>Abstractions</li> <li>Landmarks</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#delete-relaxation","title":"Delete Relaxation","text":"<ul> <li>Apply \\(Del(a) = \\emptyset, \\forall a \\in A\\)<ul> <li>which makes \"once\"  <code>TRUE</code> predications remain <code>TRUE</code> \"forever\"</li> </ul> </li> <li>\\(a^+\\) - Actions after delete relaxation</li> <li>Optimal Delete Relaxation is Admissible \\(h^+(n) \\le h^*(n)\\)<ul> <li>(but find an optimal solution is still NP-hard)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#dominance","title":"Dominance","text":"<ul> <li>\\(s' \\supseteq s\\) - \\(s'\\) dominate \\(s\\)<ul> <li>If \\(s\\) is a goal state, then \\(s'\\) must be a goal state.</li> <li>If \\(a^+\\) is applicable in \\(s\\), then \\(a^+\\) must be applicable in \\(s'\\).</li> </ul> </li> <li>\\(Result(s, a^+)\\) dominates both \\(s\\) and \\(Result(s, a)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#optimal-delete-relaxation-heuristic-h","title":"Optimal Delete Relaxation Heuristic (\\(h^+\\))","text":"<pre><code>def h_plus(s, G, A):\n    if G in s:\n        return 0\n\n    open = [s]\n    cost[s] = 0\n    cost[others] = inf\n    best = inf\n\n    while open:\n        cur_state = pop(open)\n        if G in cur_state:\n            best = min(best, cost[cur_state])\n            continue\n\n        for a in A:\n            if pre(a) in current:\n                next_state = current + add(a)\n                if cost[next_state] &gt; cost[cur_state] + 1:\n                    cost[next_state] = cost[current] + 1\n                    push(open, next_state)\n\n    if best == inf:\n        return unsolvable\n    else:\n        return best\n</code></pre> <ul> <li>Native relaxation\u2705</li> <li>Safe\u2705, goal-aware\u2705, admissible guarantee\u2705</li> <li>Efficiently constructible\u2705 Polytime</li> <li>Efficiently computable\u274c<ul> <li>\\(\\mathrm{PlanOpt^+} = \\sum h^+\\) still NP-hard</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<ul> <li>Application based on \\(h^+\\)</li> <li>Distance estimate during the iterations in using shortest-distance algorithm</li> <li>Initially set start point \\(= 0\\); others \\(= \\infty\\).</li> <li>Relax every edge (One iteration = all neighbour unexplored edges from one point)</li> <li>Use a table to note estimate length of shortest path from start to current point</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#example","title":"Example","text":"<ul> <li>A\u2192B(+4); A\u2192C(+5); B\u2192C(-2)</li> </ul> Vertex Initialisation Round 1 Round 2 A 0 0 0 B \u221e 4 4 C \u221e 5 2 (= 4 - 2)"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#h-approximation-methods","title":"\\(h^+\\)-Approximation Methods","text":""},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#plan-existence-checking","title":"Plan Existence Checking","text":"<ul> <li> <p>based on Fast Forward Heuristic (\\(h_{FF}\\))</p> <ul> <li>Expand the current state based on BrFS <pre><code>def is_plan_existence(s, G, A):\n    if G in s:\n        return TRUE\n\n    reached = s\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return FALSE\n        reached = new_facts\n\n    return TRUE\n</code></pre></li> </ul> </li> <li> <p>Sound, complete, Terminates in polytime\u2705</p> <ul> <li>\\(\\mathrm{PlanEx}^+\\) now becomes a polytime problem</li> </ul> </li> <li>Safe\u2705, goal-aware\u2705, admissible\u274c (Existence Check Only, usually far from optimal)</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#max-heuristic-hmax","title":"Max Heuristic (\\(h^{max}\\))","text":"<pre><code>def h_max(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n                # a is reachable\n                new_cost = 1 + max(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n    return max(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime<ul> <li>However, sometimes maybe too optimistic\u26a0\ufe0f</li> </ul> </li> <li>Optimistic Estimation: admissible\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#additive-heuristic-hadd","title":"Additive Heuristic (\\(h^{add}\\))","text":"<pre><code>def h_sum(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n\u270f\ufe0f              new_cost = 1 + sum(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n\u270f\ufe0f  return sum(cost[g] for g in G)\n</code></pre> <ul> <li>Efficient Computable\u2705 Polytime</li> <li>Pessimistic Estimation: admissible\u274c informative\u2705 </li> <li>Overcounts by ignoring positive interactions, i.e. shared sub-plans<ul> <li>May result in \\(dramatic\\) over-estimates of \\(h^*\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#relaxed-plans","title":"Relaxed Plans","text":""},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#best-supporter-function-bsp","title":"Best Supporter Function \\(bs(p)\\)","text":"<ul> <li>Input: A predication \\(p\\)</li> <li>Output: The \\(cheapest\\) action \\(a\\) which make this predication <code>TRUE</code></li> <li>Prerequisites<ul> <li>\\(p \\in add(a)\\) iff \\(bs(p) = a\\)</li> <li>\\(bs(\\cdot)\\) is closed<ul> <li>\\(bs(p)\\) is defined for every \\(p \\in (P \\backslash s)\\) that has a path to a goal \\(g \\in G\\)</li> </ul> </li> <li>\\(bs(\\cdot)\\) is well-bounded<ul> <li>Support Graph is acyclic </li> </ul> </li> </ul> </li> <li>If a relaxed plan exists, the closed well-founded \\(bs(\\cdot)\\) definitely exists.<ul> <li>There is a relaxed path from \\(I\\) to \\(G\\)</li> <li>Every \\(g\\) has at least one supporter, so as its subgoals</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#helpful-actions","title":"Helpful Actions","text":"<ul> <li>An action is helpful iff:<ul> <li>it is applicable in the current state (\\(pre(a) \\subseteq s\\))</li> <li>it is contained in the final plan</li> </ul> </li> <li>Expanding only helpful actions does not guarantee completeness.</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-5-Relaxation/#relaxed-plan-heuristic-h_ff","title":"Relaxed Plan Heuristic (\\(h_{FF}\\))","text":"<ul> <li> <p>Fast Forward Expansion + Greedy Backward Extraction</p> <ul> <li>For each goal state \\(g \\in G\\):<ul> <li>Find the cheapest \\(a = bs(g)\\)</li> <li>Add this action to the plan</li> <li>repeat on \\(bs(pre(a))\\) <pre><code>def h_FF(I, G, A):\n    if G in I:\n        return 0\n\n    # Forward Expansion - BFS\n    reached = I\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return unsolvable\n        reached = new_facts\n\n    # Backward Extraction - Greedy\n    plan = []\n    subgoals = G\n    while subgoals not in I:\n        new_subgoals = []\n        for g in subgoals:\n            pick a s.t. g in add(a) and pre(a) in reached\n            plan += {a}\n            new_subgoals += pre(a)\n        subgoals = new_subgoals\n\n    return len(plan)\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Same theoretical properties as \\(h^{add}\\) but better in practice</p> <ul> <li>Overcount sub-plans shared by different sub-goals</li> <li>Best Supporter is greedily chosen and sub-optimal</li> <li>In practice, \\(h_{FF}\\) typically does not over-estimate \\(h^*\\), or not by a large amount.</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/","title":"WEEK 6 Exploration &amp; Exploitation","text":""},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#novelty","title":"Novelty","text":"<ul> <li>The size of the smallest subset of \\(P \\subseteq s\\), such that \\(s\\) is the first state that makes \\(P\\) <code>TRUE</code> during the search</li> <li>Width: The size of the smallest subset of \\(P\\) needed to be considered to achieve the goal</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#example","title":"EXAMPLE","text":"\\[ p \\to (p, q) \\to (q, r) \\to (p, r) \\to (p, q, r) \\] Current State Smallest New Subset Novelty \\(p\\) \\(p\\) 1 \\(p, q\\) \\(q\\) 1 \\(q, r\\) \\(r\\) 1 \\(p, r\\) \\(p, r\\) 2 \\(p, q, r\\) \\(p, q\\) 2"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#width-based-planning","title":"Width-Based Planning","text":""},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#iterated-width-iw","title":"Iterated Width (IW)","text":"<ul> <li>\\(IW(k) =\\) BFS on \\((Q \\; \\backslash \\; s), \\mathrm{novelty}(s) &gt; k\\)<ul> <li>\\(IW(1) =\\) There is new \\(p\\) appearing in every step in search</li> </ul> </li> <li>\\(IW\\) Algorithm<ul> <li>A sequence of calls \\(IW(k), k=1,2,3,\\ldots\\), until the problem solved or \\(k &gt; len(\\bigcup P)\\) (return <code>unsolvable</code>). </li> <li>The \\(minimum\\) \\(k\\) is the \\(\\text{Width}\\) of the problem</li> </ul> </li> <li>Outcomes<ul> <li>Simple and Blind<ul> <li>Even don't need to know \\(G\\) </li> </ul> </li> <li>However performs pretty well in practice<ul> <li>\\(IW(k \\le 2)\\) can solve 88.3% IPC problems with single goals.</li> <li>Most classical problem (e.g. Blocks, Logistics, Gripper, n-puzzle) have a bounded width independent of problem size and initial situation</li> </ul> </li> <li>Fast \\(\\mathrm{O}(n^k)\\)</li> <li>Optimal if in uniform cost</li> <li>Complete</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#serialised-iterated-width-siw","title":"Serialised Iterated Width (SIW)","text":"<ul> <li> <p>Use \\(IW\\) for decomposing problem and solving sub-problems individually <pre><code>def SIW(s, G):\n    state = s\n    plan = []\n    for g in serialize(G):\n        subplan = IW(state, goal=g)\n        plan += subplan\n        state = Result(subplan, state)\n    return plan\n</code></pre></p> </li> <li> <p>Better performance in Joint Goals problem</p> <ul> <li>(Multi goals but similar approaches)</li> </ul> </li> <li>\u24c1 Goals should be easy to serialise and have low width</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#balancing-exploration-exploitation","title":"Balancing Exploration &amp; Exploitation","text":"<ul> <li>Exploitation: Trusting your heuristic function<ul> <li>State-based Satisfying Planning Often Rely on:<ul> <li>heuristics derived from problem</li> <li>plugged into Greedy Best-First Search (GBFS)  </li> <li>extensions (e.g. helpful actions and landmarks)  </li> </ul> </li> <li>Often gets stuck in local minima<ul> <li>poly + sub-optimal or optimal + NP-hard</li> </ul> </li> </ul> </li> <li>Exploration: Searching for Novelty<ul> <li>Novelty leads to much better performance in practice</li> <li>Can be model-free (No Rely/Assumption)</li> <li>Required for optimal behaviour (in RL and MTCS)</li> </ul> </li> <li>A good agent need to balance the Exploration and Exploitation</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#best-first-width-search-bfws","title":"Best-First Width Search (BFWS)","text":"<ul> <li>Do BFS (Priority Queue) on a sequence of measures:     $$     BFWS(f) \\text{ for } f= \\langle w,f_1,f_2,\\ldots \\rangle     $$<ul> <li>\\(w\\) - Novelty-Measure</li> <li>\\(f_i\\) - tie breaker</li> </ul> </li> <li>Much more efficient than GBFS</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#models-to-simulators","title":"Models to Simulators","text":""},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#models","title":"Models","text":"<ul> <li>Nowadays, models become more powerful with helps of declarative programming<ul> <li>Expressive language features easily supported</li> <li>Development of external development tools</li> <li>Fully-Black-Box procedures for higher-level abstraction and decomposing problems</li> </ul> </li> <li>However, Declarative Languages also have their downsides:<ul> <li>Model \\(\\ne\\) Language<ul> <li>Many problems fit Classical Planning model, but hard to express in PDDL</li> </ul> </li> </ul> </li> <li>We need for planners that work without complete declarative representations</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#simulators","title":"Simulators","text":"<ul> <li>= Models but no \\(pre(a)\\) and \\(add(a)\\) for presenting \\(a\\)</li> <li>Outcomes<ul> <li>At the same level of efficiency as classic models</li> <li>Open up exciting possibilities for modelling beyond PDDL</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#simulated-bfws-framework","title":"Simulated BFWS Framework","text":"<ul> <li>Framework<ul> <li>Get communication across researchers and to build on each others\u2019 work</li> </ul> </li> <li>Approaches<ul> <li>Get \"states\" from the simulator</li> <li>The optimal combination is \\(BFWS(\\langle w_h(s), h(s) \\rangle)\\) <ul> <li>\\(w_h(s) =\\) \\(s'\\) which has smallest novelty and \\(h(s') = h(s)\\)</li> </ul> </li> </ul> </li> <li>BFWS is the first planners using simulators</li> <li>Challenges of Width-Based Planning over Simulators<ul> <li>Non-linear dynamics  </li> <li>Perturbation in flight controls  </li> <li>Partial observability  </li> <li>Uncertainty about opponent strategy</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-6-Exploration-and-Exploitation/#arcade-learning-environment","title":"Arcade Learning Environment","text":"<ul> <li>A simple object-oriented framework to develop AI agents for Atari 2600 games<ul> <li>Deterministic</li> <li>Initial state fully known</li> </ul> </li> <li>Performance of \\(IW(1)\\) <ul> <li>better in 34/54 games than 2BFS</li> <li>better in 31/54 games than UCT</li> <li>better in 45/49 games than DeepMind (RL method)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-7-Introduce-to-Reinforced-Learning/","title":"WEEK 7 Introduce to Reinforced Learning","text":"<p>Table of Contents generated with DocToc</p> <ul> <li>WEEK 7 Introduce to Reinforced Learning</li> </ul>"},{"location":"Unimelb/25S2/AIP/WEEK-7-Introduce-to-Reinforced-Learning/#week-7-introduce-to-reinforced-learning","title":"WEEK 7 Introduce to Reinforced Learning","text":"<ul> <li>Map Situations to Actions - so as to \\(maximise\\) the Rewards</li> </ul> <p>Different from Automated Planning - Outcomes of actions are non-deterministic - Use probabilistic presentation on environment &amp; policy</p> <p>Different from Other ML - Sequential Data - Time Series (non-i.i.d) - Trial-and-error Search - No supervisor, only signal from environment - Delayed Reward - credit assignment matters</p> <p>Scenario - Environment is initially unknown - Agent interacts with environment - Agent improves its policy through prediction, planning, control and approximation</p> <p>Components of the environment</p> <p>State</p> <ul> <li>\u24b6 State is Markov</li> <li> <p>State = f(History) </p> </li> <li> <p>Markov property \ud835\udc43(\ud835\ude34\u2019 | \ud835\ude34+\ud835\udc3b(\ud835\ude34), \ud835\udc4e) = \ud835\udc43(\ud835\ude34\u2019 | \ud835\ude34, \ud835\udc4e)</p> </li> <li>Once S is known, H can be thrown away</li> </ul> <p>Rewards </p> <ul> <li>A scalar feedback signal</li> <li>Indicates how well agent is doing at one step</li> <li>Reward Hypothesis All\u00a0goals can be described by the maximisation of expected cumulative reward.</li> </ul> <p>Components of an RL Agent</p> <p>Two Key Problems\u00a0 of RL</p> <ul> <li> <p>Prediction</p> </li> <li> <p>Evaluate the future rewards of state-actions \u2192 Value Function</p> </li> <li> <p>Control</p> </li> <li> <p>Find the optimal policy \u2192 Policy Function</p> </li> </ul> <p>Model</p> <ul> <li>Simulator of the environment based on observations of the agent</li> <li>Not necessary \u2192 Model-based/Model-free</li> <li> <p>Probability Function </p> </li> <li> <p>\ud835\udcdf(\ud835\ude34, \ud835\udc4e, \ud835\ude34\u2019) = \ud835\udc43(\ud835\ude34\u2019 | \ud835\ude34, \ud835\udc4e)</p> </li> <li> <p>Reward Function</p> </li> <li> <p>\ud835\udce1(\ud835\udc4e, \ud835\ude34) \u2009=\u2009 \ud835\udd3c[\ud835\ude19\u2019 | \ud835\ude34, \ud835\udc4e]</p> </li> </ul> <p>Value Function </p> <ul> <li>Not necessary \u2192 Value-based/Policy-based</li> <li> <p>Define and predict value of the state and future reward</p> </li> <li> <p>\ud835\udc49(\ud835\ude34) = \ud835\udd3c[\ud835\udc3a | \ud835\ude34] \u00a0</p> </li> <li>Value of State = The Expect of future rewards \ud835\udc3a</li> <li> <p>Bellman Equation O(n\u00b3)</p> </li> <li> <p>\ud835\udc3a = \ud835\ude19 + \ud835\udefe\ud835\udc3a\u2019</p> </li> <li> <p>\ud835\udefe = discount factor</p> </li> <li> <p>\u2192 \ud835\udc49 = \ud835\udce1 + \ud835\udefe\ud835\udcdf\ud835\udc49 \u2192 \ud835\udc49 = (\ud835\udc3c - \ud835\udefe\ud835\udcdf)\u207b\u00b9\ud835\udce1</p> </li> </ul> <p>Policy Function</p> <ul> <li>Output: agent\u2019s behaviour = next step action \ud835\udc4e</li> <li> <p>No Policy - Brute Force</p> </li> <li> <p>\ud835\udc4e = \ud835\udc1a\ud835\udc2b\ud835\udc20 \ud835\udc26\ud835\udc1a\ud835\udc31 \ud835\udc44(\ud835\ude34, \ud835\udc4e) = \ud835\udc1a\ud835\udc2b\ud835\udc20 \ud835\udc26\ud835\udc1a\ud835\udc31 \ud835\udeba \ud835\udcdf(\ud835\ude34, \ud835\udc4e, \ud835\ude34\u2019) \u00b7 \ud835\udc49(\ud835\ude34\u2019)</p> </li> <li> <p>Compare expectations of all valid actions</p> </li> <li> <p>Deterministic policy </p> </li> <li> <p>\ud835\udf45(\ud835\ude34) = \ud835\udc4e\u00a0</p> </li> <li> <p>Stochastic Policy </p> </li> <li> <p>\ud835\udf45(\ud835\udc4e | \ud835\ude34) = \ud835\udc43(\ud835\udc4e | \ud835\ude34)</p> </li> <li>Choose the optimal action or random one with a small probability</li> <li> <ul> <li>Ability of exploration; Robustness</li> </ul> </li> <li> <p>Policy Gradient</p> </li> <li> <p>\ud835\udc09(\ud835\udf03) = \ud835\udd3c[\ud835\udc3a] = \ud835\udeba \ud835\udc43(\ud835\udf0f | \ud835\udf03)\ud835\udc3a(\ud835\udf0f)</p> </li> <li> <p>\ud835\udf0f = A full path from initial state to the goal</p> </li> <li> <p>\u2207\ud835\udc09(\ud835\udf03) = \ud835\udeba \u2207\ud835\udc43(\ud835\udf0f | \ud835\udf03)\ud835\udc3a(\ud835\udf0f)</p> </li> <li> <p>\u2190 \u2207\ud835\udc43(\ud835\udf0f | \ud835\udf03) = \ud835\udc43(\ud835\udf0f | \ud835\udf03)\u2207\u33d2\ud835\udc43(\ud835\udf0f | \ud835\udf03) (score function trick)</p> </li> <li>= \ud835\udd3c[\u2207\u33d2\ud835\udc43(\ud835\udf0f | \ud835\udf03)\ud835\udc3a(\ud835\udf0f)]</li> <li>\u2190 \ud835\udc43(\ud835\udf0f | \ud835\udf03)\ud835\udc3a(\ud835\udf0f) = \ud835\udeba \ud835\udf0b(\ud835\udc4e | \ud835\ude34)\ud835\udc3a</li> <li> <p>= \ud835\udd3c[\u2207\u33d2\ud835\udf0b(\ud835\udc4e | \ud835\ude34)\ud835\udc3a]</p> </li> <li> <p>Maintain a set of parameters for maximising future rewards</p> </li> </ul> <p>Learning Process</p> <p>Environment \u2192 State, Reward \u2192 Agent \u2192 Update the previous policy \u2192 Make the\u00a0 Decision \u2192 Environment</p> <p>Categorise of RL agents</p> <ul> <li>Model Based - Simulate environment to gain first experience\u00a0</li> <li> <p>Model Free - Gain experience from real interaction directly</p> </li> <li> <p>Value Based - No Policy</p> </li> <li>Policy Based - No Value Function = Policy Gradient</li> <li>Actor-Critic - Policy Based \u6f14\u5458 + Value Based \u8bc4\u8bba\u5bb6</li> </ul> <p>Exploration vs Exploitation Trade-off</p> <ul> <li>Exploitation - select currently-known best action</li> <li>Exploration - try a new action</li> </ul> <p>Trade-off Strategies</p> <ul> <li>\u03b5-greedy all most all time (1-\u03b5) exploitation (\u03b5 is very small)</li> <li> <p>softmax set a probability to each action and a temperature \u03c4 to control\u00a0</p> </li> <li> <p>P(a) = exp(Q_a/\u03c4)/\u03a3exp(Q_others/\u03c4)</p> </li> <li> <p>Upper Confidence Bound (UCB) exploitation + exploration bonus</p> </li> <li> <p>a= argmax(Q\u0302(a) + c\u221a(ln t/N(a)))</p> </li> <li>c importance of exploration (parameter)</li> <li>ln t explore more when time goes</li> <li>N(a) not try too many times on same action</li> </ul>"}]}