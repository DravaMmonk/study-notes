{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Drava's Study Notes","text":""},{"location":"index.html#master-of-information-technology-in-the-university-of-melbourne","title":"\ud83c\udf93 Master of Information Technology in the University of Melbourne","text":""},{"location":"index.html#2025-semester-2","title":"\ud83d\uddd3\ufe0f 2025 Semester 2","text":"<ul> <li>\ud83d\udcd8 AI Planning for Autonomy - COMP90054</li> <li>\ud83d\uddd3\ufe0f Introduce to the Machine Learning - COMP90049</li> <li>\ud83d\uddd3\ufe0f Model of Computation - COMP30026</li> </ul>"},{"location":"index.html#other","title":"\ud83d\udcbb Other","text":"<ul> <li>\ud83d\udcad CNN for Visual Recognition - Stanford CS231n</li> <li>\ud83d\udcad Natural Language Processing - Stanford CS224n</li> </ul>"},{"location":"index.html#reference","title":"\ud83d\udcda Reference","text":""},{"location":"Other/01-LC.html","title":"1 Linear Classifier","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html","title":"1 Search","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#search-space","title":"Search Space","text":"<p>Search Space - The set of all possible states and transitions that a search or planning algorithm must consider</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#example","title":"EXAMPLE","text":"<p>In a robot delivery scene:</p> <pre><code>world_state_of_a_robot = {\n\u00a0 \u00a0 \"position\": (3, 5),\n\u00a0 \u00a0 \"direction\": \"North\",\n\u00a0 \u00a0 \"battery\": 80,\n\u00a0 \u00a0 \"carrying_package\": True\n}\n\nsubgoals = {\"position\": (5, 8), \"carrying_package\": True}\n</code></pre> Common Functions <p>Search states:</p> <ul> <li>\\(\\mathrm{is}\\_\\mathrm{start}(s)\\) return if the state is the start state of the search space</li> <li>\\(\\mathrm{is}\\_\\mathrm{target}(s)\\) mark if the state is the goal state of the search space</li> <li>\\(\\mathrm{succ}(s)\\) return a list of successors/next states of \\(s\\)</li> </ul> <p>Search nodes:\u00a0</p> <ul> <li>\\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(\\mathrm{parent}(\\sigma)\\) where \\(\\sigma\\) was reached</li> <li>\\(\\mathrm{action}(\\sigma)\\) leads from \\(\\mathrm{state}(\\mathrm{parent}(\\sigma))\\) to \\(\\mathrm{state}(\\sigma)\\)</li> <li>\\(g(\\sigma)\\) denotes cost of path from the root to \\(\\sigma\\)</li> <li>The root\u2019s \\(\\mathrm{parent}(\\cdot)\\) and \\(\\mathrm{action}(\\cdot)\\) are undefined</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#search-methods","title":"Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#forward-search-vs-backward-search","title":"Forward Search vs. Backward Search","text":"<ul> <li>Forward Search (Progression)<ul> <li>search space = world space (represents the current real world)</li> <li>BFS, DFS, A*, MDP</li> </ul> </li> <li>Backward Search (Regression)<ul> <li>search space = a sets of world spaces (represents all predications of sub-goals)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#blind-search-vs-informed-search","title":"Blind Search vs. Informed Search","text":"<ul> <li>Blind search not require any input beyond the problem</li> <li>No additional work but rarely effective</li> <li>Informed search requires function \\(h(x)\\) mapping states to estimates their goal distance</li> <li>Effective but lots of work to construct \\(h(x)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#blind-systematic-search","title":"Blind Systematic Search","text":"<ul> <li>Breadth-First Search</li> <li>Depth-First Search</li> <li>Iterative Deepening Search<ul> <li>Do DLS(DFS with depth limited) with continuously increasing depth limited by 1.</li> </ul> </li> </ul> Properties <ul> <li>Completeness 100%\u2705 </li> <li>Really poor efficient when scale up\u2639\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#informed-systematic-search","title":"Informed Systematic Search","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#heuristic-functions","title":"Heuristic Functions","text":"<ul> <li>\\(h(n)\\) - Estimated remaining cost (from the current state to the goal state)</li> <li>\\(h^*(n)\\) - Actual remaining cost</li> </ul> <p>proficiency of \\(h(n)\\):</p> <ul> <li>\\(h = h^{\\ast}\\) perfectly informed, \\(h(n) = h^{\\ast}(n) - \\textbf{optimal } A^{\\ast}\\)</li> <li>\\(h = 0\\) no information at all - uniform cost search</li> </ul> Properties Description Safe \\(h(n) = \\infty\\) iff \\(h^{\\ast}(n) = \\infty\\) Goal-Aware \\(h(\\text{goal}) = 0\\) Admissible \\(h(n) \\le h^*(n)\\) Consistent \\(h(n) \\le c(n,n\u2019) + h(n\u2019)\\) for all possible \\(c(n, n\u2019)\\) <p>Relations</p> <ul> <li>Consistent &amp; Goal-aware \u2192 Admissible</li> <li>Admissible \u2192 Safe &amp; Goal-aware</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#greedy-best-first-search-gbfs","title":"Greedy Best-First Search (GBFS)","text":"<ul> <li>Use priority queue to sort \\(h(n)\\) of each node in ascending order<ul> <li>If \\(h(n) = 0\\), it becomes what fully depends\u00a0on how\u00a0we\u00a0break\u00a0ties</li> </ul> </li> </ul> <pre><code>def greedy_BFS:\n\u00a0 \u00a0 frontier = priority queue ordered by h(n)\n\u00a0 \u00a0 explored = set\n\u00a0 \u00a0 path = list\n\u00a0 \u00a0 frontier.add(start, h(start), path)\n\u00a0 \u00a0 \n\u00a0 \u00a0 while frontier:\n\u00a0 \u00a0 \u00a0 \u00a0 current = frontier.pop()\n\u00a0 \u00a0 \u00a0 \u00a0 if current == goal:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return path\n\u00a0 \u00a0 \u00a0 \u00a0 if current in explored:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n\u00a0 \u00a0 \u00a0 \u00a0 explored.add(current)\n\u00a0 \u00a0 \u00a0 \u00a0 for successor in succ(current):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new_path = path + action(current, successor)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frontier.add(h(current), current, new_path)\n\u00a0 \u00a0 return unsolvable\n</code></pre> <p>Completeness\u2705for safe heuristics; Optimal\u274c</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#a","title":"A*","text":"<ul> <li>Only difference from GBFS:<ul> <li>\\(h(n) \\rightarrow f(n) = g(n) + h(n)\\)</li> </ul> </li> </ul> <p><pre><code>function A*(start, goal):\n    open = priority queue\n    open.push(start, f(start))\n\n    while open is not empty:\n        n = open.pop_min_f()\n\n        if n == goal:\n            return reconstruct_path(n)\n\n        for each successor s of n:\n            compute g(s) and f(s)\n            open.push_or_update(s)\n</code></pre> <pre><code>#### Re-opening\n\nA node $n$ could be re-opened if we find a cheaper $g(n)$.\n</code></pre> def a_star_with_re_opening:     frontier = priority queue ordered by f(n) = g(n) + h(n)     explored = set     path = list \u00a0 \u00a0 frontier.add(h(start), start, path)</p> <p>\u270f\ufe0f  g = dict \u270f\ufe0f  g[start] = 0</p> <pre><code>while frontier:\n    _, current, path = frontier.pop()\n    if current == goal:\n        return path\n    explored.add(current)\n    for successor in succ(current):\n        new_path = path + action(current, successor)\n        new_g = cost(new_path)\n        if successor not in g or new_g &lt; g[successor]:\n            [successor] = new_g\n            f = g[successor] + h(successor)\n            frontier.add(f, successor, f)\n</code></pre> <p>\u270f\ufe0f              if successor in explored:     # Re-opening \u270f\ufe0f                  explored.remove(successor)     return unsolvable</p> <p><pre><code>Not needed if consistent\u2705  $g$ is already cheapest\n\n---\n### Weighted A*\n$$\nf_W(n) = g(n) + W \\cdot h(n)\n$$\n\n| Weight         |       Algorithm        |\n|:-------------- |:----------------------:|\n| $W \\to 0$      |   Digkstra Algorithm   |\n| $W \\to 1$      |           A*           |\n| $W &gt; 1$        | Bounded sub-optimal A* |\n| $W \\to \\infty$ |          GBFS          |\n\n&gt; If $h$ is admissible, $f_W(n) \\le W \\cdot h(n)$\n\n---\n### Hill-Climbing\n\nBrute Force to find greedy best direction for heuristic descent\n</code></pre> def hill_climbing: \u00a0 \u00a0 path = list \u00a0 \u00a0 current = start \u00a0 \u00a0 while h(n) &gt; 0: \u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current)) \u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best) \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break \u00a0 \u00a0 \u00a0 \u00a0 return path <pre><code>&gt; - Can only find local maxima\n&gt; - Available only if $h(n) &gt; 0$ for all non-goal states\n\n---\n### Enforced Hill-Climbing\n\nDo small range BFS when find local optimal\n</code></pre> def enforced_hill_climbing: \u00a0 \u00a0 path = list \u00a0 \u00a0 explored = set \u00a0 \u00a0 current = start \u00a0 \u00a0 while h(n) &gt; 0: \u00a0 \u00a0 \u00a0 \u00a0 explored.add(current) \u00a0 \u00a0 \u00a0 \u00a0 best = argmin_h(succ(current)) \u00a0 \u00a0 \u00a0 \u00a0 if best and h(best) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(current, best) \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subs = n for n in neighbours_of(current) and not in explored \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 best_sub = argmin_h(subs, goal=current) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if best_sub and h(best_sub) &lt; h(current): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path -= action(parent, current) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path += action(parent, best_sub) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 current = best_sub \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable \u00a0 \u00a0 return path <pre><code>&gt; Can across small gap between local best and global best\n\n---\n### Iterative Deepening A*\n\nIDS + $A^*$: Use f(n) instead of depth to limit IDS\n- In First Search:\u00a0 f(n) = f(start) = 0 + h(start)\n- Following Searches: f(n) = min_out_of_bound_excess\n</code></pre> def ida_star: \u00a0 \u00a0 bound = f(start) \u00a0 \u00a0 while True: \u00a0 \u00a0 \u00a0 \u00a0 t = ids(start, bound) \u00a0 \u00a0 \u00a0 \u00a0 if t == goal: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return solution \u00a0 \u00a0 \u00a0 \u00a0 if t == infinity: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return unsolvable \u00a0 \u00a0 \u00a0 \u00a0 bound = t ```</p> <p>Solve one of \\(A^*\\)\u2019s problem: large queue/closed_set</p>"},{"location":"Unimelb/25S2/AIP/01-Search.html#evaluation-of-search-methods","title":"Evaluation of Search Methods","text":""},{"location":"Unimelb/25S2/AIP/01-Search.html#guarantees","title":"Guarantees","text":"<ul> <li>Completeness sure to find a solution if there is one</li> <li>Optimality solutions sure be optimal</li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#complexity","title":"Complexity","text":"<ul> <li>Time/Space (Measured in generated states/states cost)</li> <li>Typical state space features governing complexity<ul> <li>Branching factor \\(b\\) how many successors</li> <li>Goal depth \\(d\\) number of actions to reach shallowest goal state</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/01-Search.html#summary","title":"Summary","text":"DFS BrFS IDS A* HC IDA* Complete \u274c \u2705 \u2705 \u2705 \u274c \u2705 Optimal \u274c \u2705* \u2705 \u2705* \u274c \u2705* Time \\(\\infty\\) \\(b^d\\) \\(b^d\\) \\(b^d\\) \\(\\infty\\) \\(b^d\\) Space \\(b \\cdot d\\) \\(b^d\\) \\(b \\cdot d\\) \\(b^d\\) \\(b\\) \\(b \\cdot d\\) <p>where</p> <ul> <li>\\(b\\) - Branching Factor: sum of number of child nodes of each node</li> <li>\\(d\\) - Solution Depth: \\(minimum\\) depth among all goal nodes</li> <li>DFS cannot handle cyclic graph</li> <li>BFS is optimal only when uniform costs are applied </li> <li>\\(A^*\\) / Iterative Deepening \\(A^*\\) is optimal only when \\(h\\) is admissible (\\(h \\le h^*\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html","title":"2 Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#ai-solver","title":"AI Solver","text":"3-level Solver Ambitions <p>Ambition: Write one program to solve all classical search problems</p> <ul> <li>Ambition 1.0 (Problem Solving) Write one program to solve a problem</li> <li>Ambition 2.0 (Problem Generation) Write one program to solve a large class of problems</li> <li>Ambition 3.0 (Meta Problem Solving) Write one program to solve a large class of problems effectively</li> </ul> <p>Programming-Based:</p> <ul> <li>Use static programs designed by the programmer</li> <li>e.g. Pac-man</li> </ul> <p>Learning-Based:</p> <ul> <li>Unsupervised/Reinforced Learning: Use reward &amp; penalise</li> <li>Supervised: Use labelled data</li> <li>Evolutionary: Use original controllers to mutate and recombine to build better controller</li> <li>e.g. OpenAI-Five</li> </ul> <p>Model-Based (Planning):</p> <ul> <li>Construct a model for one specific class of problems</li> <li>Different models yield different types of controllers</li> </ul> <p></p> <p>Pros &amp; Cons</p> <p>Programming-Based</p> <ul> <li>Domain-knowledge easy to express\u2705</li> <li>Less Flexible\u2639\ufe0f </li> <li>Can\u2019t deal with no anticipation of programmer\u274c</li> </ul> <p>Learning-Based</p> <ul> <li>Dose not require much knowledge in principle\u2705</li> <li>Slower; Hard to know which features to learn\u2639\ufe0f</li> </ul> <p>Model-Based</p> <ul> <li>Powerful; Quick; Flexible; Clear; Intelligent; Domain-independent\u2705</li> <li>Need a model (sometimes very hard); Efficiency loss\u2639\ufe0f</li> </ul> <p>Tip</p> <p>To design a good solver, always balance between \u201cAutomatic and General\u201d vs. \u201cManual but Effective\u201d</p>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#models-for-planning","title":"Models for Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#classical-planning-model","title":"Classical Planning Model","text":"<p>The most basic model for describing the environment, the goal output is a sequence of actions map \\(I\\) to \\(G\\).</p> \\[ M = \\langle S, A, T, I, G \\rangle \\] <p>where</p> <ul> <li>\\(S\\) - State spaces </li> <li>\\(A(s)\\) - Actions applicable for \\(s \\in S\\)</li> <li>\\(T\\) - Deterministic Transition Function: \\(s\u2019 = T(A(s), s)\\) shows one successor \\(s\u2019\\) of \\(s\\)</li> <li>\\(I\\) - Initial state</li> <li>\\(G\\) - Goal states</li> <li>Uniform action costs \\(c(A(s), s) = 1\\)</li> </ul> <p>Properties of Classical Planning Model</p> <ul> <li>Simple; Intuitive\u2705</li> <li>Very strict assumptions\u2757\ufe0f\u2757\ufe0f<ul> <li>Deterministic</li> <li>Fully Observable</li> <li>Static World</li> <li>Discrete Time &amp; Finite Actions</li> <li>Uniform Cost</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#conformant-planning-model","title":"Conformant Planning Model","text":"<p>Derived from Brute Force, implement the ability to handle stochasticity\uff1a</p> <ul> <li>Initial State \u2192 A set of possible initial states</li> <li>Deterministic Transition Function \u2192 Non-deterministic</li> </ul> <p>Outcomes: Conformant</p> <ul> <li>Goal Guarantee \u2705</li> <li>No Observation - No new info; must pre-planning\u26a0\ufe0f</li> <li>Rarely optimal \u26a0\ufe0f<ul> <li>Must map any possible \\(s_0\\) to \\(g\\), too much unnecessary work</li> <li>Sensitive to Worst/Special case </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/02-Planning.html#languages-for-planning","title":"Languages for Planning","text":""},{"location":"Unimelb/25S2/AIP/02-Planning.html#strips","title":"STRIPS","text":"<p>STRIPS = \"STanford Research Institute Problem Solver\", which designed a language for:</p> <ul> <li>Specification - concise model description</li> <li>Computation - reveal useful information/structure for heuristics</li> </ul> <p>All problems in following format can be solved by STRIPS:</p> \\[ P = \\langle P,A,I,G \\rangle \\] <p>where</p> <ul> <li>\\(P\\) - All Possible Predicates</li> <li>\\(A\\) - Actions</li> <li>\\(I\\) - Initial States: Predicates initially to be <code>TRUE</code></li> <li>\\(G\\) - Goal Conditions: Predicates Finally need to be <code>TRUE</code></li> </ul> <p>Actions</p> \\[ a \\in A, a = \\langle Pre(a), Add(a), Del(a) \\rangle \\] <p>where</p> <ul> <li>\\(Pre(a)\\) - Preconditions need to be <code>TRUE</code> before executing the action</li> <li>\\(Add(a)\\) - Add Effects: Preconditions set to be<code>TRUE</code> after execution</li> <li>\\(Del(a)\\) - Delete Effects: Preconditions set to be <code>FALSE</code> after execution</li> </ul> <p>Result Function (Application/Transition Function)</p> \\[ Result(s, a) = (s - Del(a)) \\cup Add(a) \\] <p>Goal of STRIPS</p> <p>Find a list of actions \\([a_1,a_2,a_3,\\ldots]\\) which satisfies:</p> \\[ Result(Result(\\ldots Result(I,a_1\u200b) \\ldots ,a_n\u22121\u200b),a_n\u200b) \\models G \\] <p>Outcomes: STRIPS</p> <p>It turns \"Solving a problem\" into \"Search for solutions\".</p> <p>Still PSPACE-complete, but problem is now more concise and intuitive:     - Explicit Search         - e.g. Blind/Heuristic search         - Optimal Solution Guarantee\u2705; Not effective\u274c</p> <p>And \"approximation\" is now able to be applied as well:     - Near Decomposition         - e.g. Relaxation         - Effective\u2705; Maybe fail\u274c</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html","title":"3 Relaxation","text":""},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#complexity","title":"Complexity","text":"<ul> <li>P: Problem can be solved using polytime</li> <li>NP: Problem can be solved using non-deterministic polytime</li> <li>PSPACE: Problem can be solved using polynomial space</li> <li>EXP: Problem can be solved using exponential time</li> </ul> \\[ P \\subseteq NP \\subseteq PSPACE \\subseteq EXP \\] <p>Savitch\u2019s Theorem </p> <p>NPSPACE = PSPACE - Non-determinism won't make problems to be harder or solvers to be more powerful.</p> <p>A decision problem \\(L\\) is PSPACE-complete iff:</p> <ol> <li>\\(L \\in\\) PSPACE<ul> <li>\\(L\\) can be solved using polynomial space</li> </ul> </li> <li>\\(L\\) is PSPACE-hard<ul> <li>every problem in PSPACE can be polynomial-time reduced to \\(L\\)</li> </ul> </li> </ol>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#goals-definition","title":"Goals Definition","text":"<p>\\(\\mathrm{PlanEx}(P)\\)</p> <ul> <li>for making a Satisficing planning</li> <li>\\(\\mathrm{PlanEx}(P) =\\) <code>TRUE</code> - Existence of a plan for problem \\(P\\).</li> </ul> <p>\\(\\mathrm{PlanLen}(P)\\) </p> <ul> <li>for making a Optimal planning</li> <li>\\(mathrm{PlanLen}(P) \\le B\\) - Existence of a plan which length is at most \\(B\\).</li> </ul> <p>Note</p> <p>Both \\(\\mathrm{PlanEx}\\) and \\(\\mathrm{PlanLen}\\) are PSPACE-complete. - In practice, optimal planning is almost never easy</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxation","title":"Relaxation","text":"<p>A transformation for simplifying the heuristic value computation.</p> \\[ \\mathcal{R} = (\\mathcal{P'}, r, h'^*) \\] <p>where</p> <ul> <li>\\(\\mathcal{P}'\\) - the class of the simpler problem </li> <li>\\(r\\) - transformer turning the original problem into a simplified one</li> <li>\\(h'^*(n)\\) - Perfect heuristic function of the simplified problem </li> </ul> <p>Properties of a Relaxation</p> <p>The relaxation is:</p> <ul> <li>Native - if \\(\\mathcal{P'} \\subseteq \\mathcal{P}\\) and  \\(h'^*(n) = h(n)\\)</li> <li>Efficiently Constructible - if a polynomial \\(r\\) exists</li> <li>Efficiently Computable - if a polynomial \\(h\u2019(n)\\) exists</li> </ul> Applications of Relaxation <ol> <li> <p>Route-Finding</p> <ul> <li>Relaxation<ul> <li>Route-find as a bird (ignoring the road)</li> </ul> </li> <li>Outcome<ul> <li>Road Distance \u2192 Manhattan distance, Euclidean distance, etc.</li> </ul> </li> <li>Native\u274c Efficiently constructible\u2705 Efficiently computable\u2705</li> </ul> </li> <li> <p>Goal-Counting</p> <ul> <li>Relaxation<ul> <li>Assume we can achieve each goal directly</li> </ul> </li> <li>Outcome<ul> <li>Tasks \u2190 No precondition and delete</li> </ul> </li> <li>Admissible but still NP-hard</li> <li>Native\u2705 Efficiently constructible\u2705 Efficiently computable\u274c</li> </ul> </li> </ol> <p>Still inefficiency?</p> <ul> <li>Approximate \\(h\u2019^*\\)</li> <li>Re-design \\(h\u2019^*\\) in a way so that it will typically be feasible<ul> <li>Critical path heuristics</li> <li>Delete relaxation \u2190 wide-spread for satisficing planning</li> <li>Abstractions</li> <li>Landmarks</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#delete-relaxation","title":"Delete Relaxation","text":"<p>Apply \\(Del(a) = \\emptyset, \\forall a \\in A\\).</p> <ul> <li>Facts once be <code>TRUE</code> will remain <code>TRUE</code> \"forever\"</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#dominance","title":"Dominance","text":"<p>\\(s' \\supseteq s\\) - \\(s'\\) dominate \\(s\\)</p> <p>Properties of a Dominance</p> <ul> <li>If \\(s\\) is a goal state, then \\(s'\\) must be a goal state.</li> <li>If \\(a^+\\) is applicable in \\(s\\), then \\(a^+\\) must be applicable in \\(s'\\).</li> <li>\\(Result(s, a^+)\\) dominates both \\(s\\) and \\(Result(s, a)\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#optimal-delete-relaxation-heuristic-h","title":"Optimal Delete Relaxation Heuristic (\\(h^+\\))","text":"<p>Apply delete relaxation on heuristic computation.</p> <pre><code>def h_plus(s, G, A):\n    if G in s:\n        return 0\n\n    open = [s]\n    cost[s] = 0\n    cost[others] = inf\n    best = inf\n\n    while open:\n        cur_state = pop(open)\n        if G in cur_state:\n            best = min(best, cost[cur_state])\n            continue\n\n        for a in A:\n            if pre(a) in current:\n                next_state = current + add(a)\n                if cost[next_state] &gt; cost[cur_state] + 1:\n                    cost[next_state] = cost[current] + 1\n                    push(open, next_state)\n\n    if best == inf:\n        return unsolvable\n    else:\n        return best\n</code></pre> <p>Outcomes: \\(h^+\\)</p> <ul> <li>Native relaxation\u2705</li> <li>Safe\u2705, goal-aware\u2705, admissible guarantee\u2705</li> <li>Efficiently constructible\u2705 Polytime</li> <li>Efficiently computable\u274c<ul> <li>\\(\\mathrm{PlanOpt^+} = \\sum h^+\\) still NP-hard</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<p>A extension planning algorithm based on \\(h^+\\).</p> <ol> <li>Initially set \\(d[s_0] = 0\\); other \\(d[s] = \\infty\\).</li> <li>Relax every edge:<ul> <li>For each edge \\(\\langle u, r, v \\rangle\\):<ul> <li>If \\(d[u] + w &lt; d[v]\\), update \\(d[v] \\gets d[u] + w\\) </li> </ul> </li> <li>Repeat \\(|V| - 1\\) times to ensure all shortest paths are propagated</li> </ul> </li> <li>Check all edges again, if any relaxable edges still exist, there are negative cycles in the graph</li> </ol> <p>Outcomes: Bellman-Ford</p> <ul> <li>Find all shortest path to every node at most \\(|V| - 1\\) steps \u2705</li> <li>Able to detect negative circles</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#additive-heuristic-hadd","title":"Additive Heuristic (\\(h^{add}\\))","text":"<p>Take the sum of heuristic values to all goals from the current state.</p> <pre><code>def h_sum(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n\u270f\ufe0f              new_cost = 1 + sum(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n\u270f\ufe0f  return sum(cost[g] for g in G)\n</code></pre> <p>Outcomes of \\(h^{add}\\)</p> <ul> <li>Efficiently Computable\u2705 Polytime</li> <li>Pessimistic Estimation: admissible\u274c informative\u2705 </li> <li>Overcounts by ignoring positive interactions, i.e. shared sub-plans<ul> <li>May result in \\(dramatic\\) over-estimates of \\(h^*\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#max-heuristic-hmax","title":"Max Heuristic (\\(h^{max}\\))","text":"<p>Only take the \\(highest\\) heuristic value to such goal from the current state.</p> <pre><code>def h_max(s, G, A):\n    for p in P:\n        if p in s:\n            cost[p] = 0\n        else:\n            cost[p] = inf\n\n    changed = True\n    while changed:\n        changed = false\n        for a in A:\n            if cost[q] &lt; inf for all q in pre(a):\n                # a is reachable\n                new_cost = 1 + max(cost[q] for q in pre(a))\n                for p in add(a):\n                    if new_cost &lt; cost[p]:\n                        cost[p] = new_cost\n                        changed = true\n\n    return max(cost[g] for g in G)\n</code></pre> <p>Outcomes of \\(h^{max}\\)</p> <ul> <li>Efficient Computable\u2705 Polytime<ul> <li>However, sometimes maybe too optimistic\u26a0\ufe0f</li> </ul> </li> <li>Optimistic Estimation: admissible\u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxed-plan-existence-mathrmplanex","title":"Relaxed Plan Existence  (\\(\\mathrm{PlanEx}^+\\))","text":"<p>Check if there exists a plan by Fast Forward Expansion.</p> <ul> <li>Fast Forward Expansion: Continuously expand facts set of the initial state by BrFS until it dominates all goal states.</li> </ul> <pre><code>def plan_exists(s, G, A):\n    if G in s:\n        return TRUE\n\n    reached = s\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return FALSE\n        reached = new_facts\n\n    return TRUE\n</code></pre> <p>Outcomes of \\(\\mathrm{PlanEx}^+\\)</p> <ul> <li>Sound, complete, Terminates in polytime\u2705<ul> <li>\\(\\mathrm{PlanEx}^+\\) now becomes a polytime problem</li> </ul> </li> <li>Safe\u2705, goal-aware\u2705, admissible\u274c</li> <li>Can ONLY be used to check existence of a Relaxed plan \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#helpful-actions","title":"Helpful Actions","text":"<p>An action is helpful iff:</p> <ul> <li>it is applicable in the current state (\\(pre(a) \\subseteq s\\))</li> <li>it is contained in the relaxed plan given by Fast Forward Expansion</li> </ul> <p>Expanding only helpful actions does not guarantee completeness.</p> <p>Helpful actions is a heuristic-based pruning, some necessary but non-intuitive might unachievable</p>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#relaxed-plan-heuristic-h_ff","title":"Relaxed Plan Heuristic (\\(h_{FF}\\))","text":"<p>Derived from Fast Forward Expansion, we can further perform Greedy Backward Extraction for finding the optimal relaxed plan.</p> <ul> <li>Greedy Backward Extraction: Find the cheapest helpful action based on Best Supporter Function.</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#best-supporter-function-bsp","title":"Best Supporter Function \\(bs(p)\\)","text":"<p>Output the \\(cheapest\\) action \\(a\\) where \\(add(a) \\supseteq p\\).</p> <p>Prerequisites of a valid \\(bs(\\cdot)\\)</p> <p>\\(bs(\\cdot)\\) is closed</p> <ul> <li>Every predicate that can appear during regression must have a defined regression (no missing cases).</li> <li>Guarantees that backward search never gets stuck due to undefined subgoals.\u2705</li> </ul> <p>\\(bs(\\cdot)\\) is well-bounded</p> <ul> <li>The dependency / support graph of subgoals is acyclic.</li> <li>Ensures regression does not loop back to previous subgoals, preventing infinite recursion.\u2705</li> </ul> <p>By performing Fast Forward Expansion, if a relaxed plan exists, a closed well-founded \\(bs(\\cdot)\\) definitely exists, which means:</p> <ul> <li>There is a relaxed path from \\(I\\) to \\(G\\)</li> <li>Every \\(g\\) has at least one supporter, so as its subgoals</li> </ul>"},{"location":"Unimelb/25S2/AIP/03-Relaxation.html#algorithm-h_ff","title":"Algorithm: \\(h_{FF}\\)","text":"<ul> <li>Perform Fast Forward Expansion</li> <li>For each goal states \\(g \\in G\\):<ul> <li>Find the cheapest \\(a = bs(g)\\)</li> <li>Add this action to the plan</li> <li>repeat on \\(bs(pre(a))\\)</li> </ul> </li> </ul> <pre><code>def h_FF(I, G, A):\n    if G in I:\n        return 0\n\n    # Forward Expansion - BrFS\n    reached = I\n    while G not in reached:\n        new_facts = reached\n        for a in A:\n            if pre(a) in reached:\n                new_facts += add(a)\n        if new_facts == reached:\n            return unsolvable\n        reached = new_facts\n\n    # Backward Extraction - Greedy\n    plan = []\n    subgoals = G\n    while subgoals not in I:\n        new_subgoals = []\n        for g in subgoals:\n            pick a s.t. g in add(a) and pre(a) in reached\n            plan += {a}\n            new_subgoals += pre(a)\n        subgoals = new_subgoals\n\n    return len(plan)\n</code></pre> <p>Outcomes of \\(h_{FF}\\)</p> <ul> <li>Same theoretical properties as \\(h^{add}\\):<ul> <li>May overcount sub-plans shared by different sub-goals \u26a0\ufe0f</li> </ul> </li> <li>Best Supporter is greedily selected and sub-optimal \u26a0\ufe0f</li> </ul> <p>However in practice, \\(h_{FF}\\) typically does not over-estimate \\(h^*\\), or not by a large amount.\ud83d\udcad</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html","title":"4 Exploration","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#balancing-the-exploitation-exploration","title":"Balancing the Exploitation &amp; Exploration","text":"<p>Exploitation: Trusting your heuristic function</p> <p>Exploration: Searching for Unseen Facts (Novelty)</p> <p>Trade-off Exploitation &amp; Exploration</p> <p>Exploitation</p> <ul> <li>State-based Satisfying Planning has multiple reliances \u26a0\ufe0f<ul> <li>heuristics derived from problem</li> <li>plugged into Greedy Best-First Search (GBFS)  </li> <li>extensions (e.g. helpful actions and landmarks)  </li> </ul> </li> <li>Often gets stuck in local minima \u274c<ul> <li>poly + sub-optimal or optimal + NP-hard</li> </ul> </li> </ul> <p>Exploration</p> <ul> <li>Novelty leads to much better performance in practice \u2705</li> <li>Can be model-free (No Reliance/Assumption) \u2705</li> <li>Required for optimal behaviour (in RL and MTCS) \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#novelty","title":"Novelty","text":"<p>Novelty: The size of the smallest subset of facts \\(P \\subseteq s\\), where such \\(s\\) is the first state that makes \\(P\\) <code>TRUE</code> during the search</p> <p>Width: The size of the smallest subset of \\(P\\) needed to be considered to achieve the goal</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#example","title":"EXAMPLE","text":"\\[ p \\to (p, q) \\to (q, r) \\to (p, r) \\to (p, q, r) \\] Current State Smallest New Subset Novelty \\(p\\) \\(p\\) 1 \\(p, q\\) \\(q\\) 1 \\(q, r\\) \\(r\\) 1 \\(p, r\\) \\(p, r\\) 2 \\(p, q, r\\) \\(p, q\\) 2"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#width-based-planning","title":"Width-Based Planning","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#iterated-width-iw","title":"Iterated Width (IW)","text":"<p>\\(IW(k)\\) </p> <ul> <li>BFS on \\((Q \\; \\backslash \\; s), \\mathrm{novelty}(s) &gt; k\\)</li> <li>\\(IW(1) =\\) There is new \\(p\\) appearing in every step in search</li> </ul> <p>\\(IW\\) Algorithm</p> <ul> <li>A sequence of calls \\(IW(k), k=1,2,3,\\ldots\\), until the problem solved or \\(k &gt; len(\\bigcup P)\\) (return <code>unsolvable</code>). </li> <li>The \\(minimum\\) \\(k\\) is the \\(\\text{Width}\\) of the problem</li> </ul> <p>Outcomes: IW</p> <ul> <li>Simple and Blind; No need for heuristic \u2b50\ufe0f</li> <li>Fast if small width \\(\\mathrm{O}(n^k)\\)\u2705<ul> <li>In practical, only \\(IW(k \\le 2)\\) can solve most of classical planning problems. </li> </ul> </li> <li>Optimal if in uniform cost \u2705</li> <li>Complete \u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#serialised-iterated-width-siw","title":"Serialised Iterated Width (SIW)","text":"<p>Use \\(IW\\) for decomposing multi-goals problem and solving sub-problems (with single goal) individually.</p> <pre><code>def SIW(s, G):\n    state = s\n    plan = []\n    for g in serialize(G):\n        subplan = IW(state, goal=g)\n        plan += subplan\n        state = Result(subplan, state)\n    return plan\n</code></pre> <p>Outcomes: SIW</p> <ul> <li>Better performance in Joint Goals problem (Multi goals but similar approaches). \u2705</li> <li>Goals need to be easy to serialise and have low width. \u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#best-first-width-search-bfws","title":"Best-First Width Search (BFWS)","text":"<p>BFWS is a Framework:</p> <ul> <li>Framework: Get communication across researchers and to build on each others\u2019 work</li> <li>BFWS can be used as a framework to implement different measures</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#bfwsf","title":"BFWS(\\(f\\))","text":"<p>Apply Best-first Search on a sequence of measures:</p> \\[ BFWS(f) \\text{ for } f= \\langle w,f_1,f_2,\\ldots \\rangle \\] <p>where</p> <ul> <li>\\(w\\) - Novelty-Measure</li> <li>\\(f_i\\) - tie breakers</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#classical-bfws","title":"Classical BFWS","text":"\\[ f = \\langle w, h \\rangle \\] <p>where</p> <ul> <li>\\(h = h_{add} \\text{ or } h_{FF}\\) </li> <li>\\(w = w_h(s) =\\) \\(s'\\) which has smallest novelty and \\(h(s') = h(s)\\)</li> </ul> <p>Outcomes: Classical BFWS</p> <ul> <li>Still simple but practical \u2705</li> <li>Able to be implemented as a simulator \u2705</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#model-simulator","title":"Model \u2192 Simulator","text":"<p>Model: Describe problems in a compact form. (e.g. STRIPS, PDDL)</p> <p>Simulator: Black-box function that returns only the next state and reward. </p> <ul> <li>No explicit preconditions or effects (no \\(pre(a), add(a), del(a)\\)).  </li> <li>Used by RL / MCTS for forward simulation, not symbolic reasoning.</li> </ul> <p>Model vs Simulator</p> <p>Model</p> <ul> <li>Powerful with development of declarative programming:<ul> <li>Expressive language features easily supported</li> <li>Development of external development tools</li> <li>Fully-Black-Box detailed procedures for higher-level abstraction and reasoning and decomposing problems</li> </ul> </li> <li>Limitations:<ul> <li>Model \\(\\ne\\) Language<ul> <li>Many problems fit Classical Planning model, but hard to express in STRIPS or PDDL</li> </ul> </li> <li>Declarative \\(\\ne\\) Practical<ul> <li>Simulation platforms may be black-boxes as well, they need for planners that work without complete declarative representations</li> </ul> </li> </ul> </li> </ul> <p>Simulator</p> <ul> <li>At the same level of efficiency as classic models</li> <li>Open up exciting possibilities for modelling beyond PDDL (Model-free RL)</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#simulated-bfws","title":"Simulated BFWS","text":"<p>Implement BFWS(\\(f\\)) algorithm to plan for simulators:</p> <ul> <li>Instead of using Transition Function in PDDL or STRIPS, run the simulator and record such transitions:</li> </ul> \\[ s' = \\text{Simulator.step}(s,a) \\] <ul> <li>Choose the best action based on \\(BFWS(\\langle w_h(s), h(s) \\rangle)\\)</li> </ul> Challenges of Width-Based Planning over Simulators <ul> <li>Non-linear dynamics  </li> <li>Perturbation in flight controls  </li> <li>Partial observability  </li> <li>Uncertainty about opponent strategy</li> </ul>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#plan-goal-recognition","title":"Plan &amp; Goal Recognition","text":""},{"location":"Unimelb/25S2/AIP/04-Exploration.html#motivation","title":"Motivation","text":"<p>Folk Psychology</p> <p>Folk Psychology</p> <p>Humans can explain and predict behaviour and mental state of others.</p> <p>To solve hard problems, we may need our planners recognise the \"intention\" and \"motivation\" behind the problem and environment.</p> <p>Common Sense</p> <p>Common sense makes human's initiative possible.</p> <p>A Theory of Common Sense</p> <ul> <li>We always assume that others are rational.</li> <li>We speculate beliefs, goals, reasons of others' actions.</li> <li>We use practical reasoning to assess what others' will do next.</li> </ul> <p>Planners can also use such formalised reasoning system as well.</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#planning-plan-goal-recognition","title":"Planning \u2192 Plan &amp; Goal Recognition","text":"<p>Planning</p> <p>Given a set of goals, find a possible plan to achieve all goals from the current state.</p> <p>Plan &amp; Goal Recognition (PR/GR)</p> <p>Given a list of actions, find most possible goals accounting for such partially observed plan.</p> <p>PR/GR = Planning in reverse</p>"},{"location":"Unimelb/25S2/AIP/04-Exploration.html#formalising-gr","title":"Formalising GR","text":"<ol> <li>Input: Partially Observed Actions + Domain Model</li> <li>For every potential goals \\(g_i\\)<ul> <li>Use classical planner to generate a plan from \\(s_0\\) to \\(g_i\\)</li> <li>Check if such plan accounting for input actions</li> </ul> </li> <li>Output: Select the most reasonable goals and/or in probabilistic distribution form</li> </ol> Transfer to Probabilistic Goals <p>Derive from Bayes' Rule</p> \\[ P(G \\mid O) = \\alpha \\, P(O \\mid G)\\, P(G), \\] <p>where</p> \\[ P(O \\mid G) = \\mathrm{exp}\\!\\left( - \\big(c^{*}(P'[\\neg O \\mid G]) - c^{*}(P'[O \\mid G])\\big) \\right). \\] <ul> <li>\\(c^{*}(P'[\\neg O \\mid G])\\) gives the optimal cost to \\(G\\) NOT accounting for the partial observations \\(O\\)</li> <li>\\(c^{*}(P'[O \\mid G])\\) gives the optimal cost to \\(G\\) accounting for \\(O\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html","title":"5 Reinforced Learning","text":""},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning","title":"Reinforced Learning","text":"<p>Key: Policy + Reward + Trial-and-error Interaction</p>"},{"location":"Unimelb/25S2/AIP/05-RL.html#planning-vs-learning","title":"Planning vs Learning","text":"Dimension Planning Learning Environment Model Model is known Model is unknown Learning Mode Offline computation Online trial-and-error Agent\u2013Environment Interaction No interaction with the real environment; uses internal simulator Must act in the real environment to gather experience Policy Improvement Through search, deliberation,  planning and introspection Through reward-driven learning Suitable Scenarios A precise model exists and simulation is cheap The model is unknown or hard to specify Advantages Safe, interpretable, no real-world risk Adaptive, risky working in complex/unknown environments Disadvantages Requires accurate model; modeling may be expensive Requires exploration; may be costly or risky Example - Atari Game Agent can query emulator for perfect model (source code) Agent can only sees pixels and scores on the screen \u2192 trial-and-error gameplay is necessary"},{"location":"Unimelb/25S2/AIP/05-RL.html#rl-vs-planning-vs-other-ml","title":"RL vs. Planning vs. Other ML","text":"Dimension Reinforcement Learning Automated Planning Other ML Action Outcomes Non-deterministic \u2014 actions lead to probabilistic transitions Deterministic \u2014 outcome fully known from model Usually not modelled as sequential decisions Environment Representation Probabilistic model of states, transitions, and rewards Symbolic or logical model (e.g., STRIPS) No explicit environment Learning Signal Reward Signal - Feedback from environment Predefined goal or planner objective Labels or self-structures Data Structure Sequential / Time series (non-i.i.d.) Discrete steps in a planning domain Often i.i.d. samples (independent &amp; identically distributed) Search &amp; Optimisation Trail-and-error search + Reward-driven state-space search + Predefined policy Gradient-based or statistical fitting Credit Assignment Required \u2014 reward may be delayed over time Not relevant \u2014 goal known a priori Not Required - no delay"},{"location":"Unimelb/25S2/AIP/05-RL.html#example-common-applications","title":"Example - Common Applications","text":"<ul> <li>Making a humanoid robot walk</li> <li>Fine tuning LLMs using human/AI feedback</li> <li>Optimising operating system routines</li> <li>Controlling a power station</li> <li>Managing an investment portfolio</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning-process","title":"Reinforced Learning Process","text":"<pre><code>flowchart LR\n    subgraph Env[Environment]\n        S[(State)]\n        R[(Reward)]\n    end\n\n    subgraph Agent[Agent]\n        P[\"Policy \u03c0(a|s)\"]\n        V[\"(Optional) Value Function\"]\n    end\n\n    S --&gt;|\"(1) Initial State s\"| P\n    P --&gt;|\"(2) Action a\"| Env\n    Env --&gt;|\"(3) Reward r, Next State s'\"| V\n    V --&gt;|\"(4) Update Policy\"| P\n</code></pre>"},{"location":"Unimelb/25S2/AIP/05-RL.html#environments","title":"Environments","text":""},{"location":"Unimelb/25S2/AIP/05-RL.html#state-s-p","title":"State - \\(S, P\\)","text":"<p>All RL Algorithms assume that State is Markov:</p> \\[ P(s' \\mid s + H(s), a) = P(s' \\mid s, a) \\] <pre><code>- Once $S$ is known, $H$ can be thrown away\n</code></pre> <ul> <li>Any RL problem can be made Markov by expanding the state<ul> <li>Add history into the current state</li> <li>Add belief state (POMDP \u2192 belief-MDP)</li> <li>Add latent state (Model-based RL)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reward-r","title":"Reward - \\(R\\)","text":"<ul> <li>A scalar feedback signal</li> <li>Indicates how well agent is doing at one step</li> <li>Reward Hypothesis<ul> <li>All\u00a0goals can be described by the \\(maximisation\\) of expected cumulative reward</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#agents","title":"Agents","text":"<ul> <li>Prediction: Evaluate the future rewards of state-actions<ul> <li>\u2192 Value Function</li> </ul> </li> <li>Control: Find the optimal policy<ul> <li>\u2192 Policy Function</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#model-latent-p-r","title":"Model - Latent \\(P, R\\)","text":"<ul> <li>An internal simulator for predicting what the environment will do next<ul> <li>Transition model: \\(P_{ss'}^a\\) - how each action changes the state</li> <li>Reward model: \\(R_{s}^a\\) - immediate reward from each state</li> <li>The model can be imperfect but supports planning and prediction</li> </ul> </li> <li>Simulation is NOT necessary for RL <ul> <li>\u2192 Model-based/Model-free</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#value-function-v-q","title":"Value Function - \\(V, Q\\)","text":"<ul> <li>Define and predict values of states based on the expectation of future rewards</li> <li> <p>State-value Function</p> <ul> <li>Output the value of current state     $$     V_\\pi(s) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t = s\\big]     $$</li> </ul> </li> <li> <p>State-action-value Function</p> <ul> <li>Output the value of the current state with a deterministic action applied     $$     Q_\\pi(s,a) = \\mathbb{E}_\\pi \\big[\\, G_t \\mid S_t = s,\\, A_t = a \\,\\big]     $$</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#return-g_t","title":"Return - \\(G_t\\)","text":"<ul> <li>total discounted reward of the future     $$     G_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma (R_{t+3} + \\ldots)) = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}     $$</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#bellman-expectation-equation","title":"Bellman Expectation Equation","text":"<ul> <li> <p>Bellman Equation: Gives a recursive form of Return         $$ G_t = R_{t+1} + \\gamma G_{t+1} $$</p> <ul> <li>Based on \"look-ahead then back-up\" computational mechanism<ul> <li>Look-ahead: to the next step \u2192 \\(r, s'\\)</li> <li>Back-up: to the current state \u2192 \\(v(s) = f(r,s')\\)</li> </ul> </li> </ul> </li> <li> <p>State-value Function (Deriving BEE)</p> </li> </ul> \\[ \\begin{aligned} v_\\pi(s) &amp;= \\mathbb{E}\\!\\left[r + \\gamma v_\\pi(s')\\right] \\\\[6pt] &amp;= \\sum_a \\pi(a\\mid s)\\Bigl[\\sum_r p(r\\mid s,a)\\,r + \\gamma \\sum_{s'} p(s'\\mid s,a) \\, v_\\pi(s')\\Bigr] \\end{aligned} \\] <ul> <li>State-action-value Function (Deriving BEE)</li> </ul> \\[ \\begin{aligned} q_\\pi(s,a) &amp;= \\mathbb{E}\\!\\left[\\, r + \\gamma\\, \\mathbb{E}_{a'\\sim\\pi(\\cdot\\mid s')} \\bigl[q_\\pi(s',a')\\bigr] \\right] \\\\[6pt] &amp;= \\sum_{s',r} p(s',r\\mid s,a)\\left[ r + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\, q_\\pi(s',a') \\right] \\end{aligned} \\] <ul> <li> <p>Solving the BEE</p> <ul> <li> <p>Directly Compute \\(O(n^3)\\)     $$     v = R + \\gamma Pv \\to v = (I - \\gamma P)^{-1}R     $$</p> <ul> <li>only possible for small \\(P\\) matrix</li> </ul> </li> <li> <p>Dynamic Programming</p> </li> <li>Monte-Carlo Evaluation</li> <li>Temporal-Difference Learning</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#discount-factor-gamma","title":"Discount Factor\u00a0- \\(\\gamma\\)","text":"Gamma (\\(0 \\le \\gamma \\le 1\\)) Behaviour \\(\\gamma = 0\\) Greedy \\(\\gamma \\to 0\\) Myopic \\(\\gamma \\to 1\\) Far-sighted \\(\\gamma = 1\\) Guarantee only if all sequence terminate Why Discounting is Used <p>Technical Reasons</p> <ul> <li>Makes modelling and computation easier (Bellman equations converge cleanly)</li> <li>Prevents infinite returns in cyclic Markov processes</li> <li>Reflects uncertainty about far-future outcomes</li> </ul> <p>Realistic Reasons</p> <ul> <li>In financial settings, immediate rewards can be reinvested (time value of money)</li> <li>Human and animal behaviour shows preference for immediate rewards over delayed rewards</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#policy-pi","title":"Policy - \\(\\pi\\)","text":"<p>Fully defines agent's behaviour</p> <ul> <li>Stationary(Time-independent): Only relies on the current state</li> </ul>"},{"location":"Unimelb/25S2/AIP/05-RL.html#reinforced-learning-algorithms","title":"Reinforced Learning Algorithms","text":"<ul> <li>Markov Decision Processing<ul> <li>Value Iteration</li> <li>Policy Iteration</li> </ul> </li> </ul> <p>Model-based \u2192 Model-free</p> <ul> <li>Monte Carlo</li> </ul> <p>Non-incremental \u2192 Incremental</p> <ul> <li>Temporal Difference<ul> <li>SARSA</li> <li>Q-Learning</li> </ul> </li> </ul> <p>Tabular representation \u2192 Function representation</p> <ul> <li>Value Function Approximation</li> </ul> <p>Deterministic \u2192 Stochastic Policy</p> <ul> <li>Finite Difference</li> <li>Policy Gradient<ul> <li>REINFORCE</li> <li>Actor-Critic</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html","title":"6 Markov Decision Process","text":""},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-decision-process","title":"Markov Decision Process","text":"<p>General Simulator of the environment for Model-based RL</p>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-process","title":"Markov process","text":"<p>A memoryless\u00a0random process </p> <ul> <li>Memoryless: the state is Markov</li> <li>Introduce non-deterministic in terms of probabilistic transition \\(P\\)</li> </ul> \\[ M = \\langle S, P \\rangle \\] <p>where</p> <ul> <li>\\(S\\) - a finite set of states</li> <li>\\(P\\) - a matrix showing state transition probability<ul> <li>\\(P(s \\to s') = P(s' \\mid s), \\forall s, s' \\in S\\)</li> <li>Each row/column of \\(P\\) sums to 1</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-reward-processes-mrp","title":"Markov Reward Processes (MRP)","text":"<p>A Markov chain with reward values:</p> \\[ M = \\langle S, P, R, \\gamma \\rangle \\] <p>where</p> <ul> <li>\\(R\\) - the reward function<ul> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul> </li> <li>\\(\\gamma\\) - discount factor</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>Introduce agency in terms of actions:</p> \\[ M = \\langle S, A, P, R, \\gamma \\rangle \\] <ul> <li>\\(A\\) - a finite set of actions</li> <li>\\(P(s \\to s) = P(s' \\mid s)\\)</li> <li>\\(R(s) = \\mathbb{E}[R' \\mid s]\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#partially-observable-mdp-pomdp","title":"Partially Observable MDP (POMDP)","text":"<p>Assumption</p> <ul> <li>Markov Property</li> <li>Limited Observable</li> </ul> <p>Components (Diff with MDP)</p> <ul> <li>Introduce Sensor Model (based on Probability Distribution)</li> </ul> \\[ \\mathrm{b}(s_t) = P(s_t \\mid \\text{historical actions and observations}) \\] <ul> <li>Initial/Goal States \u2192 Belief States \\(\\mathrm{b}(s_0)\\) and \\(\\mathrm{b}(g)\\)</li> </ul> <p>Outcomes</p> <ul> <li>Map belief states into actions</li> <li>Optimal if total expected cost from \\(\\mathrm{b}(s_0)\\) to \\(\\mathrm{b}(g)\\) is minimum</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#greedy-optimal-policy","title":"Greedy Optimal Policy","text":"<p>Give a policy with \\(maximum\\) state-action-value</p> <p>Optimal State-value Function</p> \\[ V^\\ast(s) = \\max_\\pi V_\\pi(s) \\] <p>Optimal State-action-value Function</p> \\[ Q^\\ast(s,a) = \\max_\\pi Q_\\pi(s,a) \\] <p>Optimal Policy</p> \\[ a=\\arg\\max_a Q^\\ast(s,a) \\]"},{"location":"Unimelb/25S2/AIP/06-MDP.html#bellman-optimality-equation","title":"Bellman Optimality Equation","text":"<p>Derived from the Bellman Equation, we can use \"immediate reward + discounted return of next state\" to express the expected return:</p> <p>Optimal State-value Function</p> \\[ \\begin{align} V^\\ast(s) &amp;= \\max_a \\; \\mathbb{E}\\!\\left[\\, r + \\gamma V^\\ast(s') \\,\\right] \\\\[6pt] &amp;= \\max_a \\; \\sum_{s'} P(s' \\mid s,a)\\left[\\, r + \\gamma V^\\ast(s') \\,\\right] \\end{align} \\] <p>Optimal State-action-value Function</p> <p>Select the action leading to \\(maximum\\) state-action-value of the next state:</p> \\[ \\begin{align} Q^\\ast(s,a) &amp;= \\mathbb{E} \\left[\\, r + \\gamma \\max_{a'} Q^\\ast(s',a') \\,\\right] \\\\[6pt] &amp;= \\sum_{s',r} P(s',r\\mid s,a)\\left[\\, r + \\gamma \\max_{a'} Q^\\ast(s',a') \\,\\right] \\end{align} \\] How to solve the Bellman Optimality Equation  <ul> <li>No closed form solution</li> <li>Value Iteration</li> <li>Policy Iteration</li> <li>SARSA</li> <li>Q-learning</li> </ul>"},{"location":"Unimelb/25S2/AIP/06-MDP.html#example-maze","title":"Example - MAZE","text":"<p>Environment:</p> <ul> <li>\\(S\\) - Agent's possible locations</li> <li>\\(A\\) - Step directions \\(\\mathtt{N, E, S, W}\\)</li> <li>\\(P\\) - Map \\(s, a \\to s'\\)</li> <li>\\(R\\) - \\(-1\\) per time-step (encourage short-path solution)</li> </ul> <p>Agent:</p> <ul> <li>\\(V(s)\\) - the expected return of following the policy from each \\(s\\) <ul> <li>closer to \\(g\\) - \\(V(s)\\) \u2934</li> <li>Farther away - \\(V(S)\\) \u2935</li> </ul> </li> <li>\\(\\pi(s)\\) - Optimal Policy = Best \\(V(s')\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html","title":"7 Monte Carlo","text":""},{"location":"Unimelb/25S2/AIP/07-MC.html#model-based-model-free","title":"Model-based \u2192 Model-free","text":"<p>Model-based</p> <ul> <li>Learn from Simulator of Environment<ul> <li>Commonly use MDP to model the Markov Environment</li> </ul> </li> </ul> <p>Model-free</p> <ul> <li>Learn Experience from Environment<ul> <li>Experience: \\(E_t = \\langle S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, ... \\rangle\\)</li> </ul> </li> <li>Optimise \\(V, Q, \\pi\\) directly</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#monte-carlo-mc-learning","title":"Monte Carlo (MC) Learning","text":"<ol> <li>Run-to-end in the environment for multiple times</li> <li>Use Average Actual Return to estimate value function</li> </ol> \\[ V(s) \\approx \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)} \\] <p>where</p> <ul> <li>\\(N(s)\\) - how many times \\(G_t\\) is visited</li> <li>When \\(N(s) \\to \\infty\\), \\(V(s) \\to v_\\pi(s)\\).</li> </ul> <p>Outcomes: Monte Carlo</p> <ul> <li>Unbiased\u2705</li> <li>Can only apply to episodic (terminating) Markov environment\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-policy-iteration","title":"MC Policy Iteration","text":"<p>Directly transfer Policy Iteration into a Monte-Carlo version.</p> <p>Algorithm</p> <ul> <li>For each iteration:<ol> <li>Policy Evaluation (Predication)<ul> <li>For each \\((s, a)\\):<ul> <li>run sufficiently many episodes and get average return</li> <li>When \\(N \\to \\infty\\), \\(Q(s, a) \\to q_\\pi(s, a)\\)</li> </ul> </li> </ul> </li> <li>Policy Improvement (Control)<ul> <li>Give the Greedy Optimal Policy based on estimated \\(Q\\)<ul> <li>\\(a^\\ast = \\arg\\max_a q_\\pi(s,a)\\)</li> </ul> </li> </ul> </li> </ol> </li> </ul> <p>Outcomes: MCPI</p> <ul> <li>Convergence guarantee\u2705 </li> <li>Very low Time Efficiency; Not practical\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-exploring-starts","title":"MC Exploring Starts","text":"<p>Generalised Policy Iteration (GPI): Repeatedly interleave evaluation and improvement, instead of doing them once.</p> <p>Exploring Starts: Randomly select start points to avoid missing the optimal episode.</p> <p>Algorithm</p> <ul> <li>Repeat:<ul> <li>Start from a random \\((s_0, a_0)\\)</li> <li>For each \\((s,a)\\) visited:<ul> <li>\\(N(s,a) \\gets N(s,a) + 1\\)</li> <li>Update \\(Q(s,a) \\gets \\frac{1}{N(s,a)}(Q(s,a) + (G - Q(s,a))\\)</li> <li>If \\(Q\\) is updated, update \\(a^\\ast\\)</li> </ul> </li> </ul> </li> </ul> <p>Outcomes: ES-MC</p> <ul> <li>Data is used much more efficiently</li> <li>Exploring Start can highly guarantee that every node is visited</li> <li>However difficult in practice (limitation in environment)</li> </ul>"},{"location":"Unimelb/25S2/AIP/07-MC.html#first-visit-vs-every-visit","title":"First-visit vs. Every-visit","text":"<p>Different ways to handle repeated visits of the same \\((s,a)\\) pair during an episode:</p> Aspect First-Visit MC Every-Visit MC When? Only the first time visit Every time visit Pros Avoids dependence between multiple visits Faster convergence in practice Cons Some visits ignored Higher variance Suitable Scenario Long episodes with repeated states Short episodes or rare-state situations"},{"location":"Unimelb/25S2/AIP/07-MC.html#mc-greedy","title":"MC + \u03b5-greedy","text":"<p>Use a soft policy (e.g., \u03b5-greedy) to avoid the need for exploring starts.</p> \\[ \\pi(a|s) = \\begin{cases} 1 - (1 - \\frac{1}{|\\mathcal{A}(s)|}) \\, \\varepsilon, &amp; a = a^*_k \\\\[6pt] \\frac{1}{|\\mathcal{A}(s)|} \\, \\varepsilon, &amp; \\text{otherwise} \\end{cases} \\] <p>where</p> <ul> <li>\\(|\\mathcal{A}(s)|\\) - number of valid actions of state</li> <li>Balance Exploration &amp; Exploitation</li> </ul> <p>Outcomes: \u03b5-greedy MC</p> <ul> <li>Ensures continual exploration by assigning non-zero probability to all actions. \u2705</li> <li>More stable than pure greedy MC because it prevents being trapped in suboptimal actions early. \u2b50\ufe0f</li> <li>Slight performance trade-off: exploration introduces variance, but improves long-term value estimation and policy quality. \ud83d\udcad</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html","title":"8 Temporal Difference","text":""},{"location":"Unimelb/25S2/AIP/08-TD.html#non-incremental-incremental","title":"Non-incremental \u2192 Incremental","text":"<p>Non-incremental Methods(Batch)</p> <ul> <li>Parameters updating per a batch of samples (a episode)</li> <li>Solving the optimal problem at once</li> <li>Offline Learning <ul> <li>Can handle high stochastic/batch environment\u2705</li> <li>Large space complexity\u274c </li> </ul> </li> <li>Closed-form<ul> <li>High computation cost\u274c</li> </ul> </li> </ul> <p>Incremental Methods</p> <ul> <li>Parameters updating per one sample (a step)</li> <li>Step by step approaching the optimum</li> <li>Online Learning \u2192 One sample use once\u2705</li> <li>Higher generalisation\u2705</li> <li>Lower convergence; may can only approach local optima\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>TD is the most basic incremental method based on Bootstrapping.</p> <p>Bootstrapping: Use estimation of future values to update the current value.</p> \\[ \\begin{align} \\text{TD-target} &amp; = \\text{sample} + \\gamma \\cdot \\text{bootstrap} \\\\ &amp; = \\text{immediate reward} + \\gamma \\cdot \\text{future value} \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#prediction-of-state-value","title":"Prediction of State Value","text":"<p>For each step, if \\(s_t\\) is visited, update its value by looking forward to the reward of next step(s).</p> <ul> <li>CANNOT estimate action values or optimal policies \u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#td0","title":"TD(0)","text":"<p>Look into the next \\(1\\) step:</p> \\[ \\begin{align} V(s_t)  &amp;\\gets V(s_t) + \\alpha \\big({{G_t^{(1)} - V(s_t))}} \\\\[2pt] &amp;= V(s_t) + \\alpha(R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\end{align} \\] <p>where</p> <ul> <li>\\(\\vec{v} = R_{t+1} + \\gamma V(s_{t+1})\\) - TD Target</li> <li>\\(\\delta = \\vec{v} - V(s_t)\\) - TD Error<ul> <li>Reflects the difference between \\(v_t\\) and \\(v_\\pi\\)</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#tdn","title":"TD(n)","text":"<p>Look into \\(n-1\\) steps:</p> \\[ G^{(n)}_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + {{\\gamma^n V(S_{t+n})}} \\] \\[ \\begin{align*} \\color{red}{n} &amp; \\color{red}{= 1}\\ \\text{(TD(0))}      &amp; G_t^{(1)}      &amp; = R_{t+1} + \\gamma V(S_{t+1})\\\\[2pt] \\color{red}{n} &amp; \\color{red}{= 2}                   &amp; G_t^{(2)}      &amp; = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})\\\\[2pt] \\color{red}{\\vdots} &amp;                   &amp; \\vdots         &amp; \\\\[2pt] \\color{red}{n} &amp; \\color{red}{= \\infty}\\ \\text{(MC)} &amp; G_t^{(\\infty)} &amp; = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_T \\end{align*} \\] <p>Update in direction of error:</p> \\[ V(S_t) \\;\\gets\\; V(S_t) + \\alpha \\big({{G_t^{(n)} - V(S_t)}} \\big) \\] How to choose a good \\(n\\) <ul> <li>Root Mean Square (RMS) Errors<ul> <li>change \\(\\alpha\\) \u2192 vary RMS Errors \u2192 different optimal \\(n\\)</li> </ul> </li> <li>Large \\(n\\) \u2192 More rely on exploitation, more precise but higher variance</li> <li>Small \\(n\\) \u2192 More rely on prediction, faster but higher bias</li> <li>If \\(n = \\infty\\) , it turns into MC basic</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#td","title":"TD(\u03bb)","text":"<p>Average n-Steps Returns - use weight \\({{(1 - \\lambda)\\lambda^{\\,n-1}}}\\) to balance short-term and long-term rewards</p> \\[ G_t^{\\lambda} \\;=\\; {{(1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{\\,n-1}}} G_t^{(n)} \\] \\[ V(S_t) \\;\\leftarrow\\; V(S_t) + \\alpha \\big( G_t^{\\lambda} - V(S_t) \\big) \\] <p>Outcomes: TD(\u03bb)</p> <ul> <li>\u2705Memoryless - i.e space complexity is same as TD(0)</li> <li>\u26a0\ufe0fForward View - We cannot look into the far future during learning\ud83d\udc47</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#backward-view","title":"Backward View","text":"<p>Update weight step by step by using Eligibility Trace:</p> <ul> <li>Initially set traces of all states \\(E_0(s) \\gets 0\\)</li> <li> <p>In each step:</p> <ul> <li> <p>For each state:</p> <ul> <li>reduce traces value by \\(\\gamma \\lambda\\)</li> <li>\\(E_t(s)+1\\) if \\(s\\) is currently visited</li> </ul> \\[ E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}(S_t = s) \\] </li> <li> <p>Compute TD Error</p> \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\] </li> <li> <p>Update Value Function</p> \\[ V(s) \\gets V(s) + \\alpha \\, \\delta_t \\, E_t(s) \\] </li> </ul> </li> </ul> <p>At the end of episodes: Backward View = Forward View</p> \\[ \\sum_{t=1}^T \\alpha \\, \\delta_t \\, E_t(s) = \\sum_{t=1}^T \\alpha (G_t^\\lambda  - V(S_t)) \\; \\mathbf{1}(S_t = S) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#prediction-of-action-value","title":"Prediction of Action Value","text":"<p>Sarsa : \"State-action-reward-state-action\" Algorithm</p> <p>A state-action function version of TD</p> \\[ Q(s_t,a_t) \\gets Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#sarsa","title":"Sarsa","text":"<ul> <li>For each episode:<ul> <li>For each state:<ul> <li>If \\(s_t \\ne g\\):<ul> <li>Collect the experience \\(\\{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\\}\\)<ul> <li>\\(a_t\\) \u2190 \\(\\pi(a \\mid s_t)\\)</li> <li>\\(r_{t+1}, s_{t+1}\\)  \u2190 Environment</li> <li>\\(a_{t+1}\\)  \u2190 \\(\\pi(a \\mid s_{t+1})\\)</li> </ul> </li> <li>Update \\(q(s_t, a_t)\\)</li> <li>Update \\(\\pi\\) based on policy iteration</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#n-step-sarsa","title":"\\(n\\)-step Sarsa","text":"<p>Sarsa version of TD(n)</p> \\[ q_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q(S_{t+n}) \\] \\[ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t)   + \\alpha \\Big( q_t^{(n)} - Q(S_t, A_t) \\Big) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#sarsalambda","title":"Sarsa(\\(\\lambda\\))","text":"<p>Sarsa version of TD(\\(\\lambda\\))</p> \\[ q_t^\\lambda \\;=\\; (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} q_t^{(n)} \\] <ul> <li> <p>Forward View: $$ Q(S_t, A_t) \\;\\leftarrow\\; Q(S_t, A_t) + \\alpha \\Big( q_t^\\lambda - Q(S_t, A_t) \\Big) $$</p> </li> <li> <p>Backward View:</p> </li> </ul> \\[ \\begin{align*} E_0(s,a) &amp; = 0 \\\\[0pt] E_t(s,a) &amp; = \\gamma \\lambda E_{t-1}(s,a) + \\mathbf{1}(S_t = s, A_t = a) \\end{align*} \\] \\[ \\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\] \\[ Q(s,a) \\;\\leftarrow\\; Q(s,a) + \\alpha \\,\\delta_t \\, E_t(s,a) \\]"},{"location":"Unimelb/25S2/AIP/08-TD.html#off-policy-control","title":"Off-Policy Control","text":"<p>Off-policy: Distinguish Target Policy and Behaviour (Sampling) Policy</p> <ul> <li>\u2705Take use of experience from other policies</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#on-policy-off-policy","title":"On-policy \u2192 Off-policy","text":"<p>Sarsa (On-policy)</p> <ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_t(s_t)\\) and \\(a_{t+1}\\) from \\(\\pi_t(s_{t+1})\\)</li> <li>Target Policy = Behaviour Policy = \\(\\pi_t\\) </li> </ul> <p>Q-Learning (Off-policy)</p> <ul> <li>Target: \\(\\vec{v} = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} q_t(s_{t+1}, a)\\)</li> <li>Sample: Generate \\(a_t\\) from \\(\\pi_b(s_t)\\)</li> <li>Target Policy = Greedy Optimal \\(q_t\\)</li> <li>Behaviour Policy = Any policies (\\(\\pi_b\\))</li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#q-learning","title":"Q-Learning","text":"<ul> <li>For each episode generated by \\(\\pi_b\\):<ul> <li>For each time step:<ul> <li>Update q-value<ul> <li>\\(q_{t+1}(s_t, a_t) \\gets q_t(s_t, a_t) + \\alpha \\bigl(  \\vec v - q_t(s_t, a_t) \\bigr)\\)</li> </ul> </li> <li>Update target policy<ul> <li>\\(a = \\arg\\max_a q_{t+1} (s_t, a)\\)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/08-TD.html#mc-vs-td","title":"MC vs. TD","text":"Aspect Monte-Carlo (MC) Temporal-Difference (TD) Learning timing Offline - Update after the episode ends Online - Update immediately after receiving a reward Environment Assumption episodic  only both episodic and continuing Target used \\(\\vec{v} = G_t\\) \\(\\vec{v} = R_{t+1} + \\gamma V(S_{t+1}\\)) Bootstrapping? \u274c \u2705Value Updating relies on the previous estimate of this value (require initial guesses) Variance \u26a0\ufe0fHigh - Samples of lots of variables \u2705Low Bias Unbiased (return is true target) Biased (bootstraps using V(s\u2032)) Exploits Markov property? \u274c \u2705 Efficiency in Markov environment \u26a0\ufe0fLess efficient \u2705More efficient Efficiency in non-Markov or PMDP env \u2705More efficient with backup learning \u26a0\ufe0fLess efficient Optimisation in limited experience (Batch) Average Return = minimise MSE Fit the most likelihood Markov model that best explains the data"},{"location":"Unimelb/25S2/AIP/08-TD.html#example-driving-home","title":"Example - Driving Home","text":"State Elapsed Time (min) Predicted Time to Go Predicted Total Time MC Update (After Back Home) MC New Predicated Time TD(0) Update - \\(\\gamma = 1, \\alpha = 0.5\\) TD New Predicated Time leaving office 0 30 30 43 43-0=43 30+0.5(5+35-30)=35 35-0=35 reach car, raining 5 35 40 43 43-5=38 40+0.5(20+15-40)=37.5 37.5-5=32.5 exit highway 20 15 35 43 43-20=23 35+0.5(30+10-35)=37.5 37.5-20=17.5 behind truck 30 10 40 43 43-30=13 40+0.5(40+3-40)=41.5 41.5-30=11.5 home street 40 3 43 43 43-40=3 43+0.5(43+0-43)=43 43-40=3 arrive home 43 0 43 43 43-43=0 43 43-43=0"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html","title":"9 Value Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#tabular-representation-function-representation","title":"Tabular Representation \u2192 Function Representation","text":"<p>Tabular Presentation:</p> <ul> <li>All values/policies are discrete and stored in a table</li> <li>We update values by change it directly</li> </ul> <p>Function Presentation:</p> <ul> <li>We construct a function for fitting the value distribution</li> <li>We update value by tuning parameters of the function</li> </ul> Aspect Tabular Function Approx. Interpretability \u2705Intuitive \u274cHard to interpret Scalability \u274cDifficult for large/continuous spaces \u2705Much fewer memories required Generalisability \u274cPoor \u2705Generalises from seen situations to unseen situations Bias \u2705Accurate when state space is small \u26a0\ufe0fBiased - State values cannot be represented accurately"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#function-approximation","title":"Function Approximation","text":"<p>Approximate value function by constructing a new function and tuning parameters \\(w\\).</p> <ul> <li>instead of a table (\\(s\\)-\\(a\\)) of discrete values (tabular)</li> </ul> \\[ \\begin{align} \\hat{v}(s, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; v_{\\pi}(s)\\\\[0pt] \\;\\;\\;\\hat{q}(s, a, {\\color{red}{\\mathbf{w}}})\\; &amp; {\\color{red}{\\approx}}\\; q_{\\pi}(s, a) \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#object-function","title":"Object Function","text":"<p>Goal: \\(minimise\\) MSE between approx function and true value function.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_\\pi \\left[ (v_\\pi(S) - \\hat{v}(S, \\mathbf{w}))^2 \\right] \\] <p>where \\(S \\in \\mathcal{S}\\) - the state is a Random Variable which follows the probability distribution, e.g.:</p> <ul> <li>Uniform Distribution<ul> <li>All with the same probability</li> <li>\\(S = \\frac{1}{|\\mathcal{S}|}\\)</li> <li>\u274cToo simple; not practical</li> </ul> </li> <li>Stationary Distribution<ul> <li>Describe the long-term behaviour of a Markov process<ul> <li>\\(d_\\pi(s) \\approx \\frac{N(s)}{\\sum_{s \\in \\mathcal{S}} N(s)}\\)</li> </ul> </li> <li>Which can also be predicted by using \\(P\\) from a model (MDP)<ul> <li>\\(d_\\pi = d_\\pi P\\)</li> </ul> </li> <li>\u2705Stationary; can be learned</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> \\[ w_{k+1} = w_k - \\alpha \\nabla J(w_k) \\] <p>\\(\\nabla J(w_k)\\) can be computed by:</p> <ul> <li>Ture Gradient Descent<ul> <li>need to calculate the expectation (collect all \\(N\\) samples), not practical\u274c</li> </ul> </li> </ul> \\[ \\begin{align} \\nabla J(w) &amp; = \\nabla \\mathbb{E} \\left[ (v_\\pi(S) - \\hat{v}(S, w))^2 \\right] \\\\[0pt] &amp; \\propto \\mathbb{E}[(v_\\pi(S)-\\hat{v}(S,w))\\,\\nabla\\hat{v}(S,w)] \\end{align} \\] <ul> <li>Stochastic Gradient Descent<ul> <li>Use only 1 sample \\(s_t\\) </li> <li>Online update, better generalisation\u2705</li> </ul> </li> </ul> \\[ \\nabla J(w) \\approx \\bigl(v_\\pi(s_t) - \\hat{v}(s_t,w_t)\\bigr) \\, \\nabla \\hat{v}(s_t,w_t) \\] <p>For remaining two terms:</p> <ul> <li>\\(\\nabla\\hat{v}(s_t,w_t)\\) \u2190 Feature Vectors (Linear)</li> <li>\\(v_\\pi(s_t)\\) \u2190 Value Function Approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#approximate-the-value-function","title":"Approximate the Value Function","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#feature-vectors","title":"Feature Vectors","text":"<p>Represent the state by a feature of vectors \\(\\mathbf{x}(s)\\), so that in linear case:</p> <ul> <li>State-value Gradient:</li> </ul> \\[ \\nabla\\hat{v}(s,w) = \\frac{\\partial \\hat v(s)}{\\partial w} = \\mathbf{x}(s) \\] <ul> <li>State-action-value Gradient:</li> </ul> \\[ \\nabla \\hat q(s,a,w) = \\mathbf{x}(s,a) \\] <ul> <li>Intuitive; easy to implement; high interpretability\u2705</li> <li>Difficult to select appropriate feature vectors\u26a0\ufe0f</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#state-value-approximator","title":"State-value Approximator","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-monte-carlo","title":"Derive from Monte Carlo","text":"<p>Use \\(g_t\\) (discounted return starting from \\(s_t\\)) to approximate:</p> \\[ v_\\pi(s_t) = g_t \\] <ul> <li>converges to a local optimum, even for\u00a0non-linear approximation</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-temporal-difference","title":"Derive from Temporal Difference","text":"<p>Use \\(r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) - \\hat{v}(s_t, w_t)\\) (TD Error) to update approximator at each step:</p> \\[ v_\\pi(s_t) = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t) \\] <ul> <li>Linear TD(0) converges close to the global optimum</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-td","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ v_\\pi(s_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat v(s_{t+1}, w_t)-\\hat{v}(s_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{v}(s_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#action-state-value-approximator","title":"Action-state-value Approximator","text":"<p>Generally change \\(v\\) to \\(q\\) :</p> \\[ w_{k+1} = w_k  - \\alpha\\, \\bigl(q_\\pi(s_t,a_t)-\\hat{q}(s_t,a_t,w_t)\\bigr)\\, \\nabla \\hat{q}(s_t,a_t,w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-monte-carlo_1","title":"Derive from Monte Carlo","text":"\\[ q_\\pi(s_t,a_t) = g_t \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-sarsa","title":"Derive from Sarsa","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\hat{q} (s_{t+1}, a_{t+1}, w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-td_1","title":"Derive from TD(\u03bb)","text":"<ul> <li>Forward View:</li> </ul> \\[ q_\\pi(s_t, a_t) = g_t^\\lambda \\] <ul> <li>Backward View: </li> </ul> \\[ \\begin{align} \\delta_t &amp; = r_{t+1} + \\gamma \\hat q(s_{t+1}, a_{t+1}, w_t)-\\hat{q}(s_t, a_t, w_t) \\\\[4pt] E_t &amp; = \\gamma \\lambda E_{t-1} + \\nabla\\hat{q}(s_t,a_t,w_t) \\\\[4pt] w_{k+1} &amp;= w_k - \\alpha \\, \\delta_t \\, E_t \\end{align} \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#derive-from-q-learning","title":"Derive from Q-learning","text":"\\[ q_\\pi(s_t,a_t) = r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}(s_{t+1})}\\hat{q} (s_{t+1}, a, w_t) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#batch","title":"Batch","text":"<p>Restricted Environment: Only limited and static samples can algorithms learn from</p> \\[ \\mathcal{D} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_n, v^\\pi_n \\rangle\\} \\] <p>Distribution Shift: These samples may generate by a sub-optimal policy, so that: - Data may be out of distribution - Q-value may be over-estimated - Overfitting</p> <p>Potential Suitable Methods: - Function Approximation \u2192 generalisability - Off-policy (e.g. Q-Learning) \u2192 static sample set - Supervised Learning Methods (e.g. Neural Network) \u2192 static sample set</p>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#example-mc-td-in-a-batch","title":"Example - MC &amp; TD In a Batch","text":"<p>Sampled Episodes: 8 Samples (Limited)</p> \\[ \\begin{align} A \\to 0, B \\to 0 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 1 \\\\ B \\to 0 \\end{align} \\] <p>MC Update:</p> <ul> <li>\\(v(B) = 6/8 = 0.75\\)</li> <li>\\(v(A) = 0 / 1 = 0\\)</li> </ul> <p>TD Update:</p> <ul> <li>\\(v(B) \\to 0.75\\)</li> <li>\\(v(A) = 0 + v(B) \\to 0.75\\)</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#batch-methods","title":"Batch Methods","text":""},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#least-squares-solution","title":"Least Squares Solution","text":"<p>By \\(minimising\\) SSE between approximate and true values:</p> \\[ LS(w_i) = \\mathbb{E} \\left[ (v^\\pi(S) - \\hat{v}(S, w))^2 \\right] \\] <ul> <li>True values can be approximated by Return (MC) or Temporal Difference (TD)</li> <li>Using all samples at once</li> <li>\\(O(n^3)\\) or \\(O(n^2)\\) by using \"Sherman-Morrison\"</li> </ul>"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>By randomly sample a mini-batch of samples from \\(\\mathcal{D}\\):</p> \\[ \\mathcal{B} = \\{\\langle s_1, v^\\pi_1 \\rangle, \\ldots, \\langle s_k, v^\\pi_k \\rangle\\} \\] <p>Then use gradient descent for updating:</p> \\[ w_{k+1} = w_k - \\alpha\\, \\frac{1}{m} \\sum_{\\langle s_i, v^\\pi_i \\rangle \\in \\mathcal{B}} \\bigl( v^\\pi_i(s_i) - \\hat{v}(s_i, w_i) \\bigr) \\nabla \\hat{v}(s_i, w_i) \\]"},{"location":"Unimelb/25S2/AIP/09-Value-Approx.html#deep-q-learning","title":"Deep Q-Learning","text":"<p>By using Q-Learning + Neural Network (\\(minimise\\) MSE \\(\\mathbf{Loss}(\\cdot)\\)): </p> \\[ \\mathbf{Loss}(w_i) =\\underbrace{\\mathbb{E}_{(s,a,r,s')\\sim \\mathcal{D}_i}}_{\\text{sampled from}\\;D_i} \\Big[ \\big( \\underbrace{r+\\gamma \\max_{a'} Q(s',a';w_i^-)}_{\\text{TD target }} - \\underbrace{Q(s,a;w_i)}_{\\text{Q estimate}} \\big)^2 \\Big] \\] <ul> <li>Experience replay: <ul> <li>Randomly sample from non-i.i.d data \u2192 single sample/mini-batch</li> <li>\u2705For decorrelation (near-i.i.d)</li> </ul> </li> <li>Fixed Q-targets: <ul> <li>\\(w_i^-\\) will only be updated by \\(w_i\\) (lively updated) after running a while (like 1000 steps)</li> <li>\u2705Reduce variance</li> </ul> </li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html","title":"10 Policy Function Approximation","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#value-based-policy-based","title":"Value-based \u2192 Policy-based","text":"<p>In Value-based Algorithms:</p> <ul> <li>A policy is optimal if it can \\(maximise\\) every state value</li> </ul> \\[ \\pi^\\ast = \\arg\\max_\\pi v_\\pi(S) \\] <ul> <li>We store all \\(v_\\pi(S)\\) in a tabular based on different \\(\\pi\\) so that we can choose best policy from them.</li> </ul> <p>In Policy-based Algorithms:</p> <ul> <li>We construct a model of \\(\\pi\\) and optimise by tuning its parameters \\(\\theta\\).</li> </ul> \\[ \\pi^\\ast = \\pi(a\\mid S,\\theta) \\] <p>Outcomes of Policy-Based Methods</p> <p>Advantages</p> <ul> <li>High-level function approximation  </li> <li>Much more space-efficient (no huge tabular, just a function with features/variables)  \u2b50\ufe0f</li> <li>Better generalisation across large/continuous state spaces \u2705 </li> <li>Naturally supports stochastic policies \u2705</li> </ul> <p>Disadvantages</p> <ul> <li>May converge only to a local optimum  \u26a0\ufe0f</li> <li>Policy evaluation suffers from high variance \u26a0\ufe0f</li> <li>Monte-Carlo returns require full trajectory sampling \u2192 inefficient </li> <li>Difficult to use off-policy \u274c</li> <li>Updating \\(\\theta\\) changes the policy \u2192 changes the data distribution  </li> <li>Requires resampling under the new policy (no reuse of old data)</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#policy-gradient","title":"Policy Gradient","text":"<ol> <li>Use a object function to define optimal policies: \\(J(\\theta\\))</li> <li>Use gradient-based optimisation to search for optimal policies</li> </ol> \\[ \\theta_{t+1} = \\theta + \\alpha \\nabla J(\\theta_t) \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#objective-function","title":"Objective Function","text":"<p>Goal: \\(maximise\\) the expected long-term return under the policy:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\, G(\\tau) \\,\\right], \\] <p>where</p> <ul> <li>\\(\\tau\\) represents the trajectory generated by following such policy</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#optimise-the-object-function","title":"Optimise the Object Function","text":"<p>By using Gradient Descent:</p> <p>$$ \\theta_{k+1} = \\theta_k - \\alpha \\nabla J(\\theta_k) $$ \\(\\nabla J(\\theta_k)\\) can be computed by:</p> <ul> <li>Finite Difference<ul> <li>Run one episode on \\(\\theta_k\\) and get \\(J(\\theta_k)\\)</li> <li>Slightly change \\(\\theta_k\\) to \\(\\theta_k + \\epsilon \\vec{e}_k\\)<ul> <li>where \\(\\epsilon \\vec{e}_k\\) is a small noise (in vector form)</li> </ul> </li> <li>Run one episode again on \\(\\theta_k + \\epsilon \\vec{e}_k\\) and get \\(J(\\theta_k + \\epsilon \\vec{e}_k)\\)</li> <li>Estimate gradient by using finite difference:</li> </ul> </li> </ul> \\[ \\nabla J(\\theta) \\approx  \\frac{J(\\theta + \\epsilon e_i) - J(\\theta)}{\\epsilon} \\] <ul> <li>True Gradient Descent (The Gradient-ascent Algorithm)</li> </ul> \\[ \\begin{aligned} \\nabla J(\\theta) &amp;= \\sum_{\\tau} \\nabla P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(definition of expected return)} \\\\[4pt] &amp;= \\sum_{\\tau} P(\\tau \\mid \\theta) \\, \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) &amp;&amp; \\text{(apply log-derivative trick)} \\\\[4pt] &amp;= \\mathbb{E}_{\\tau \\sim P(\\tau \\mid \\theta)} \\big[ \\nabla \\log P(\\tau \\mid \\theta) \\, G(\\tau) \\big] &amp;&amp; \\text{(convert sum to expectation)} \\\\[4pt] &amp;= \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla \\log \\pi_\\theta(A \\mid S) \\, G(\\tau) \\big] &amp;&amp; \\text{(expand trajectory likelihood)} \\end{aligned} \\] <ul> <li>Stochastic Gradient Descent</li> </ul> \\[ \\nabla J(\\theta) \\approx \\nabla \\log \\pi_\\theta (a_t \\mid s_t) \\, G(\\tau) \\] <p>where</p> <ul> <li>\\(\\nabla \\log \\pi_\\theta(a \\mid s)\\) is also called Score Function</li> </ul> <p>To compute Score Function, we can no longer use deterministic/non-linear/non-differentiable \\(\\{0,1\\}\\) to describe policy. </p> <p>So we introduce some soft policy strategies for making policy outputs are probabilistic.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#deterministic-policy-stochastic-policy","title":"Deterministic Policy\u00a0\u2192 Stochastic Policy","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#deterministicnear-deterministic-policy","title":"Deterministic/Near-deterministic Policy","text":"<p>Give the optimal action directly:</p> \\[ \\pi(a \\mid s) = \\mathbf{1}\\{\\, a = a^*(s) \\,\\} \\] <p>Or use soft policy for small explorations:</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#-greedy","title":"\u03b5-greedy","text":"<p>most time exploitation (\\(P = 1 - \\varepsilon\\)), little time exploration (\\(\\varepsilon\\)):</p> \\[ \\pi(a \\mid s) = \\begin{cases} 1 - \\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\dfrac{\\varepsilon}{|\\mathcal{A}|}, &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#upper-confidence-bound-ucb","title":"Upper Confidence Bound (UCB)","text":"<p>exploitation + exploration bonus:</p> \\[ a = \\arg\\max_a \\left( Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right) \\] <p>where</p> <ul> <li>\\(c\\) - importance of exploration (parameter)</li> <li>\\(\\ln{t}\\) - explore more when time goes</li> <li>\\(N(a)\\) - not try too many times on one same action</li> </ul> <p>All previous RL methods' outputs can be deterministic (The Optimum) which do not need differentiable probabilities.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#stochastic-policy","title":"Stochastic Policy","text":"<p>Give the probability of each action: $$ \\pi(a \\mid s) = P(a \\mid s) $$ - Ability of exploration; Robustness\u2705</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#softmax","title":"softmax","text":"\\[ \\pi(a \\mid s) = \\frac{\\exp\\!\\left(h_\\theta(s,a)\\right)}        {\\sum_{a' \\in \\mathcal{A}} \\exp\\!\\left(h_\\theta(s,a')\\right)} \\] <p>where \\(h(\\cdot)\\) is another feature function</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#gaussian","title":"Gaussian","text":"<p>Action space needs to be continuous:</p> \\[ \\pi(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\theta^2(s)}}   \\exp\\!\\left(     -\\frac{\\left(a-\\mu_\\theta(s)\\right)^2}{2\\sigma_\\theta^2(s)}   \\right). \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#solve-the-score-function","title":"Solve the Score Function","text":"<p>By applying Softmax:</p> \\[ \\begin{align} \\nabla \\log \\pi_\\theta(a \\mid s) &amp;= \\nabla h(a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\nabla h(a')] \\end{align} \\] <p>By applying Gaussian (assume \\(\\sigma\\) is fixed):</p> \\[ \\nabla \\log \\pi_\\theta(a \\mid s) = \\frac{a - \\mu_\\theta(s)}{\\sigma^2}\\nabla \\mu_\\theta(s) \\] <p>\u03b5-greedy CANNOT Be Used to solve the Score Function</p> <p>The score-function estimator requires a differentiable stochastic policy so that the log-probability gradient is well-defined.</p> <p>However, the \\(\\arg\\max Q(s,a)\\) step used in \u03b5-greedy is discrete and therefore\u00a0non-differentiable, violating this requirement.</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#approximate-the-expected-return","title":"Approximate the Expected Return","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#average-state-value","title":"Average State Value","text":"<p>By \\(maximise\\) weighted average of all state values:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)v^{\\pi_\\theta}(s) \\] <p>where</p> <ul> <li>\\(d^{\\pi_\\theta}(s)\\) is the weight (importance) of each state</li> </ul>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#start-state-value","title":"Start State Value","text":"<p>In episodic environments, we may only care about value of start states:</p> \\[ J(\\theta)  = v^{\\pi_\\theta}(s_0) \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#average-one-step-reward","title":"Average One-step Reward","text":"<p>By \\(maximise\\) weighted average of all immediate rewards:</p> \\[ J(\\theta)  = \\sum_{s \\in S}d^{\\pi_\\theta}(s)  \\sum_{a \\in A} \\pi_\\theta(a \\mid s) \\, R^a_s \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Average State Value and Average One-step Reward are Equivalent:</p> \\[ \\bar{R}_\\pi = (1 - \\gamma) \\, \\bar{v}_\\pi \\] <p>In Gradient Form:</p> \\[ \\nabla \\bar{R}_\\pi \\simeq { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s,a) \\bigr]} \\] \\[ \\nabla \\bar{v}_\\pi = \\frac{1}{1 - \\gamma} \\nabla \\bar{R}_\\pi \\] \\[ \\nabla v_\\pi(s_0) = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s_0) \\; q_{\\pi_{\\theta}}(s_0,a) \\bigr]} \\] <p>In summary, gradient version of above methods can be simplified to just one formula:</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\] <p>Then, we can use our familiar value approximators (MC/TD) to solve \\(q_{\\pi_{\\theta}}(s, a)\\).</p>"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#reinforce","title":"REINFORCE","text":"<p>Derive from MC, we can use return \\(v_t\\) as an unbiased sample of \\(q_{\\pi_{\\theta}}(s,a)\\):</p> \\[ q_{\\pi_{\\theta}}(s,a) = v_t \\]"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#actor-critic","title":"Actor-Critic","text":""},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#qac","title":"QAC","text":"<p>Derive from TD, we can use a\u00a0value function approximator to estimate action-value function:</p> \\[ q_{\\pi_{\\theta}}(s,a) = q(s,a, w) \\] Actor Critic Role Policy Policy Evaluation (Value Function) Input Score State, Reward, Action Task Update \\(\\theta\\) by Policy Gradient Update \\(w\\) by Value Learning Output Action chosen by the policy Score (Value / Advantage)"},{"location":"Unimelb/25S2/AIP/10-Policy-Approx.html#a2c-advantage-actor-critic","title":"A2C - Advantage actor-critic","text":"<p>Introduce a baseline for reducing variance without changing the expectation<sup>1</sup>:</p> \\[ \\begin{align} \\nabla_{\\theta} J(\\theta)  &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; q_{\\pi_{\\theta}}(s, a) \\bigr]} \\\\[4pt] &amp;= { \\mathbb{E}_{\\pi_{\\theta}}  \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\;  \\left[ q_{\\pi_{\\theta}}(s, a) - B(s) \\right] \\bigr]} \\end{align} \\] <ul> <li>A good (sub-optimal) baseline is the state value function \\(v_{\\pi_\\theta}(s)\\)</li> </ul> <p>By applying it, we can get the Advantage Function:</p> \\[ A_{\\pi_{\\theta}}(s,a) =q_{\\pi_{\\theta}}(s, a) - v_{\\pi_\\theta}(s) \\] <p>Then,</p> \\[ \\nabla_{\\theta} J(\\theta)  = { \\mathbb{E}_{\\pi_{\\theta}} \\bigl[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\mid s) \\; A_{\\pi_{\\theta}}(s,a) \\bigr]} \\] <p>Derive from TD, we can use TD error to unbiased estimate such function</p> \\[ \\delta_{\\pi_{\\theta}} = r + \\gamma v_{\\pi_{\\theta}}(s') - v_{\\pi_{\\theta}}(s) \\] \\[ \\begin{align} \\mathbb{E}_{\\pi_{\\theta}}[\\delta_{\\pi_{\\theta}} \\mid s,a] &amp;= \\mathbb{E}_{\\pi_{\\theta}}[r + \\gamma v_{\\pi_{\\theta}}(s') \\mid s,a] - v_{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= q^{\\pi_{\\theta}}(s,a) - v^{\\pi_{\\theta}}(s) \\\\[2pt] &amp;= A^{\\pi_{\\theta}}(s,a) \\end{align} \\] <p>Finally, we actually just use the TD error to compute the policy gradient:</p> \\[ \\nabla_{\\theta} J(\\theta) = {\\mathbb{E}_{\\pi_{\\theta}} \\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\\; \\delta_{\\pi_{\\theta}} \\big]} \\] <ol> <li> <p>\u3010\u7b2c10\u8bfe-Actor-Critic\u65b9\u6cd5\uff08Part2-Advantage Actor-Critic (A2C)\uff09\u3010\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b66\u539f\u7406\u3011-\u54d4\u54e9\u54d4\u54e9\u3011 https://b23.tv/SlGApOj\u00a0\u21a9</p> </li> </ol>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html","title":"11 Integrating Learning and Planning","text":""},{"location":"Unimelb/25S2/AIP/11-Integrating.html#model-free-integrated-architectures","title":"Model-free \u2192 Integrated Architectures","text":"<p>Model-free</p> <ul> <li>No model</li> <li>Learn Value/Policy from experience</li> </ul> <p>Outcome</p> <ul> <li>Cannot look ahead\u274c</li> <li>Sample may be expensive\u26a0\ufe0f</li> <li>May fall into sub-optimal</li> </ul> <p>Integrated Architectures</p> <ul> <li>Learn Model from experience</li> <li>Learn and plan Value/Policy from real and simulated experience</li> </ul> <p>Outcome</p> <ul> <li>Sample is efficient\u2705 (directly from simulators)</li> <li>Avoid model error\u2705 (Beyond Model-based)</li> <li>Supervised Learning methods can be used in model training\u2705</li> <li>Uncertainty can be learned in model training\u2705</li> <li>Modelling is hard\u274c<ul> <li>Need environment to be model-learnable\u26a0\ufe0f</li> <li>Compounding model bias\u274c (experience is from model)</li> </ul> </li> <li>High computation cost\u274c</li> </ul>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#simulation-based-search","title":"Simulation-based Search","text":"<p>Model-based RL + Model-free RL backup</p> <ol> <li>Collect the current state \\(s_t\\) from real world</li> <li>For each \\(a \\in A\\), simulate \\(K\\) episodes from \\(s_t\\):<ul> <li>Use a model of MDP to look ahead</li> <li>Use Forward Search methods (e.g. BFS, DFS)\u00a0to select action</li> <li>Terminate after running \\(T-t\\) steps</li> </ul> </li> </ol> \\[ \\{ s_t^k, A_t^k, R_{t+1}^k, \\ldots, S_T^k \\}_{k=1}^K \\;\\sim\\; \\mathcal{M}_\\nu \\] <ol> <li>Take each episode as a experience for Model-free update</li> <li>Select the action in real world based on optimal policy from Model-free RL</li> </ol>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#simple-monte-carlo-search","title":"Simple Monte-Carlo Search","text":"<p>Derive from MC, update \\(q\\) value by mean return of all episodes, select the optimal action with \\(maximum\\) \\(q\\).</p>"},{"location":"Unimelb/25S2/AIP/11-Integrating.html#monte-carlo-tree-search","title":"Monte-Carlo Tree Search","text":"<ul> <li>Build a\u00a0search tree\u00a0containing visited states and actions</li> <li>Update \\(q\\) by mean return of episodes starting from each \\((s,a)\\)</li> </ul> \\[ q(s,a) = \\frac{1}{N(s,a)} \\sum_{k=1}^K \\sum_{u=t}^T \\mathbf{1}(S_u, A_u = s,a) \\, G_u \\] <p>MCTS is Off-policy</p> <ul> <li>Behaviour policy: rollout/simulation policy</li> <li>Target policy: greedy w.r.t search Q(s,a)</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html","title":"1 Foundations &amp; Basic Statistics","text":""},{"location":"Unimelb/25S2/IML/01-foundations.html#data","title":"Data","text":"<ul> <li>Discrete vs. Continuous</li> <li>Big vs. Small</li> <li>Labeled vs. Unlabelled</li> <li>Public vs. Sensitive</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html#models","title":"Models","text":"<ul> <li>Basic Assumption: Samples are independent</li> <li> <p>Function mapping from inputs to outputs</p> </li> <li> <p>Parameters of function are unknown \u2190 what the model need to learn</p> </li> <li> <p>Hyper-parameters are pre-defined and fixed</p> </li> <li> <p>Probabilistic vs. Geometric</p> </li> <li>Discriminative vs. Generative</li> <li>Linear vs. Non-linear</li> <li>Parametric vs. Non-parametric</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html#learning","title":"Learning","text":"<ul> <li>Find the best model parameters</li> <li>Supervised vs. Unsupervised vs. Weak Supervised</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html#probability-basics-formula","title":"Probability Basics Formula","text":""},{"location":"Unimelb/25S2/IML/01-foundations.html#change-of-logarithm-base","title":"Change of Logarithm Base","text":"\\[ \\log_a b = \\frac{\\log_c b}{\\log_c a} \\] <p>Commonly used for converting between \\(\\log_2\\), \\(\\log_{10}\\), and \\(\\ln\\).</p>"},{"location":"Unimelb/25S2/IML/01-foundations.html#conditional-probability","title":"Conditional Probability","text":"\\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\]"},{"location":"Unimelb/25S2/IML/01-foundations.html#joint-probability","title":"Joint Probability","text":"\\[ P(A \\cap B) = P(A \\mid B)\\, P(B) = P(B \\mid A)\\, P(A) \\]"},{"location":"Unimelb/25S2/IML/01-foundations.html#law-of-total-probability","title":"Law of Total Probability","text":"\\[ P(A) = \\sum_i P(A \\mid B_i)\\, P(B_i) \\]"},{"location":"Unimelb/25S2/IML/01-foundations.html#bayes-theorem-naive-bayes","title":"Bayes' Theorem (Na\u00efve Bayes)","text":"\\[ P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)} \\] <p>In Na\u00efve Bayes, assuming feature independence:</p> \\[ P(y \\mid x_1, x_2, \\ldots, x_n) \\propto P(y)\\, \\prod_i P(x_i \\mid y) \\]"},{"location":"Unimelb/25S2/IML/01-foundations.html#entropy","title":"Entropy","text":""},{"location":"Unimelb/25S2/IML/01-foundations.html#information-self-information","title":"Information (Self-Information)","text":"<p>The information content of an outcome \\(x\\):</p> \\[ I(x) = -\\log_2 P(x) \\] <ul> <li>Maps probability range \\([0, 1]\\) to information range \\([0, +\\infty)\\) </li> <li>Rare events \\(\\to\\) larger information gain</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html#entropy-expected-information","title":"Entropy (Expected Information)","text":"<p>Entropy is the expected information obtained from a random variable \\(X\\):</p> \\[ H(X) = \\mathbb{E}[I(X)] = -\\sum_x P(x)\\, \\log_2 P(x) \\] <p>Interpretation:</p> <ul> <li>\\(X\\) is a random variable with possible values \\(x\\)</li> <li>\\(P(x)\\) is the probability (weight) of outcome \\(x\\)</li> <li>\\(-\\log_2 P(x)\\) is the information carried by outcome \\(x\\)</li> </ul> <p>High entropy = the random variable is highly uncertain / disordered.</p>"},{"location":"Unimelb/25S2/IML/01-foundations.html#joint-entropy","title":"Joint Entropy","text":"\\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 P(x, y) \\]"},{"location":"Unimelb/25S2/IML/01-foundations.html#conditional-entropy","title":"Conditional Entropy","text":"<p>Definition:</p> \\[ H(X \\mid Y) = H(X, Y) - H(Y) \\] <p>Derivation via chain rule:</p> <p>Given $$ P(x, y) = P(y)\\, P(x \\mid y), $$</p> <p>Substitute into joint entropy:</p> \\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 \\big[ P(y)\\, P(x \\mid y) \\big] \\] <p>Split the log:</p> \\[ H(X, Y) = -\\sum_{x,y} P(x, y)\\, \\log_2 P(y)   - \\sum_{x,y} P(x, y)\\, \\log_2 P(x \\mid y) \\] <p>Since \\(\\sum_x P(x,y) = P(y)\\):</p> \\[ H(X, Y) = -\\sum_y P(y)\\, \\log_2 P(y)   - \\sum_{y} P(y) \\sum_x P(x \\mid y)\\, \\log_2 P(x \\mid y) \\] <p>Thus:</p> \\[ H(X, Y) = H(Y) + H(X \\mid Y) \\] <p>Interpretation:</p> <ul> <li>For each \\(y\\), compute uncertainty of \\(P(x \\mid y)\\) </li> <li>Weight it by \\(P(y)\\) </li> <li>Sum over all \\(y\\)</li> </ul>"},{"location":"Unimelb/25S2/IML/01-foundations.html#information-gain-mutual-information","title":"Information Gain (Mutual Information)","text":"<p>Mutual Information measures how much knowing \\(Y\\) reduces uncertainty about \\(X\\):</p> \\[ IG(X; Y) = H(X) - H(X \\mid Y)          = H(Y) - H(Y \\mid X) \\] <p>Mutual information can be expressed using joint probabilities:</p> \\[ IG(X; Y) = \\sum_{x,y} P(x,y)\\, \\log_2 \\frac{P(x,y)}{P(x)\\, P(y)} \\] <p>Interpretation:</p> <ul> <li>Enumerate all \\((x, y)\\) pairs</li> <li>Compute marginal probabilities \\(P(x)\\) and \\(P(y)\\)</li> <li>Compute joint probabilities \\(P(x,y)\\) </li> <li>Compare joint probability with the \u201cindependent joint\u201d \\(P(x)P(y)\\)</li> </ul> <p>Meaning of the log ratio:</p> <ul> <li>\\(\\log_2 \\frac{P(x,y)}{P(x)P(y)} = 0\\) \u2192 occurs as expected under independence  </li> <li>\\(&gt; 0\\) \u2192 co-occur more than expected \u2192 positive association  </li> <li>\\(&lt; 0\\) \u2192 co-occur less than expected \u2192 negative association</li> </ul>"}]}